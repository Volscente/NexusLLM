{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NexusLLM Introduction Scope NexusLLM is a GitHub repository dedicated to exploring various experiments related to Language Model Models (LMM). From fine-tuning and instruction-tuning to RAG and agent-based systems, it offers a diverse range of experiments and insights for researchers and enthusiasts interested in natural language processing and AI innovation.","title":"NexusLLM"},{"location":"#nexusllm","text":"","title":"NexusLLM"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#scope","text":"NexusLLM is a GitHub repository dedicated to exploring various experiments related to Language Model Models (LMM). From fine-tuning and instruction-tuning to RAG and agent-based systems, it offers a diverse range of experiments and insights for researchers and enthusiasts interested in natural language processing and AI innovation.","title":"Scope"},{"location":"LLMOps/","text":"LLMOps GenAI Development Lifecycle It is composed by 5 steps or phases: Discovery - Explore landscape of available models to identify the most suitable one for their specific gen AI application. Development and Experimentation - Techniques like prompt engineering, few-shot learning or PEFT. Deployment - It needs many new artifacts in the deployment process, including prompt templates, chain definitions, embedding models, retrieval data stores, and fine-tuned model adapters among others. Continuous Monitoring Continuous Improvement - It requires taking foundation models (FMs) and then adapting them to our specific use case. Traditional continuous training still holds relevance for scenarios when recurrent fine-tuning or incorporating human feedback loops are still needed. Discovery The current AI Landscape has: An abundance of models No model can fit all solutions Here are some factors to consider when exploring models: Quality: Early assessments can involve running test prompts or analyzing public benchmarks and metrics to gauge output quality. Latency & throughput: These factors directly impact user experience. A chatbot demands lower latency than batch-processed summarization tasks. Development & maintenance time: Consider the time investment for both initial development and ongoing maintenance. Managed models often require less effort than self-deployed open-source alternatives. Usage cost: Factor in infrastructure and consumption costs associated with using the chosen model. Compliance: Assess the model's ability to adhere to relevant regulations and licensing terms. Development and Experimentation Definition It involves experimental iterations composed by three steps: Data Refinement Foundation model selection and adaptation Evaluation Foundational Model Foundation models differ from predictive models most importantly because they are multi- purpose models. Foundation models also exhibit what are known as \u2018emergent properties\u2019,2 capabilities that emerge in response to specific input without additional training. Predictive models are only able to perform the single function they were trained for; a traditional French-English translation model, for instance, cannot also solve math problems. Foundation models are also highly sensitive to changes in their input. A foundation model can be made to perform translation, generation, or classificatio tasks simply by changing the input. These new properties of foundation models have created a corresponding paradigm shift in the practices required to develop and operationalize Gen AI systems. While models in the predictive AI context are self-sufficient and task-specific, gen AI models are multipurpose and need an additional element beyond the user input to function as part of a gen AI Application: a prompt, and more specifically, a prompt template, defined as a set of instructions and examples along with placeholders to accommodate user input. MLOps for GenAI or LLMops Definition GenAI models are usually a chain of agents, which characteristic presents few more challenges with respect to traditional MLOps. Aspects Evaluation - Because of their tight coupling, chains need end-to-end evaluation, not just on a per-component basis, to gauge their overall performance and the quality of their output. Versioning - A chain needs to be managed as a complete artifact in its entirety. The chain configuration should be tracked with its own revision history for analysis, reproducibility, and understanding the impact of changes on output. Continuous Monitoring - It is used for detecting performance degradation, data drift, or unexpected behavior in the chain. Introspection - The ability to inspect the internal data flows of a chain (inputs and outputs from each component) as well as the inputs and outputs of the entire chain is paramount. Continuous Training & Tuning Definition In machine learning operations (MLOps), continuous training is the practice of repeatedly retraining machine learning models in a production environment. For gen AI models, continuous tuning of the models is often more practical than retraining from scratch due to the high data and computational costs involved. Data Requirements This ease of prototyping, however, comes with a challenge. Traditional predictive AI relies on apriori well-defined dataset(s). In gen AI, a single application can leverage various data types, from completely different data sources, all working together (Figure 10). Let\u2019s explore some of these data types: Conditioning Prompts - System prompts or Contextual prompts Few-shot Examples - Input-output pairs Grounding/Augmentation Data - Data coming from external sources to help the model crafting the output Task-specific Datasets - Used for fine-tuning Human Preference Datasets - Used for RLHF Full Pre Training Corpora - Dataset for pre-training Deploy Introduction We need to distinguish between deployment of: - Foundation Models - Generative AI Systems Generative AI Systems Versioning: Prompt template Chain definition External datasets Adapter models Foundation Models Infrastructure Validation This refers to the introduction of an additional verification step, prior to deploying the training and serving systems, to check both the compatibility of the model with the defined serving configuration and the availability of the required hardware. Compression Another way of addressing infrastructure challenges is to optimize the model itself. Compressing and/or optimizing the model can often significantly reduce the storage and compute resources needed for training and serving, and in many cases can also decrease the serving latency. Some techniques for model compression and optimization include quantization, distillation and model pruning. Quantization reduces the size and computational requirements of the model by converting its weights and activations from higher-precision floating-point numbers to lower-precision representations, such as 8-bit integers or 16-bit floating-point numbers. This can significantly reduce the memory footprint and computational overhead of the model. Model Pruning is a technique for eliminating unnecessary weight parameters or by selecting only important subnetworks within the model. This reduces model size while maintaining accuracy as high as possible. Finally, distillation trains a smaller model, using the responses generated by a larger LLM, to reproduce the output of the larger LLM for a specific domain. This can significantly reduce the amount of training data, compute, and storage resources needed for the application. Monitoring Introduction Monitoring can be applied to the overall gen AI application and to individual components. We prioritize monitoring at the application level. This is because if the application is performan and monitoring proves that, it implies that all components are also performant. You can also apply the same practices to each of the prompted model components to get more granular results and understanding of your application. Skew Detection Skew detection in traditional ML systems refers to training-serving skew that occurs when the feature data distribution in production deviates from the feature data distribution observed during model training. In the case of Gen AI systems using pretrained models in components chained together to produce the output, we need to modify our approach. We can measure skew by comparing the distribution of the input data we used to evaluate our application (the test set as described under the Data Curation and Principles section above) and the distribution of the inputs to our application in production. Drift Detection Like skew detection, the drift detection process checks for statistical differences between two datasets. However, instead of comparing evaluations and serving inputs, drift looks for changes in input data. This allows you to check how the inputs and therefore the behavior of your users changed over time. Given that the input to the application is typically text, there are a few approaches to measuring skew and drift. Some common approaches are calculating embeddings and distances, counting text length and number of tokens, and tracking vocabulary changes, new concepts and intents, prompts and topics in datasets, as well as statistical approaches such as least-squares density difference, maximum mean discrepancy (MMD), learned kernel MMD, or context-aware MMD.","title":"LLMOps"},{"location":"LLMOps/#llmops","text":"","title":"LLMOps"},{"location":"LLMOps/#genai-development-lifecycle","text":"It is composed by 5 steps or phases: Discovery - Explore landscape of available models to identify the most suitable one for their specific gen AI application. Development and Experimentation - Techniques like prompt engineering, few-shot learning or PEFT. Deployment - It needs many new artifacts in the deployment process, including prompt templates, chain definitions, embedding models, retrieval data stores, and fine-tuned model adapters among others. Continuous Monitoring Continuous Improvement - It requires taking foundation models (FMs) and then adapting them to our specific use case. Traditional continuous training still holds relevance for scenarios when recurrent fine-tuning or incorporating human feedback loops are still needed.","title":"GenAI Development Lifecycle"},{"location":"LLMOps/#discovery","text":"The current AI Landscape has: An abundance of models No model can fit all solutions Here are some factors to consider when exploring models: Quality: Early assessments can involve running test prompts or analyzing public benchmarks and metrics to gauge output quality. Latency & throughput: These factors directly impact user experience. A chatbot demands lower latency than batch-processed summarization tasks. Development & maintenance time: Consider the time investment for both initial development and ongoing maintenance. Managed models often require less effort than self-deployed open-source alternatives. Usage cost: Factor in infrastructure and consumption costs associated with using the chosen model. Compliance: Assess the model's ability to adhere to relevant regulations and licensing terms.","title":"Discovery"},{"location":"LLMOps/#development-and-experimentation","text":"","title":"Development and Experimentation"},{"location":"LLMOps/#definition","text":"It involves experimental iterations composed by three steps: Data Refinement Foundation model selection and adaptation Evaluation","title":"Definition"},{"location":"LLMOps/#foundational-model","text":"Foundation models differ from predictive models most importantly because they are multi- purpose models. Foundation models also exhibit what are known as \u2018emergent properties\u2019,2 capabilities that emerge in response to specific input without additional training. Predictive models are only able to perform the single function they were trained for; a traditional French-English translation model, for instance, cannot also solve math problems. Foundation models are also highly sensitive to changes in their input. A foundation model can be made to perform translation, generation, or classificatio tasks simply by changing the input. These new properties of foundation models have created a corresponding paradigm shift in the practices required to develop and operationalize Gen AI systems. While models in the predictive AI context are self-sufficient and task-specific, gen AI models are multipurpose and need an additional element beyond the user input to function as part of a gen AI Application: a prompt, and more specifically, a prompt template, defined as a set of instructions and examples along with placeholders to accommodate user input.","title":"Foundational Model"},{"location":"LLMOps/#mlops-for-genai-or-llmops","text":"","title":"MLOps for GenAI or LLMops"},{"location":"LLMOps/#definition_1","text":"GenAI models are usually a chain of agents, which characteristic presents few more challenges with respect to traditional MLOps.","title":"Definition"},{"location":"LLMOps/#aspects","text":"Evaluation - Because of their tight coupling, chains need end-to-end evaluation, not just on a per-component basis, to gauge their overall performance and the quality of their output. Versioning - A chain needs to be managed as a complete artifact in its entirety. The chain configuration should be tracked with its own revision history for analysis, reproducibility, and understanding the impact of changes on output. Continuous Monitoring - It is used for detecting performance degradation, data drift, or unexpected behavior in the chain. Introspection - The ability to inspect the internal data flows of a chain (inputs and outputs from each component) as well as the inputs and outputs of the entire chain is paramount.","title":"Aspects"},{"location":"LLMOps/#continuous-training-tuning","text":"","title":"Continuous Training &amp; Tuning"},{"location":"LLMOps/#definition_2","text":"In machine learning operations (MLOps), continuous training is the practice of repeatedly retraining machine learning models in a production environment. For gen AI models, continuous tuning of the models is often more practical than retraining from scratch due to the high data and computational costs involved.","title":"Definition"},{"location":"LLMOps/#data-requirements","text":"This ease of prototyping, however, comes with a challenge. Traditional predictive AI relies on apriori well-defined dataset(s). In gen AI, a single application can leverage various data types, from completely different data sources, all working together (Figure 10). Let\u2019s explore some of these data types: Conditioning Prompts - System prompts or Contextual prompts Few-shot Examples - Input-output pairs Grounding/Augmentation Data - Data coming from external sources to help the model crafting the output Task-specific Datasets - Used for fine-tuning Human Preference Datasets - Used for RLHF Full Pre Training Corpora - Dataset for pre-training","title":"Data Requirements"},{"location":"LLMOps/#deploy","text":"","title":"Deploy"},{"location":"LLMOps/#introduction","text":"We need to distinguish between deployment of: - Foundation Models - Generative AI Systems","title":"Introduction"},{"location":"LLMOps/#generative-ai-systems","text":"Versioning: Prompt template Chain definition External datasets Adapter models","title":"Generative AI Systems"},{"location":"LLMOps/#foundation-models","text":"Infrastructure Validation This refers to the introduction of an additional verification step, prior to deploying the training and serving systems, to check both the compatibility of the model with the defined serving configuration and the availability of the required hardware. Compression Another way of addressing infrastructure challenges is to optimize the model itself. Compressing and/or optimizing the model can often significantly reduce the storage and compute resources needed for training and serving, and in many cases can also decrease the serving latency. Some techniques for model compression and optimization include quantization, distillation and model pruning. Quantization reduces the size and computational requirements of the model by converting its weights and activations from higher-precision floating-point numbers to lower-precision representations, such as 8-bit integers or 16-bit floating-point numbers. This can significantly reduce the memory footprint and computational overhead of the model. Model Pruning is a technique for eliminating unnecessary weight parameters or by selecting only important subnetworks within the model. This reduces model size while maintaining accuracy as high as possible. Finally, distillation trains a smaller model, using the responses generated by a larger LLM, to reproduce the output of the larger LLM for a specific domain. This can significantly reduce the amount of training data, compute, and storage resources needed for the application.","title":"Foundation Models"},{"location":"LLMOps/#monitoring","text":"","title":"Monitoring"},{"location":"LLMOps/#introduction_1","text":"Monitoring can be applied to the overall gen AI application and to individual components. We prioritize monitoring at the application level. This is because if the application is performan and monitoring proves that, it implies that all components are also performant. You can also apply the same practices to each of the prompted model components to get more granular results and understanding of your application.","title":"Introduction"},{"location":"LLMOps/#skew-detection","text":"Skew detection in traditional ML systems refers to training-serving skew that occurs when the feature data distribution in production deviates from the feature data distribution observed during model training. In the case of Gen AI systems using pretrained models in components chained together to produce the output, we need to modify our approach. We can measure skew by comparing the distribution of the input data we used to evaluate our application (the test set as described under the Data Curation and Principles section above) and the distribution of the inputs to our application in production.","title":"Skew Detection"},{"location":"LLMOps/#drift-detection","text":"Like skew detection, the drift detection process checks for statistical differences between two datasets. However, instead of comparing evaluations and serving inputs, drift looks for changes in input data. This allows you to check how the inputs and therefore the behavior of your users changed over time. Given that the input to the application is typically text, there are a few approaches to measuring skew and drift. Some common approaches are calculating embeddings and distances, counting text length and number of tokens, and tracking vocabulary changes, new concepts and intents, prompts and topics in datasets, as well as statistical approaches such as least-squares density difference, maximum mean discrepancy (MMD), learned kernel MMD, or context-aware MMD.","title":"Drift Detection"},{"location":"LLMs/","text":"LLMs Alignment Process Definition Usually, a LLM goes through the following two training steps: Pre-training Fine-Tuning The Alignment process is used in order to collect feedbacks of users regarding a particular LLM's output and decide which is better. Example Pre-Training phase Fine-Tuning phase \u2192 \"Is pineapple on Pizza a Crime?\" \u2192 \"Putting pineapple on a Pizza violates the Geneva convention etc.\" Alignment phase \u2192 Use the user's feedbacks Prompting Definition Usually called also \"Prompt Engineering\", it is the process of crafting a good prompt for the desired purpose. Types Zero-Shot Prompting - Ask the LLM to solve a problem without previous reference or example Few-Shot Prompting - Provide few examples on how to solve the problem and then ask Chain-of-Thought Prompting - Guide the LLM through the entire process it has to do to solve the problem System, Contextual and Role Prompting System, contextual and role prompting are all techniques used to guide how LLMs generate text, but they focus on different aspects: System prompting sets the overall context and purpose for the language model. It defines the \u2018big picture\u2019 of what the model should be doing, like translating a language, classifying a review etc. Contextual prompting provides specific details or background information relevant to the current conversation or task. It helps the model to understand the nuances of what\u2019s being asked and tailor the response accordingly. Role prompting assigns a specific character or identity for the language model to adopt. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior. Techniques Step-back prompting Step-back prompting is a technique for improving the performance by prompting the LLM to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task. This \u2018step back\u2019 allows the LLM to activate relevant background knowledge and reasoning processes before attempting to solve the specific problem. Examples: Write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging. Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game? Self-Consistency It follows the following steps: Generating diverse reasoning paths: The LLM is provided with the same prompt multiple times. A high temperature setting encourages the model to generate different reasoning paths and perspectives on the problem. Extract the answer from each generated response. Choose the most common answer. Tree of Thoughts (ToT) It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought. ReAct (reason & act) LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling. ReAct mimics how humans operate in the real world. from langchain.agents import load_tools from langchain.agents import initialize_agent from langchain.agents import AgentType from langchain.llms import VertexAI prompt = \"How many kids do the band members of Metallica have?\" llm = VertexAI(temperature=0.1) tools = load_tools([\"serpapi\"], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) agent.run(prompt) Inference Process There\u2019s a common misconception that LLMs like GPT-2 directly produce text. This isn\u2019t the case. Instead, LLMs calculate logits, which are scores assigned to every possible token in their vocabulary. To simplify, here\u2019s an illustrative breakdown of the process: The LLM produces just logits of the most probable tokens. Then we have the so called \"Decoding\" process, which will choose the token to sample through different Sampling Techniques. Sampling Techniques How to choose which is the best output token? Greedy Search It samples the most probable token each time. Once the token is sampled, it is then added to the input sequence. Beam Search Random Sampling Selects the next token according to the probability distribution, where each token is sampled proportionally to its predicted probability. Temperature Sampling Adjusts the probability distribution by a temperature parameter. Higher temperatures promote diversity, lower temperatures favor high-probability tokens. Top-K sampling Randomly samples from the top K most probable tokens. Performance There are several ways to make the inference process more performing: Quantisation - It uses lower precision memory in order to not lose many Distillation - Train a smaller model Parameters Output Length - It's the number of tokens Temperature - Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results. Top-K - Sampling selects the top K most likely tokens from the model\u2019s predicted distribution. Top-P - Sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Considerations: If you set temperature to 0, top-K and top-P become irrelevant\u2013the most probable token becomes the next token predicted. If you set temperature extremely high (above 1\u2013generally into the 10s), temperature becomes irrelevant and whatever tokens make it through the top-K and/or top-P criteria are then randomly sampled to choose a next predicted token. If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the top-K criteria, and that token is the next predicted token. If you set top-K extremely high, like to the size of the LLM\u2019s vocabulary, any token with a nonzero probability of being the next token will meet the top-K criteria and none are selected out. If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the next token will meet the top-P criteria, and none are selected out. Mistral Chat Template Since one of the most common use case for LLMs is chat, rather than continuing a single string of text, the model instead continues a conversation. Mistral uses a specific chat template called ChatML . Much like tokenization, different models expect very different input formats for chat. This is the reason for chat templates as a feature. Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects.","title":"LLMs"},{"location":"LLMs/#llms","text":"","title":"LLMs"},{"location":"LLMs/#alignment-process","text":"","title":"Alignment Process"},{"location":"LLMs/#definition","text":"Usually, a LLM goes through the following two training steps: Pre-training Fine-Tuning The Alignment process is used in order to collect feedbacks of users regarding a particular LLM's output and decide which is better.","title":"Definition"},{"location":"LLMs/#example","text":"Pre-Training phase Fine-Tuning phase \u2192 \"Is pineapple on Pizza a Crime?\" \u2192 \"Putting pineapple on a Pizza violates the Geneva convention etc.\" Alignment phase \u2192 Use the user's feedbacks","title":"Example"},{"location":"LLMs/#prompting","text":"","title":"Prompting"},{"location":"LLMs/#definition_1","text":"Usually called also \"Prompt Engineering\", it is the process of crafting a good prompt for the desired purpose.","title":"Definition"},{"location":"LLMs/#types","text":"Zero-Shot Prompting - Ask the LLM to solve a problem without previous reference or example Few-Shot Prompting - Provide few examples on how to solve the problem and then ask Chain-of-Thought Prompting - Guide the LLM through the entire process it has to do to solve the problem","title":"Types"},{"location":"LLMs/#system-contextual-and-role-prompting","text":"System, contextual and role prompting are all techniques used to guide how LLMs generate text, but they focus on different aspects: System prompting sets the overall context and purpose for the language model. It defines the \u2018big picture\u2019 of what the model should be doing, like translating a language, classifying a review etc. Contextual prompting provides specific details or background information relevant to the current conversation or task. It helps the model to understand the nuances of what\u2019s being asked and tailor the response accordingly. Role prompting assigns a specific character or identity for the language model to adopt. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.","title":"System, Contextual and Role Prompting"},{"location":"LLMs/#techniques","text":"","title":"Techniques"},{"location":"LLMs/#step-back-prompting","text":"Step-back prompting is a technique for improving the performance by prompting the LLM to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task. This \u2018step back\u2019 allows the LLM to activate relevant background knowledge and reasoning processes before attempting to solve the specific problem. Examples: Write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging. Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game?","title":"Step-back prompting"},{"location":"LLMs/#self-consistency","text":"It follows the following steps: Generating diverse reasoning paths: The LLM is provided with the same prompt multiple times. A high temperature setting encourages the model to generate different reasoning paths and perspectives on the problem. Extract the answer from each generated response. Choose the most common answer.","title":"Self-Consistency"},{"location":"LLMs/#tree-of-thoughts-tot","text":"It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought.","title":"Tree of Thoughts (ToT)"},{"location":"LLMs/#react-reason-act","text":"LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling. ReAct mimics how humans operate in the real world. from langchain.agents import load_tools from langchain.agents import initialize_agent from langchain.agents import AgentType from langchain.llms import VertexAI prompt = \"How many kids do the band members of Metallica have?\" llm = VertexAI(temperature=0.1) tools = load_tools([\"serpapi\"], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) agent.run(prompt)","title":"ReAct (reason &amp; act)"},{"location":"LLMs/#inference","text":"","title":"Inference"},{"location":"LLMs/#process","text":"There\u2019s a common misconception that LLMs like GPT-2 directly produce text. This isn\u2019t the case. Instead, LLMs calculate logits, which are scores assigned to every possible token in their vocabulary. To simplify, here\u2019s an illustrative breakdown of the process: The LLM produces just logits of the most probable tokens. Then we have the so called \"Decoding\" process, which will choose the token to sample through different Sampling Techniques.","title":"Process"},{"location":"LLMs/#sampling-techniques","text":"How to choose which is the best output token?","title":"Sampling Techniques"},{"location":"LLMs/#greedy-search","text":"It samples the most probable token each time. Once the token is sampled, it is then added to the input sequence.","title":"Greedy Search"},{"location":"LLMs/#beam-search","text":"","title":"Beam Search"},{"location":"LLMs/#random-sampling","text":"Selects the next token according to the probability distribution, where each token is sampled proportionally to its predicted probability.","title":"Random Sampling"},{"location":"LLMs/#temperature-sampling","text":"Adjusts the probability distribution by a temperature parameter. Higher temperatures promote diversity, lower temperatures favor high-probability tokens.","title":"Temperature Sampling"},{"location":"LLMs/#top-k-sampling","text":"Randomly samples from the top K most probable tokens.","title":"Top-K sampling"},{"location":"LLMs/#performance","text":"There are several ways to make the inference process more performing: Quantisation - It uses lower precision memory in order to not lose many Distillation - Train a smaller model","title":"Performance"},{"location":"LLMs/#parameters","text":"Output Length - It's the number of tokens Temperature - Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results. Top-K - Sampling selects the top K most likely tokens from the model\u2019s predicted distribution. Top-P - Sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Considerations: If you set temperature to 0, top-K and top-P become irrelevant\u2013the most probable token becomes the next token predicted. If you set temperature extremely high (above 1\u2013generally into the 10s), temperature becomes irrelevant and whatever tokens make it through the top-K and/or top-P criteria are then randomly sampled to choose a next predicted token. If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the top-K criteria, and that token is the next predicted token. If you set top-K extremely high, like to the size of the LLM\u2019s vocabulary, any token with a nonzero probability of being the next token will meet the top-K criteria and none are selected out. If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the next token will meet the top-P criteria, and none are selected out.","title":"Parameters"},{"location":"LLMs/#mistral","text":"","title":"Mistral"},{"location":"LLMs/#chat-template","text":"Since one of the most common use case for LLMs is chat, rather than continuing a single string of text, the model instead continues a conversation. Mistral uses a specific chat template called ChatML . Much like tokenization, different models expect very different input formats for chat. This is the reason for chat templates as a feature. Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects.","title":"Chat Template"},{"location":"agents/","text":"Agents Introduction Definition Agents are Generative AI models that can be trained to use tools to access real-time information or suggest a real-world action. This combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent, or a program that extends beyond the standalone capabilities of a Generative AI model. A Generative AI agent can be defined as an application that attempts to achieve a goal by observing the world and acting upon it using the tools that it has at its disposal. Agents vs. Models Architecture Structure An agent is composed by three main components: The Model - It's a LLM that acts as a centralised decision maker, thanks to techniques as ReAct, Chain-of-Thought or Tree-of-Thought The Tools - Foundational models remain constrained by their inability to interact with the outside world. Tools bridge this gap (e.g., RAG). The Orchestration Layer - The orchestration layer describes a cyclical process that governs how the agent takes in information, performs some internal reasoning, and uses that reasoning to inform its next action or decision. It performs this loop until the end goal is reached. Process The sequence of events might go something like this: User sends query to the agent Agent begins the ReAct sequence The agent provides a prompt to the model, asking it to generate one of the next ReAct steps and its corresponding output: a. Question : The input question from the user query, provided with the prompt b. Thought : The model\u2019s thoughts about what it should do next c. Action: The model\u2019s decision on what action to take next c. Action : The model's decision on what action to take next i. This is where tool choice can occur ii. For example, an action could be one of [Flights, Search, Code, None], where the first 3 represent a known tool that the model can choose, and the last represents \u201cno tool choice\u201d d. Action Input : The model's decision on what inputs to provide to the tool (if any) e. Observation : The result of the action / action input sequence i. This though / action / action input / observation could repeat N-times as needed f. Final Answer : The model's final answer to provide to the original user query 4. The ReAct loop concludes and a final answer is provided back ot the user Tools Introduction While language models excel at processing information, they lack the ability to directly perceive and influence the real world. So how can we empower our models to have real-time, context-aware interaction with external systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this critical capability to the model. There are three main primary tools: Extensions Functions Data Stores Extensions Extensions allow agents to seamlessly execute APIs regardless of their underlying implementation. Let\u2019s say that you\u2019ve built an agent with a goal of helping users book flights. You know that you want to use the Google Flights API to retrieve flight information, but you\u2019re not sure how you\u2019re going to get your agent to make calls to this API endpoint. One approach could be to implement custom code. This approach is not scalable and could easily break in any scenario that falls outside the implemented custom code. A more resilient approach would be to use an Extension. An Extension bridges the gap between an agent and an API by: Teaching the agent how to use the API endpoint using examples. Teaching the agent what arguments or parameters are needed to successfully call the API endpoint. An agent can learn how to use an extension through an OpenAPI Specification or a manifest file. These explain in a machine-readable way how to use the extension. import vertexai import pprint from vertexai.preview.extensions import Extension PROJECT_ID = \"YOUR_PROJECT_ID\" REGION = \"us-central1\" vertexai.init(project=PROJECT_ID, location=REGION) extension_code_interpreter = Extension.from_hub(\"code_interpreter\") CODE_QUERY = \"\"\"Write a python method to invert a binary tree in O(n) time.\"\"\" response = extension_code_interpreter.execute( operation_id = \"generate_and_execute\", operation_params = {\"query\": CODE_QUERY} ) print(\"Generated Code:\") pprint.pprint({response['generated_code']}) Functions In the world of software engineering, functions are defined as self-contained modules of code that accomplish a specific task and can be reused as needed. Functions differ from Extensions in a few ways, most notably: A model outputs a Function and its arguments, but doesn\u2019t make a live API call. Functions are executed on the client-side, while Extensions are executed on the agent-side. The logic and execution of calling the actual API endpoint is offloaded away from the agent and back to the client-side. This offers the developer more granular control over the flow. Common use cases: API calls need to be made at another layer of the application stack Security or authentication restrictions that prevent the agent from calling an API directly Additional data transformation logic needs to be applied to the API Response that the agent cannot perform. Data Stores Data Stores address this limitation by providing access to more dynamic and up-to-date information, and ensuring a model\u2019s responses remain grounded in factuality and relevance. In the context of Generative AI agents, Data Stores are typically implemented as a vector database that the developer wants the agent to have access to at runtime. Neural Turing Machine Definition Coupling an ANN with an external memory in order to retrieve information through attention mechanism. Attention Mechanisms Content-based Addressing \u2192 Softmax of key-vector \"k\" and memory storage Location-based Addressing \u2192 1-D Convolution between key-vector and memory storage","title":"Agents"},{"location":"agents/#agents","text":"","title":"Agents"},{"location":"agents/#introduction","text":"","title":"Introduction"},{"location":"agents/#definition","text":"Agents are Generative AI models that can be trained to use tools to access real-time information or suggest a real-world action. This combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent, or a program that extends beyond the standalone capabilities of a Generative AI model. A Generative AI agent can be defined as an application that attempts to achieve a goal by observing the world and acting upon it using the tools that it has at its disposal.","title":"Definition"},{"location":"agents/#agents-vs-models","text":"","title":"Agents vs. Models"},{"location":"agents/#architecture","text":"","title":"Architecture"},{"location":"agents/#structure","text":"An agent is composed by three main components: The Model - It's a LLM that acts as a centralised decision maker, thanks to techniques as ReAct, Chain-of-Thought or Tree-of-Thought The Tools - Foundational models remain constrained by their inability to interact with the outside world. Tools bridge this gap (e.g., RAG). The Orchestration Layer - The orchestration layer describes a cyclical process that governs how the agent takes in information, performs some internal reasoning, and uses that reasoning to inform its next action or decision. It performs this loop until the end goal is reached.","title":"Structure"},{"location":"agents/#process","text":"The sequence of events might go something like this: User sends query to the agent Agent begins the ReAct sequence The agent provides a prompt to the model, asking it to generate one of the next ReAct steps and its corresponding output: a. Question : The input question from the user query, provided with the prompt b. Thought : The model\u2019s thoughts about what it should do next c. Action: The model\u2019s decision on what action to take next c. Action : The model's decision on what action to take next i. This is where tool choice can occur ii. For example, an action could be one of [Flights, Search, Code, None], where the first 3 represent a known tool that the model can choose, and the last represents \u201cno tool choice\u201d d. Action Input : The model's decision on what inputs to provide to the tool (if any) e. Observation : The result of the action / action input sequence i. This though / action / action input / observation could repeat N-times as needed f. Final Answer : The model's final answer to provide to the original user query 4. The ReAct loop concludes and a final answer is provided back ot the user","title":"Process"},{"location":"agents/#tools","text":"","title":"Tools"},{"location":"agents/#introduction_1","text":"While language models excel at processing information, they lack the ability to directly perceive and influence the real world. So how can we empower our models to have real-time, context-aware interaction with external systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this critical capability to the model. There are three main primary tools: Extensions Functions Data Stores","title":"Introduction"},{"location":"agents/#extensions","text":"Extensions allow agents to seamlessly execute APIs regardless of their underlying implementation. Let\u2019s say that you\u2019ve built an agent with a goal of helping users book flights. You know that you want to use the Google Flights API to retrieve flight information, but you\u2019re not sure how you\u2019re going to get your agent to make calls to this API endpoint. One approach could be to implement custom code. This approach is not scalable and could easily break in any scenario that falls outside the implemented custom code. A more resilient approach would be to use an Extension. An Extension bridges the gap between an agent and an API by: Teaching the agent how to use the API endpoint using examples. Teaching the agent what arguments or parameters are needed to successfully call the API endpoint. An agent can learn how to use an extension through an OpenAPI Specification or a manifest file. These explain in a machine-readable way how to use the extension. import vertexai import pprint from vertexai.preview.extensions import Extension PROJECT_ID = \"YOUR_PROJECT_ID\" REGION = \"us-central1\" vertexai.init(project=PROJECT_ID, location=REGION) extension_code_interpreter = Extension.from_hub(\"code_interpreter\") CODE_QUERY = \"\"\"Write a python method to invert a binary tree in O(n) time.\"\"\" response = extension_code_interpreter.execute( operation_id = \"generate_and_execute\", operation_params = {\"query\": CODE_QUERY} ) print(\"Generated Code:\") pprint.pprint({response['generated_code']})","title":"Extensions"},{"location":"agents/#functions","text":"In the world of software engineering, functions are defined as self-contained modules of code that accomplish a specific task and can be reused as needed. Functions differ from Extensions in a few ways, most notably: A model outputs a Function and its arguments, but doesn\u2019t make a live API call. Functions are executed on the client-side, while Extensions are executed on the agent-side. The logic and execution of calling the actual API endpoint is offloaded away from the agent and back to the client-side. This offers the developer more granular control over the flow. Common use cases: API calls need to be made at another layer of the application stack Security or authentication restrictions that prevent the agent from calling an API directly Additional data transformation logic needs to be applied to the API Response that the agent cannot perform.","title":"Functions"},{"location":"agents/#data-stores","text":"Data Stores address this limitation by providing access to more dynamic and up-to-date information, and ensuring a model\u2019s responses remain grounded in factuality and relevance. In the context of Generative AI agents, Data Stores are typically implemented as a vector database that the developer wants the agent to have access to at runtime.","title":"Data Stores"},{"location":"agents/#neural-turing-machine","text":"","title":"Neural Turing Machine"},{"location":"agents/#definition_1","text":"Coupling an ANN with an external memory in order to retrieve information through attention mechanism.","title":"Definition"},{"location":"agents/#attention-mechanisms","text":"Content-based Addressing \u2192 Softmax of key-vector \"k\" and memory storage Location-based Addressing \u2192 1-D Convolution between key-vector and memory storage","title":"Attention Mechanisms"},{"location":"deepeval/","text":"DeepEval Installation # Install pip install deepeval # Cloud login (e.g., Confident AI) By default, DeepEval uses OpenAI models for evaluation. Set the OpenAI key in OPENAI_API_KEY . It is possible to customise the back-end LLM. Set the output folder: # linux export DEEPEVAL_RESULTS_FOLDER=\"./data\" # or windows set DEEPEVAL_RESULTS_FOLDER=.\\data Commands # Run test deepeval test run test_example.py Additional Libraries The deepteam includes any security related testing LLM Evaluation General It is composed by: Test Cases from deepeval.test_case import LLMTestCase test_case = LLMTestCase( input=\"Who is the current president of the United States of America?\", actual_output=\"Joe Biden\", retrieval_context=[\"Joe Biden serves as the current president of America.\"] ) Metrics from deepeval.metrics import AnswerRelevancyMetric answer_relevancy_metric = AnswerRelevancyMetric() Evaluation Datasets (Check the dedicated section) Running a \"Test Run\": answer_relevancy_metric.measure(test_case) print(answer_relevancy_metric.score) Types End-to-end evaluation Component-level evaluation Metrics General There are two types of metrics: Out-of-the-Box stored in deepeval.metrics Custom metrics \u2192 defined by deepeval.metrics.GEval (Non-deterministic) or deepeval.metrics.DAGMetric (Deterministic) Datasets General They are evaluation datasets instance from EvaluationDataset (groups together multiple test cases of a same category) Either LLMTestCase or Goldens (no actual_output ) instances Goldens Allow for LLM output generation during evaluation time \u2192 That's why they don't have actual_output Serve as templates before becoming fully-formed test cases TestCases Single-Turn It tests a single, atomic unit of interaction, either between LLM's components or users. It can either implement an End-to-end evaluation or a Component-level evaluation. from deepeval.test_case import LLMTestCase, ToolCall test_case = LLMTestCase( input=\"What if these shoes don't fit?\", expected_output=\"You're eligible for a 30 day refund at no extra cost.\", actual_output=\"We offer a 30-day full refund at no extra cost.\", context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"], retrieval_context=[\"Only shoes can be refunded.\"], # Retrieved documents in a RAG tools_called=[ToolCall(name=\"WebSearch\")] ) Other useful parameters are: - token_cost - completion_time Tools The tools_called is a list of ToolCall objects, which are Pydantic types: class ToolCall(BaseModel): name: str description: Optional[str] = None reasoning: Optional[str] = None # How to use the tool output: Optional[Any] = None # Tool's output - Any data type input_parameters: Optional[Dict[str, Any]] = None An example: tools_called=[ ToolCall( name=\"Calculator Tool\" description=\"A tool that calculates mathematical equations or expressions.\", input={\"user_input\": \"2+3\"} output=5 ), ToolCall( name=\"WebSearch Tool\" reasoning=\"Knowledge base does not detail why the chicken crossed the road.\" input={\"search_query\": \"Why did the chicken crossed the road?\"} output=\"Because it wanted to, duh.\" ) ] MLLM Test Case An MLLMTestCase in deepeval is designed to unit test outputs from MLLM (Multimodal Large Language Model) applications. Example: from deepeval.test_case import MLLMTestCase, MLLMImage mllm_test_case = MLLMTestCase( # Replace this with your user input input=[\"Change the color of the shoes to blue.\", MLLMImage(url=\"./shoes.png\", local=True)] # Replace this with your actual MLLM application actual_output=[\"The original image of red shoes now shows the shoes in blue.\", MLLMImage(url=\"https://shoe-images.com/edited-shoes\", local=False)] ) Multi-Turn A multi-turn test case in deepeval is represented by a ConversationalTestCase , and has TWO parameters: turns chatbot_role # Turn class definition class Turn: role: Literal[\"user\", \"assistant\"] content: str user_id: Optional[str] = None retrieval_context: Optional[List[str]] = None tools_called: Optional[List[ToolCall]] = None additional_metadata: Optional[Dict] = None # Example from deepeval.test_case import Turn, ConversationalTestCase turns = [ Turn( role=\"assistant\", content=\"Why did the chicken cross the road?\", ), Turn( role=\"user\", content=\"Are you trying to be funny?\", ), ] test_case = ConversationalTestCase(turns=turns) Usage Creation from deepeval.test_case import LLMTestCase from deepeval.dataset import EvaluationDataset, Golden # Dataset creation from LLMTestCases first_test_case = LLMTestCase(input=\"...\", actual_output=\"...\") second_test_case = LLMTestCase(input=\"...\", actual_output=\"...\") dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case]) # Dataset creation from Goldens first_golden = Golden(input=\"...\") second_golden = Golden(input=\"...\") dataset_goldens = EvaluationDataset(goldens=[first_golden, second_golden]) print(dataset_goldens.goldens) # Append dataset.test_cases.append(test_case) # or dataset.add_test_case(test_case) Pull it from the Cloud from deepeval.dataset import EvaluationDataset from deepeval.metrics import AnswerRelevancyMetric dataset = EvaluationDataset() # supply your dataset alias dataset.pull(alias=\"QA Dataset\") evaluate(dataset, metrics=[AnswerRelevancyMetric()]) Generate synthetic data from deepeval.synthesizer import Synthesizer from deepeval.dataset import EvaluationDataset synthesizer = Synthesizer() goldens = synthesizer.generate_goldens_from_docs( document_paths=['example.txt', 'example.docx', 'example.pdf'] ) dataset = EvaluationDataset(goldens=goldens) Evaluate Function Evaluate over the entire dataset from deepeval.metrics import AnswerRelevancyMetric from deepeval import evaluate ... evaluate(dataset, [AnswerRelevancyMetric()]) Run the evaluation in parallel deepeval test run test_dataset.py -n 2 Loading # From JSON from deepeval.dataset import EvaluationDataset dataset = EvaluationDataset() # Add as test cases dataset.add_test_cases_from_json_file( # file_path is the absolute path to you .json file file_path=\"example.json\", input_key_name=\"query\", actual_output_key_name=\"actual_output\", expected_output_key_name=\"expected_output\", context_key_name=\"context\", retrieval_context_key_name=\"retrieval_context\", ) # Or, add as goldens dataset.add_goldens_from_json_file( # file_path is the absolute path to you .json file file_path=\"example.json\", input_key_name=\"query\" ) # From CSV # Add as test cases dataset.add_test_cases_from_csv_file( # file_path is the absolute path to you .csv file file_path=\"example.csv\", input_col_name=\"query\", actual_output_col_name=\"actual_output\", expected_output_col_name=\"expected_output\", context_col_name=\"context\", context_col_delimiter= \";\", retrieval_context_col_name=\"retrieval_context\", retrieval_context_col_delimiter= \";\" ) # Or, add as goldens dataset.add_goldens_from_csv_file( # file_path is the absolute path to you .csv file file_path=\"example.csv\", input_col_name=\"query\" ) PyTest Integration import pytest from deepeval import assert_test from deepeval.metrics import AnswerRelevancyMetric # Loop through test cases using Pytest @pytest.mark.parametrize( \"test_case\", dataset, ) def test_customer_chatbot(test_case: LLMTestCase): hallucination_metric = HallucinationMetric(threshold=0.3) answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5) assert_test(test_case, [hallucination_metric, answer_relevancy_metric]) @deepeval.on_test_run_end def function_to_be_called_after_test_run(): print(\"Test finished!\") End-to-End # Define you LLM application def your_llm_app(input: str): print(\"Call LLM!\") # Define the Dataset for evaluation goldens = [Golden(input=\"...\")] # Create the test cases test_case = [] for golden in goldens: res, text_chunks = your_llm_app(golden.input) # Call the LLM to generate the output and maybe a RAG context test_case = LLMTestCase(input=golden.input, actual_output=res, retrieval_context=text_chunks) # Evaluate end-to-end evaluate(test_cases=test_cases, metrics=[AnswerRelevancyMetric()]) Save # Locally dataset.save_as(file_type=\"csv\", directory=\"./deepeval-test-dataset\", include_test_cases=True) Code Snippets Basic Usage from deepeval import assert_test from deepeval.test_case import LLMTestCase, LLMTestCaseParams from deepeval.metrics import GEval def test_correctness(): correctness_metric = GEval( name=\"Correctness\", criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT], threshold=0.5 ) test_case = LLMTestCase( input=\"I have a persistent cough and fever. Should I be worried?\", # Replace this with the actual output from your LLM application actual_output=\"A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don't improve in a few days.\", expected_output=\"A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.\" ) assert_test(test_case, [correctness_metric]) # Possible to specify multiple metrics Observe Decorator It is used to evaluate and keep track of the evaluation of single app's components from deepeval.tracing import observe, update_current_span from deepeval.test_case import LLMTestCase from deepeval.dataset import Golden from deepeval.metrics import GEval from deepeval import evaluate correctness = GEval(name=\"Correctness\", criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]) @observe(metrics=[correctness]) def inner_component(): # Component can be anything from an LLM call, retrieval, agent, tool use, etc. update_current_span(test_case=LLMTestCase(input=\"...\", actual_output=\"...\")) return @observe def llm_app(input: str): inner_component() return evaluate(observed_callback=llm_app, goldens=[Golden(input=\"Hi!\")]) Evaluation Component Level Component-level evaluation assess individual units of LLM interaction between internal components such as retrievers, tool calls, LLM generations, or even agents interacting with other agents, rather than treating the LLM app as a black box. Including a Component Level evaluation implies tracing some part of your code. Example: from typing import List from openai import OpenAI from deepeval.tracing import observe, update_current_span from deepeval.test_case import LLMTestCase from deepeval.metrics import AnswerRelevancyMetric client = OpenAI() def your_llm_app(input: str): def retriever(input: str): return [\"Hardcoded text chunks from your vector database\"] @observe(metrics=[AnswerRelevancyMetric()]) def generator(input: str, retrieved_chunks: List[str]): res = client.chat.completions.create( model=\"gpt-4o\", messages=[ {\"role\": \"system\", \"content\": \"Use the provided context to answer the question.\"}, {\"role\": \"user\", \"content\": \"\\n\\n\".join(retrieved_chunks) + \"\\n\\nQuestion: \" + input} ] ).choices[0].message.content # Create test case at runtime update_current_span(test_case=LLMTestCase(input=input, actual_output=res)) return res return generator(input, retriever(input)) print(your_llm_app(\"How are you?\")) Run Evaluation from somewhere import your_llm_app # Replace with your LLM app from deepeval.dataset import Golden from deepeval import evaluate # Goldens from your dataset goldens = [Golden(input=\"...\")] # Evaluate with `observed_callback` evaluate(goldens=goldens, observed_callback=your_llm_app)","title":"DeepEval"},{"location":"deepeval/#deepeval","text":"","title":"DeepEval"},{"location":"deepeval/#installation","text":"# Install pip install deepeval # Cloud login (e.g., Confident AI) By default, DeepEval uses OpenAI models for evaluation. Set the OpenAI key in OPENAI_API_KEY . It is possible to customise the back-end LLM. Set the output folder: # linux export DEEPEVAL_RESULTS_FOLDER=\"./data\" # or windows set DEEPEVAL_RESULTS_FOLDER=.\\data","title":"Installation"},{"location":"deepeval/#commands","text":"# Run test deepeval test run test_example.py","title":"Commands"},{"location":"deepeval/#additional-libraries","text":"The deepteam includes any security related testing","title":"Additional Libraries"},{"location":"deepeval/#llm-evaluation","text":"","title":"LLM Evaluation"},{"location":"deepeval/#general","text":"It is composed by: Test Cases from deepeval.test_case import LLMTestCase test_case = LLMTestCase( input=\"Who is the current president of the United States of America?\", actual_output=\"Joe Biden\", retrieval_context=[\"Joe Biden serves as the current president of America.\"] ) Metrics from deepeval.metrics import AnswerRelevancyMetric answer_relevancy_metric = AnswerRelevancyMetric() Evaluation Datasets (Check the dedicated section) Running a \"Test Run\": answer_relevancy_metric.measure(test_case) print(answer_relevancy_metric.score)","title":"General"},{"location":"deepeval/#types","text":"End-to-end evaluation Component-level evaluation","title":"Types"},{"location":"deepeval/#metrics","text":"","title":"Metrics"},{"location":"deepeval/#general_1","text":"There are two types of metrics: Out-of-the-Box stored in deepeval.metrics Custom metrics \u2192 defined by deepeval.metrics.GEval (Non-deterministic) or deepeval.metrics.DAGMetric (Deterministic)","title":"General"},{"location":"deepeval/#datasets","text":"","title":"Datasets"},{"location":"deepeval/#general_2","text":"They are evaluation datasets instance from EvaluationDataset (groups together multiple test cases of a same category) Either LLMTestCase or Goldens (no actual_output ) instances","title":"General"},{"location":"deepeval/#goldens","text":"Allow for LLM output generation during evaluation time \u2192 That's why they don't have actual_output Serve as templates before becoming fully-formed test cases","title":"Goldens"},{"location":"deepeval/#testcases","text":"","title":"TestCases"},{"location":"deepeval/#single-turn","text":"It tests a single, atomic unit of interaction, either between LLM's components or users. It can either implement an End-to-end evaluation or a Component-level evaluation. from deepeval.test_case import LLMTestCase, ToolCall test_case = LLMTestCase( input=\"What if these shoes don't fit?\", expected_output=\"You're eligible for a 30 day refund at no extra cost.\", actual_output=\"We offer a 30-day full refund at no extra cost.\", context=[\"All customers are eligible for a 30 day full refund at no extra cost.\"], retrieval_context=[\"Only shoes can be refunded.\"], # Retrieved documents in a RAG tools_called=[ToolCall(name=\"WebSearch\")] ) Other useful parameters are: - token_cost - completion_time","title":"Single-Turn"},{"location":"deepeval/#tools","text":"The tools_called is a list of ToolCall objects, which are Pydantic types: class ToolCall(BaseModel): name: str description: Optional[str] = None reasoning: Optional[str] = None # How to use the tool output: Optional[Any] = None # Tool's output - Any data type input_parameters: Optional[Dict[str, Any]] = None An example: tools_called=[ ToolCall( name=\"Calculator Tool\" description=\"A tool that calculates mathematical equations or expressions.\", input={\"user_input\": \"2+3\"} output=5 ), ToolCall( name=\"WebSearch Tool\" reasoning=\"Knowledge base does not detail why the chicken crossed the road.\" input={\"search_query\": \"Why did the chicken crossed the road?\"} output=\"Because it wanted to, duh.\" ) ]","title":"Tools"},{"location":"deepeval/#mllm-test-case","text":"An MLLMTestCase in deepeval is designed to unit test outputs from MLLM (Multimodal Large Language Model) applications. Example: from deepeval.test_case import MLLMTestCase, MLLMImage mllm_test_case = MLLMTestCase( # Replace this with your user input input=[\"Change the color of the shoes to blue.\", MLLMImage(url=\"./shoes.png\", local=True)] # Replace this with your actual MLLM application actual_output=[\"The original image of red shoes now shows the shoes in blue.\", MLLMImage(url=\"https://shoe-images.com/edited-shoes\", local=False)] )","title":"MLLM Test Case"},{"location":"deepeval/#multi-turn","text":"A multi-turn test case in deepeval is represented by a ConversationalTestCase , and has TWO parameters: turns chatbot_role # Turn class definition class Turn: role: Literal[\"user\", \"assistant\"] content: str user_id: Optional[str] = None retrieval_context: Optional[List[str]] = None tools_called: Optional[List[ToolCall]] = None additional_metadata: Optional[Dict] = None # Example from deepeval.test_case import Turn, ConversationalTestCase turns = [ Turn( role=\"assistant\", content=\"Why did the chicken cross the road?\", ), Turn( role=\"user\", content=\"Are you trying to be funny?\", ), ] test_case = ConversationalTestCase(turns=turns)","title":"Multi-Turn"},{"location":"deepeval/#usage","text":"","title":"Usage"},{"location":"deepeval/#creation","text":"from deepeval.test_case import LLMTestCase from deepeval.dataset import EvaluationDataset, Golden # Dataset creation from LLMTestCases first_test_case = LLMTestCase(input=\"...\", actual_output=\"...\") second_test_case = LLMTestCase(input=\"...\", actual_output=\"...\") dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case]) # Dataset creation from Goldens first_golden = Golden(input=\"...\") second_golden = Golden(input=\"...\") dataset_goldens = EvaluationDataset(goldens=[first_golden, second_golden]) print(dataset_goldens.goldens) # Append dataset.test_cases.append(test_case) # or dataset.add_test_case(test_case) Pull it from the Cloud from deepeval.dataset import EvaluationDataset from deepeval.metrics import AnswerRelevancyMetric dataset = EvaluationDataset() # supply your dataset alias dataset.pull(alias=\"QA Dataset\") evaluate(dataset, metrics=[AnswerRelevancyMetric()]) Generate synthetic data from deepeval.synthesizer import Synthesizer from deepeval.dataset import EvaluationDataset synthesizer = Synthesizer() goldens = synthesizer.generate_goldens_from_docs( document_paths=['example.txt', 'example.docx', 'example.pdf'] ) dataset = EvaluationDataset(goldens=goldens)","title":"Creation"},{"location":"deepeval/#evaluate-function","text":"Evaluate over the entire dataset from deepeval.metrics import AnswerRelevancyMetric from deepeval import evaluate ... evaluate(dataset, [AnswerRelevancyMetric()]) Run the evaluation in parallel deepeval test run test_dataset.py -n 2","title":"Evaluate Function"},{"location":"deepeval/#loading","text":"# From JSON from deepeval.dataset import EvaluationDataset dataset = EvaluationDataset() # Add as test cases dataset.add_test_cases_from_json_file( # file_path is the absolute path to you .json file file_path=\"example.json\", input_key_name=\"query\", actual_output_key_name=\"actual_output\", expected_output_key_name=\"expected_output\", context_key_name=\"context\", retrieval_context_key_name=\"retrieval_context\", ) # Or, add as goldens dataset.add_goldens_from_json_file( # file_path is the absolute path to you .json file file_path=\"example.json\", input_key_name=\"query\" ) # From CSV # Add as test cases dataset.add_test_cases_from_csv_file( # file_path is the absolute path to you .csv file file_path=\"example.csv\", input_col_name=\"query\", actual_output_col_name=\"actual_output\", expected_output_col_name=\"expected_output\", context_col_name=\"context\", context_col_delimiter= \";\", retrieval_context_col_name=\"retrieval_context\", retrieval_context_col_delimiter= \";\" ) # Or, add as goldens dataset.add_goldens_from_csv_file( # file_path is the absolute path to you .csv file file_path=\"example.csv\", input_col_name=\"query\" )","title":"Loading"},{"location":"deepeval/#pytest-integration","text":"import pytest from deepeval import assert_test from deepeval.metrics import AnswerRelevancyMetric # Loop through test cases using Pytest @pytest.mark.parametrize( \"test_case\", dataset, ) def test_customer_chatbot(test_case: LLMTestCase): hallucination_metric = HallucinationMetric(threshold=0.3) answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5) assert_test(test_case, [hallucination_metric, answer_relevancy_metric]) @deepeval.on_test_run_end def function_to_be_called_after_test_run(): print(\"Test finished!\")","title":"PyTest Integration"},{"location":"deepeval/#end-to-end","text":"# Define you LLM application def your_llm_app(input: str): print(\"Call LLM!\") # Define the Dataset for evaluation goldens = [Golden(input=\"...\")] # Create the test cases test_case = [] for golden in goldens: res, text_chunks = your_llm_app(golden.input) # Call the LLM to generate the output and maybe a RAG context test_case = LLMTestCase(input=golden.input, actual_output=res, retrieval_context=text_chunks) # Evaluate end-to-end evaluate(test_cases=test_cases, metrics=[AnswerRelevancyMetric()])","title":"End-to-End"},{"location":"deepeval/#save","text":"# Locally dataset.save_as(file_type=\"csv\", directory=\"./deepeval-test-dataset\", include_test_cases=True)","title":"Save"},{"location":"deepeval/#code-snippets","text":"","title":"Code Snippets"},{"location":"deepeval/#basic-usage","text":"from deepeval import assert_test from deepeval.test_case import LLMTestCase, LLMTestCaseParams from deepeval.metrics import GEval def test_correctness(): correctness_metric = GEval( name=\"Correctness\", criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT], threshold=0.5 ) test_case = LLMTestCase( input=\"I have a persistent cough and fever. Should I be worried?\", # Replace this with the actual output from your LLM application actual_output=\"A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don't improve in a few days.\", expected_output=\"A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs.\" ) assert_test(test_case, [correctness_metric]) # Possible to specify multiple metrics","title":"Basic Usage"},{"location":"deepeval/#observe-decorator","text":"It is used to evaluate and keep track of the evaluation of single app's components from deepeval.tracing import observe, update_current_span from deepeval.test_case import LLMTestCase from deepeval.dataset import Golden from deepeval.metrics import GEval from deepeval import evaluate correctness = GEval(name=\"Correctness\", criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]) @observe(metrics=[correctness]) def inner_component(): # Component can be anything from an LLM call, retrieval, agent, tool use, etc. update_current_span(test_case=LLMTestCase(input=\"...\", actual_output=\"...\")) return @observe def llm_app(input: str): inner_component() return evaluate(observed_callback=llm_app, goldens=[Golden(input=\"Hi!\")])","title":"Observe Decorator"},{"location":"deepeval/#evaluation","text":"","title":"Evaluation"},{"location":"deepeval/#component-level","text":"Component-level evaluation assess individual units of LLM interaction between internal components such as retrievers, tool calls, LLM generations, or even agents interacting with other agents, rather than treating the LLM app as a black box. Including a Component Level evaluation implies tracing some part of your code. Example: from typing import List from openai import OpenAI from deepeval.tracing import observe, update_current_span from deepeval.test_case import LLMTestCase from deepeval.metrics import AnswerRelevancyMetric client = OpenAI() def your_llm_app(input: str): def retriever(input: str): return [\"Hardcoded text chunks from your vector database\"] @observe(metrics=[AnswerRelevancyMetric()]) def generator(input: str, retrieved_chunks: List[str]): res = client.chat.completions.create( model=\"gpt-4o\", messages=[ {\"role\": \"system\", \"content\": \"Use the provided context to answer the question.\"}, {\"role\": \"user\", \"content\": \"\\n\\n\".join(retrieved_chunks) + \"\\n\\nQuestion: \" + input} ] ).choices[0].message.content # Create test case at runtime update_current_span(test_case=LLMTestCase(input=input, actual_output=res)) return res return generator(input, retriever(input)) print(your_llm_app(\"How are you?\"))","title":"Component Level"},{"location":"deepeval/#run-evaluation","text":"from somewhere import your_llm_app # Replace with your LLM app from deepeval.dataset import Golden from deepeval import evaluate # Goldens from your dataset goldens = [Golden(input=\"...\")] # Evaluate with `observed_callback` evaluate(goldens=goldens, observed_callback=your_llm_app)","title":"Run Evaluation"},{"location":"embeddings/","text":"Embeddings Definition Embeddings are numerical representations of real-world data such as text, speech, image, or videos. They are expressed as low-dimensional vectors where the geometric distances of two vectors in the vector space is a projection of the relationships between the two real-world objects that the vectors represent. Such low dimensional representation try to preserve the most of the \"essential information\" of the original objects. Ideally the embeddings are created, so they place objects with similar semantic properties closer in the embedding space. The Embeddings are usually obtained through different ML models, like Encoder-based Transformers such as BERT. Usage Semantic Search Precomputing the embeddings for billions items of the search space. Mapping query embeddings to the same embedding space. Efficient computing and retrieving of the nearest neighbors of the query embeddings in the search space. Applications Retrieval Recommendations Features for ML Models Types of Embeddings Text Embeddings Definition They are created through a process: 1. Tokenisation 2. indexing 3. Embedding Word Embeddings Usage Word embeddings can be directly used in some downstream tasks like Named Entity Recognition (NER). Technologies GloVe SWIVEL Word2Vec Word2Vec Word2Vec is a family of model architectures that operates on the principle of \u201cthe semantic meaning of a word is defined by its neighbors\u201d. It uses a matrix of shape (size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table after the training process is completed using one of the following methods: - The Continuous bag of words (CBOW) is fast to train and is slightly more accurate for frequent words. - The skip-gram is inverse of that of CBOW, with the middle word being used to predict the surrounding words within a certain range. This approach is slower to train but works well with small data and is more accurate for rare words. GloVe It uses a co-occurrence matrix, which represents the relationships between words. Then GloVe then uses a factorization technique to learn word representations from the co-occurrence matrix. SWIVEL Unlike GloVE, it uses local windows to learn the word vectors by taking into account the co-occurrence of words within a fixed window of its neighboring words. It is slightly less accurate than GloVe on average, but is considerably faster to train. Document Embeddings Definition The evolution of the embeddings models can mainly be categorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained large language models (e.g., BERT). Shallow BoW Models Early document embedding works follow the bag-of-words (BoW) paradigm, assuming a document is an unordered collection of words. These early works include latent semantic analysis (LSA)7 and latent dirichlet allocation (LDA). Another famous bag-of-words family of document embeddings is TF-IDF. It has two major weaknesses: both the word ordering and the semantic meanings are ignored. BoW models fail to capture the sequential relationships between words. Image & Multimodal Embeddings Computation Unimodal image embeddings can be derived in many ways: one of which is by training a CNN or Vision Transformer model on a large scale image classification task (for example, Imagenet), and then using the penultimate layer as the image embedding. Structured Data Embeddings Definition Unlike unstructured data, where a pre-trained embedding model is typically available, we have to create the embedding model for the structured data since it would be specific to a particular application. General Computation Use dimensionality reductions techniques such as PCA. Graph Embeddings Definition Graph embeddings are another embedding technique that lets you represent not only information about a specific object but also its neighbors. Training Two Tower Architecture Current embedding models usually use dual encoder (two tower) architecture. For example, for the text embedding model used in question-answering, one tower is used to encode the queries and the other tower is used to encode the documents. The training includes a pretraining (unsupervised learning) and fine tuning (supervised learning). Nowadays, the embedding models are usually directly initialized from foundation models such as BERT, T5, GPT, Gemini, CoCa. Applications Semi-Categorical Text Suppose you have a column \"Location\" with entries that are not coherent (e.g., \"US\", \"USA\", United States\", etc. It is possible to use directly the Text Embeddings possibility Text Embeddings SentenceTransformer The solution is preferable when a plug-and-play model is needed. It does not require pre-tokenisation and post-distillation (e.g., MeanPooling) PCA can be applied to the output from sentence_transformers import SentenceTransformer from sklearn.decomposition import PCA model = SentenceTransformer(\"all-MiniLM-L6-v2\") sentence_embeddings = model.encode(texts, convert_to_numpy=True) pca = PCA(n_components=64) reduced_embeddings = pca.fit_transform(sentence_embeddings) AutoTokenizer + AutoModel The solution is preferable when it is needed more customisation and control over the embedding process It requires pre-tokenisation and post-distillation PCA can be applied to the output from transformers import AutoTokenizer, AutoModel import torch from sklearn.decomposition import PCA tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') inputs = tokenizer(\"This is a sentence.\", return_tensors='pt') outputs = model(**inputs) # Get token embeddings (shape: [1, seq_len, hidden_dim]) token_embeddings = outputs.last_hidden_state # Mean pooling (manual) attention_mask = inputs['attention_mask'] mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() pooled = torch.sum(token_embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9) pca = PCA(n_components=64) reduced_embeddings = pca.fit_transform(pooled)","title":"Embeddings"},{"location":"embeddings/#embeddings","text":"","title":"Embeddings"},{"location":"embeddings/#definition","text":"Embeddings are numerical representations of real-world data such as text, speech, image, or videos. They are expressed as low-dimensional vectors where the geometric distances of two vectors in the vector space is a projection of the relationships between the two real-world objects that the vectors represent. Such low dimensional representation try to preserve the most of the \"essential information\" of the original objects. Ideally the embeddings are created, so they place objects with similar semantic properties closer in the embedding space. The Embeddings are usually obtained through different ML models, like Encoder-based Transformers such as BERT.","title":"Definition"},{"location":"embeddings/#usage","text":"","title":"Usage"},{"location":"embeddings/#semantic-search","text":"Precomputing the embeddings for billions items of the search space. Mapping query embeddings to the same embedding space. Efficient computing and retrieving of the nearest neighbors of the query embeddings in the search space.","title":"Semantic Search"},{"location":"embeddings/#applications","text":"Retrieval Recommendations Features for ML Models","title":"Applications"},{"location":"embeddings/#types-of-embeddings","text":"","title":"Types of Embeddings"},{"location":"embeddings/#text-embeddings","text":"","title":"Text Embeddings"},{"location":"embeddings/#definition_1","text":"They are created through a process: 1. Tokenisation 2. indexing 3. Embedding","title":"Definition"},{"location":"embeddings/#word-embeddings","text":"","title":"Word Embeddings"},{"location":"embeddings/#usage_1","text":"Word embeddings can be directly used in some downstream tasks like Named Entity Recognition (NER).","title":"Usage"},{"location":"embeddings/#technologies","text":"GloVe SWIVEL Word2Vec","title":"Technologies"},{"location":"embeddings/#word2vec","text":"Word2Vec is a family of model architectures that operates on the principle of \u201cthe semantic meaning of a word is defined by its neighbors\u201d. It uses a matrix of shape (size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table after the training process is completed using one of the following methods: - The Continuous bag of words (CBOW) is fast to train and is slightly more accurate for frequent words. - The skip-gram is inverse of that of CBOW, with the middle word being used to predict the surrounding words within a certain range. This approach is slower to train but works well with small data and is more accurate for rare words.","title":"Word2Vec"},{"location":"embeddings/#glove","text":"It uses a co-occurrence matrix, which represents the relationships between words. Then GloVe then uses a factorization technique to learn word representations from the co-occurrence matrix.","title":"GloVe"},{"location":"embeddings/#swivel","text":"Unlike GloVE, it uses local windows to learn the word vectors by taking into account the co-occurrence of words within a fixed window of its neighboring words. It is slightly less accurate than GloVe on average, but is considerably faster to train.","title":"SWIVEL"},{"location":"embeddings/#document-embeddings","text":"","title":"Document Embeddings"},{"location":"embeddings/#definition_2","text":"The evolution of the embeddings models can mainly be categorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained large language models (e.g., BERT).","title":"Definition"},{"location":"embeddings/#shallow-bow-models","text":"Early document embedding works follow the bag-of-words (BoW) paradigm, assuming a document is an unordered collection of words. These early works include latent semantic analysis (LSA)7 and latent dirichlet allocation (LDA). Another famous bag-of-words family of document embeddings is TF-IDF. It has two major weaknesses: both the word ordering and the semantic meanings are ignored. BoW models fail to capture the sequential relationships between words.","title":"Shallow BoW Models"},{"location":"embeddings/#image-multimodal-embeddings","text":"","title":"Image &amp; Multimodal Embeddings"},{"location":"embeddings/#computation","text":"Unimodal image embeddings can be derived in many ways: one of which is by training a CNN or Vision Transformer model on a large scale image classification task (for example, Imagenet), and then using the penultimate layer as the image embedding.","title":"Computation"},{"location":"embeddings/#structured-data-embeddings","text":"","title":"Structured Data Embeddings"},{"location":"embeddings/#definition_3","text":"Unlike unstructured data, where a pre-trained embedding model is typically available, we have to create the embedding model for the structured data since it would be specific to a particular application.","title":"Definition"},{"location":"embeddings/#general-computation","text":"Use dimensionality reductions techniques such as PCA.","title":"General Computation"},{"location":"embeddings/#graph-embeddings","text":"","title":"Graph Embeddings"},{"location":"embeddings/#definition_4","text":"Graph embeddings are another embedding technique that lets you represent not only information about a specific object but also its neighbors.","title":"Definition"},{"location":"embeddings/#training","text":"","title":"Training"},{"location":"embeddings/#two-tower-architecture","text":"Current embedding models usually use dual encoder (two tower) architecture. For example, for the text embedding model used in question-answering, one tower is used to encode the queries and the other tower is used to encode the documents. The training includes a pretraining (unsupervised learning) and fine tuning (supervised learning). Nowadays, the embedding models are usually directly initialized from foundation models such as BERT, T5, GPT, Gemini, CoCa.","title":"Two Tower Architecture"},{"location":"embeddings/#applications_1","text":"","title":"Applications"},{"location":"embeddings/#semi-categorical-text","text":"Suppose you have a column \"Location\" with entries that are not coherent (e.g., \"US\", \"USA\", United States\", etc. It is possible to use directly the Text Embeddings possibility","title":"Semi-Categorical Text"},{"location":"embeddings/#text-embeddings_1","text":"","title":"Text Embeddings"},{"location":"embeddings/#sentencetransformer","text":"The solution is preferable when a plug-and-play model is needed. It does not require pre-tokenisation and post-distillation (e.g., MeanPooling) PCA can be applied to the output from sentence_transformers import SentenceTransformer from sklearn.decomposition import PCA model = SentenceTransformer(\"all-MiniLM-L6-v2\") sentence_embeddings = model.encode(texts, convert_to_numpy=True) pca = PCA(n_components=64) reduced_embeddings = pca.fit_transform(sentence_embeddings)","title":"SentenceTransformer"},{"location":"embeddings/#autotokenizer-automodel","text":"The solution is preferable when it is needed more customisation and control over the embedding process It requires pre-tokenisation and post-distillation PCA can be applied to the output from transformers import AutoTokenizer, AutoModel import torch from sklearn.decomposition import PCA tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') inputs = tokenizer(\"This is a sentence.\", return_tensors='pt') outputs = model(**inputs) # Get token embeddings (shape: [1, seq_len, hidden_dim]) token_embeddings = outputs.last_hidden_state # Mean pooling (manual) attention_mask = inputs['attention_mask'] mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() pooled = torch.sum(token_embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9) pca = PCA(n_components=64) reduced_embeddings = pca.fit_transform(pooled)","title":"AutoTokenizer + AutoModel"},{"location":"evaluation/","text":"LLMs Evaluation Introduction Areas of Evaluation An evaluation framework for LLMs should target two main areas: Use Case or Dynamic Behavior Evaluation - Custom metrics that directly measure how well the LLM is performing regarding the specific task System Architecture - Generic metrics on, for example, faithfulness of information retrieved by the RAG or the Task Completion for AI Agents LLM Standalone Metrics These metrics are related to evaluate LLM against standardised benchmarks: GLUE SyperGLUE HellaSwag TruthfulQA MMLU Online vs. Offline Evaluation Offline evaluation usually proves valuable in the initial development stages of features, but it falls short in assessing how model changes impact the user experience in a live production environment. Offline - Offline evaluation scrutinizes LLMs against specific datasets Online - Test the E2E system Evaluation Methodologies LLM Evaluation Metrics General List Answer Relevancy - Determines whether an LLM output is able to address the given input and certain context and rules \u2705 Task Completion - Determines whether an LLM agent is able to complete the task it was set out to do \u26a0\ufe0f \u2192 How to determine completion state? Correctness - Determines whether an LLM output is factually correct based on some ground truth \u2705 Hallucination - Determines whether an LLM output contains fake or made-up information \u274c \u2192 Impossible to determine Tool Correctness - Determines whether an LLM agent is able to call the correct tools for a given task \u26a0\ufe0f \u2192 How to determine if the tool is correct? Contextual Relevancy - Determines whether the retriever in a RAG-based LLM system is able to extract the most relevant information for your LLM as context \u2705 \u2192 Similar to the first one, but on each retrieved document Responsible Metrics - Includes metrics such as bias and toxicity, which determines whether an LLM output contains (generally) harmful and offensive content \u2705 Task-Specific Metrics - Includes metrics such as summarization, which usually contains a custom criteria depending on the use-case \u2705 Metrics Types Some metrics are based on Statistics, while others are sometimes referred as \"Model-based\" : Statistics Metrics They might perform poorly when the output implies reasoning capabilities (No semantic is included) They do not take into account any List of Metrics: BLEU (BiLingual Evaluation Understudy) - It evaluates the output of the LLM application against annotated ground truths. It calculates the precision for each matching n-gram (n consecutive words) between an LLM output and expected output to calculate their geometric mean and applies a brevity penalty if needed. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) - It is used for text summarisation and calculates recall by comparing the overlap of n-grams between LLM outputs and expected outputs. It also leverages external linguistic databases like WordNet to account for synonyms. The final score is the harmonic mean of precision and recall, with a penalty for ordering discrepancies. METEOR (Metric for Evaluation of Translation with Explicit Ordering) - It calculates scores by assessing both precision (n-gram matches) and recall (n-gram overlaps), adjusted for word order differences between LLM outputs and expected outputs. It can also leverages exteral linguistic databases. Levenshtein distance Model-based Metrics Reliable but inaccurate (struggle to keep semantic included), because of their probabilistic nature List of Metrics: NLI - It is a Non-LLM based and uses Natural Language Inference models to classify whether an LLM output is logically consistent (entailment), contradictory, or unrelated (neutral) with respect to a given reference text. BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) - It uses pre-trained models like BERT to score LLM outputs on some expected outputs Statistical and Model-Based Scorers BERTScore - It relies on a pre-trained LLM like BERT and on the cosine similarity between expected output and predicted output. Afterward, the similarities are aggregated to produce a final score. MoverScore - It relies on LLM like BERT to obtain deeper contextualised word embeddings for both reference text and generated text before computing the similarity. Usage Tips It is good to have: 1-2 custom metrics (G-Eval or DAG) that are use case specific 2-3 generic metrics (RAG, agentic, or conversational) that are system specific G-Eval (Model-based Scorer) Introduction It is an LLM-based Scorer ( Paper ) Documentation from DeepEval Process Prompt with the following information: 1) Task Introduction; 2) Evaluation Criteria Generate through the previous output the list of Evaluation Steps through the \"Auto Chain of Thoughts\" Prompt the Scorer LLM with Evaluation Steps, Input Context and Input Target (Optional) Normalise the output score by the probabilities of the output tokens Code Snippets from deepeval.test_case import LLMTestCase, LLMTestCaseParams from deepeval.metrics import GEval test_case = LLMTestCase(input=\"input to your LLM\", actual_output=\"your LLM output\") coherence_metric = GEval( name=\"Coherence\", criteria=\"Coherence - the collective quality of all sentences in the actual output\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT], ) coherence_metric.measure(test_case) print(coherence_metric.score) print(coherence_metric.reason) DAG (Model-based Scorer) Introduction Deep Acyclic Graph is a LLM-based scorer that relies on a decision tree Each node is an LLM Judgement and each edge is a decision Each leaf node is associated with a hardcoded score Advantages Slightly more deterministic, since there's a certain degree of control in the score determination It can be used to filter away edge cases where LLM output doesn't even meet minimum requirements Code Snippets from deepeval.test_case import LLMTestCase from deepeval.metrics.dag import ( DeepAcyclicGraph, TaskNode, BinaryJudgementNode, NonBinaryJudgementNode, VerdictNode, ) from deepeval.metrics import DAGMetric correct_order_node = NonBinaryJudgementNode( criteria=\"Are the summary headings in the correct order: 'intro' => 'body' => 'conclusion'?\", children=[ VerdictNode(verdict=\"Yes\", score=10), VerdictNode(verdict=\"Two are out of order\", score=4), VerdictNode(verdict=\"All out of order\", score=2), ], ) correct_headings_node = BinaryJudgementNode( criteria=\"Does the summary headings contain all three: 'intro', 'body', and 'conclusion'?\", children=[ VerdictNode(verdict=False, score=0), VerdictNode(verdict=True, child=correct_order_node), ], ) extract_headings_node = TaskNode( instructions=\"Extract all headings in `actual_output`\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT], output_label=\"Summary headings\", children=[correct_headings_node, correct_order_node], ) # create the DAG dag = DeepAcyclicGraph(root_nodes=[extract_headings_node]) # create the metric format_correctness = DAGMetric(name=\"Format Correctness\", dag=dag) # create a test case test_case = LLMTestCase(input=\"your-original-text\", actual_output=\"your-summary\") # evaluate format_correctness.measure(test_case) print(format_correctness.score, format_correctness.reason) Prometheus (Model-based Scorer) Introduction LLM-based evaluation framework use case agnostic Based con Llama-2-chat and fine-tuned for evaluation purposes Advantages Evaluation steps are not produced by LLM, but are embedded in the node itself QAG Score (Hybrid Scorer) Introduction QAG (Question Answer Generation) Score uses binary answer (\u2018yes\u2019 or \u2018no\u2019) to close-ended questions (which can be generated or preset) to compute a final metric score. Example: So for this example LLM output: Martin Luther King Jr., the renowned civil rights leader, was assassinated on April 4, 1968, at the Lorraine Motel in Memphis, Tennessee. He was in Memphis to support striking sanitation workers and was fatally shot by James Earl Ray, an escaped convict, while standing on the motel\u2019s second-floor balcony. A claim would be: Martin Luther King Jr. assassinated on the April 4, 1968 And a corresponding close-ended question would be: Was Martin Luther King Jr. assassinated on the April 4, 1968? Advantages The score is not directly generated by an LLM GPTScore Introduction It is similar to G-Eval, but the evaluation trask is performed with a form-filling paradigm. SelfCheckGPT Introduction It samples multiple output in order to detect hallucinations through a model-based approach. RAG-Specific Metrics Faithfulness It evaluates whether the Generator is generating output that factually aligns with the information presented from the Retriever. It is possible to use a QAG Score here. For faithfulness, if you define it as the proportion of truthful claims made in an LLM output with regards to the retrieval context, we can calculate faithfulness using QAG by following this algorithm: Use LLMs to extract all claims made in the output. For each claim, check whether the it agrees or contradicts with each individual node in the retrieval context. In this case, the close-ended question in QAG will be something like: \u201cDoes the given claim agree with the reference text\u201d, where the \u201creference text\u201d will be each individual retrieved node. (Note that you need to confine the answer to either a \u2018yes\u2019, \u2018no\u2019, or \u2018idk\u2019. The \u2018idk\u2019 state represents the edge case where the retrieval context does not contain relevant information to give a yes/no answer.) Add up the total number of truthful claims (\u2018yes\u2019 and \u2018idk\u2019), and divide it by the total number of claims made. from deepeval.metrics import FaithfulnessMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", retrieval_context=[\"...\"] ) metric = FaithfulnessMetric(threshold=0.5) metric.measure(test_case) print(metric.score) print(metric.reason) print(metric.is_successful()) Answer Relevancy It assesses whether RAG generator outputs concise answers, and can be calculated by determining the proportion of sentences in an LLM output that a relevant to the input. from deepeval.metrics import AnswerRelevancyMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", retrieval_context=[\"...\"] ) metric = AnswerRelevancyMetric(threshold=0.5) metric.measure(test_case) print(metric.score) print(metric.reason) print(metric.is_successful()) Contextual Precision Contextual Precision is a RAG metric that assesses the quality of your RAG pipeline\u2019s retriever. from deepeval.metrics import ContextualPrecisionMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", # Expected output is the \"ideal\" output of your LLM, it is an # extra parameter that's needed for contextual metrics expected_output=\"...\", retrieval_context=[\"...\"] ) metric = ContextualPrecisionMetric(threshold=0.5) # Or ContextualRecallMetric metric.measure(test_case) print(metric.score) print(metric.reason) print(metric.is_successful()) Agentic Metrics Tool Correctness Tool correctness is an agentic metric that assesses the quality of your agentic systems, and is the most unusual metric here because it is based on exact matching and not any LLM-as-a-judge. It is computed by comparing the tools called for a given input to the expected tools that should be called. from deepeval.test_case import LLMTestCase, ToolCall from deepeval.metrics import ToolCorrectnessMetric test_case = LLMTestCase( input=\"What if these shoes don't fit?\", actual_output=\"We offer a 30-day full refund at no extra cost.\", # Replace this with the tools that was actually used by your LLM agent tools_called=[ToolCall(name=\"WebSearch\"), ToolCall(name=\"ToolQuery\")], expected_tools=[ToolCall(name=\"WebSearch\")], ) metric = ToolCorrectnessMetric() metric.measure(test_case) print(metric.score, metric.reason) Task Completion Task completion is an agentic metric that uses LLM-as-a-judge to evaluate whether your LLM agent is able to accomplish its given task. from deepeval.test_case import LLMTestCase from deepeval.metrics import TaskCompletionMetric metric = TaskCompletionMetric( threshold=0.7, model=\"gpt-4o\", include_reason=True ) test_case = LLMTestCase( input=\"Plan a 3-day itinerary for Paris with cultural landmarks and local cuisine.\", actual_output=( \"Day 1: Eiffel Tower, dinner at Le Jules Verne. \" \"Day 2: Louvre Museum, lunch at Angelina Paris. \" \"Day 3: Montmartre, evening at a wine bar.\" ), tools_called=[ ToolCall( name=\"Itinerary Generator\", description=\"Creates travel plans based on destination and duration.\", input_parameters={\"destination\": \"Paris\", \"days\": 3}, output=[ \"Day 1: Eiffel Tower, Le Jules Verne.\", \"Day 2: Louvre Museum, Angelina Paris.\", \"Day 3: Montmartre, wine bar.\", ], ), ToolCall( name=\"Restaurant Finder\", description=\"Finds top restaurants in a city.\", input_parameters={\"city\": \"Paris\"}, output=[\"Le Jules Verne\", \"Angelina Paris\", \"local wine bars\"], ), ], ) metric.measure(test_case) print(metric.score, metric.reason) LLM Metrics Hallucination from deepeval.metrics import HallucinationMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", # Note that 'context' is not the same as 'retrieval_context'. # While retrieval context is more concerned with RAG pipelines, # context is the ideal retrieval results for a given input, # and typically resides in the dataset used to fine-tune your LLM context=[\"...\"], ) metric = HallucinationMetric(threshold=0.5) metric.measure(test_case) print(metric.score) print(metric.is_successful()) Toxicity from deepeval.metrics import ToxicityMetric from deepeval.test_case import LLMTestCase metric = ToxicityMetric(threshold=0.5) test_case = LLMTestCase( input=\"What if these shoes don't fit?\", # Replace this with the actual output from your LLM application actual_output = \"We offer a 30-day full refund at no extra cost.\" ) metric.measure(test_case) print(metric.score) Bias The bias metric evaluates aspects such as political, gender, and social biases in textual content. from deepeval.metrics import GEval from deepeval.test_case import LLMTestCase test_case = LLMTestCase( input=\"What if these shoes don't fit?\", # Replace this with the actual output from your LLM application actual_output = \"We offer a 30-day full refund at no extra cost.\" ) toxicity_metric = GEval( name=\"Bias\", criteria=\"Bias - determine if the actual output contains any racial, gender, or political bias.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT], ) metric.measure(test_case) print(metric.score) Evaluation Framework Platforms Azure AI Foundry Automated Evaluation (Microsoft) Azure AI Foundry is an all-in-one AI platform for building, evaluating, and deploying generative AI solutions and custom copilots. OpenAI Evals (OpenAI) The OpenAI Evals framework consists of a framework to evaluate an LLM or a system built on top of an LLM, and an open-source registry of challenging evals. It is based on pre-defined dataset. The process to create a new evaluation dataset is described in custom-eval.md from the OpenAI Evals package. Although it seems pretty rigid and too schematic for the actual standards. Weights & Biases A Machine Learning platform to quickly track experiments, version and iterate on datasets, evaluate model performance. LangSmith (LangChain) Trace and evaluate language model applications and intelligent agents. TruLens (TruEra) TruLens provides a set of tools for developing and monitoring neural nets, including LLMs. Vertex AI Studio (Google) You can evaluate the performance of foundation models and your tuned generative AI models on Vertex AI. Amazon Bedrock Amazon Bedrock supports model evaluation jobs. DeepEval (Confident AI) An open-source LLM evaluation framework for LLM applications. Sample Codes . Parea AI Parea provides tools for debugging, testing, evaluating, and monitoring LLM-powered applications. Optik by Comet Opik is an open-source platform by Comet for evaluating, testing, and monitoring Large Language Models (LLMs). It provides flexible tools to track, annotate, and refine LLM applications across development and production environments. from opik.evaluation.metrics import Hallucination metric = Hallucination() score = metric.score( input=\"What is the capital of France?\", output=\"Paris\", context=[\"France is a country in Europe.\"] ) print(score) Code-Based LLM Evaluations Introduction It involves creating automated CI/CD test cases to evaluate how the LLM performs on specific tasks or datasets The advantage is that it's cost-efficient as it does not introduce token usage or latency Use Cases Test Correct Structure of Output Test Specific Data in Output by verifying that the LLM output contains specific data points","title":"LLMs Evaluation"},{"location":"evaluation/#llms-evaluation","text":"","title":"LLMs Evaluation"},{"location":"evaluation/#introduction","text":"","title":"Introduction"},{"location":"evaluation/#areas-of-evaluation","text":"An evaluation framework for LLMs should target two main areas: Use Case or Dynamic Behavior Evaluation - Custom metrics that directly measure how well the LLM is performing regarding the specific task System Architecture - Generic metrics on, for example, faithfulness of information retrieved by the RAG or the Task Completion for AI Agents","title":"Areas of Evaluation"},{"location":"evaluation/#llm-standalone-metrics","text":"These metrics are related to evaluate LLM against standardised benchmarks: GLUE SyperGLUE HellaSwag TruthfulQA MMLU","title":"LLM Standalone Metrics"},{"location":"evaluation/#online-vs-offline-evaluation","text":"Offline evaluation usually proves valuable in the initial development stages of features, but it falls short in assessing how model changes impact the user experience in a live production environment. Offline - Offline evaluation scrutinizes LLMs against specific datasets Online - Test the E2E system","title":"Online vs. Offline Evaluation"},{"location":"evaluation/#evaluation-methodologies","text":"","title":"Evaluation Methodologies"},{"location":"evaluation/#llm-evaluation-metrics","text":"","title":"LLM Evaluation Metrics"},{"location":"evaluation/#general-list","text":"Answer Relevancy - Determines whether an LLM output is able to address the given input and certain context and rules \u2705 Task Completion - Determines whether an LLM agent is able to complete the task it was set out to do \u26a0\ufe0f \u2192 How to determine completion state? Correctness - Determines whether an LLM output is factually correct based on some ground truth \u2705 Hallucination - Determines whether an LLM output contains fake or made-up information \u274c \u2192 Impossible to determine Tool Correctness - Determines whether an LLM agent is able to call the correct tools for a given task \u26a0\ufe0f \u2192 How to determine if the tool is correct? Contextual Relevancy - Determines whether the retriever in a RAG-based LLM system is able to extract the most relevant information for your LLM as context \u2705 \u2192 Similar to the first one, but on each retrieved document Responsible Metrics - Includes metrics such as bias and toxicity, which determines whether an LLM output contains (generally) harmful and offensive content \u2705 Task-Specific Metrics - Includes metrics such as summarization, which usually contains a custom criteria depending on the use-case \u2705","title":"General List"},{"location":"evaluation/#metrics-types","text":"Some metrics are based on Statistics, while others are sometimes referred as \"Model-based\" :","title":"Metrics Types"},{"location":"evaluation/#statistics-metrics","text":"They might perform poorly when the output implies reasoning capabilities (No semantic is included) They do not take into account any List of Metrics: BLEU (BiLingual Evaluation Understudy) - It evaluates the output of the LLM application against annotated ground truths. It calculates the precision for each matching n-gram (n consecutive words) between an LLM output and expected output to calculate their geometric mean and applies a brevity penalty if needed. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) - It is used for text summarisation and calculates recall by comparing the overlap of n-grams between LLM outputs and expected outputs. It also leverages external linguistic databases like WordNet to account for synonyms. The final score is the harmonic mean of precision and recall, with a penalty for ordering discrepancies. METEOR (Metric for Evaluation of Translation with Explicit Ordering) - It calculates scores by assessing both precision (n-gram matches) and recall (n-gram overlaps), adjusted for word order differences between LLM outputs and expected outputs. It can also leverages exteral linguistic databases. Levenshtein distance","title":"Statistics Metrics"},{"location":"evaluation/#model-based-metrics","text":"Reliable but inaccurate (struggle to keep semantic included), because of their probabilistic nature List of Metrics: NLI - It is a Non-LLM based and uses Natural Language Inference models to classify whether an LLM output is logically consistent (entailment), contradictory, or unrelated (neutral) with respect to a given reference text. BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) - It uses pre-trained models like BERT to score LLM outputs on some expected outputs","title":"Model-based Metrics"},{"location":"evaluation/#statistical-and-model-based-scorers","text":"BERTScore - It relies on a pre-trained LLM like BERT and on the cosine similarity between expected output and predicted output. Afterward, the similarities are aggregated to produce a final score. MoverScore - It relies on LLM like BERT to obtain deeper contextualised word embeddings for both reference text and generated text before computing the similarity.","title":"Statistical and Model-Based Scorers"},{"location":"evaluation/#usage-tips","text":"It is good to have: 1-2 custom metrics (G-Eval or DAG) that are use case specific 2-3 generic metrics (RAG, agentic, or conversational) that are system specific","title":"Usage Tips"},{"location":"evaluation/#g-eval-model-based-scorer","text":"","title":"G-Eval (Model-based Scorer)"},{"location":"evaluation/#introduction_1","text":"It is an LLM-based Scorer ( Paper ) Documentation from DeepEval","title":"Introduction"},{"location":"evaluation/#process","text":"Prompt with the following information: 1) Task Introduction; 2) Evaluation Criteria Generate through the previous output the list of Evaluation Steps through the \"Auto Chain of Thoughts\" Prompt the Scorer LLM with Evaluation Steps, Input Context and Input Target (Optional) Normalise the output score by the probabilities of the output tokens","title":"Process"},{"location":"evaluation/#code-snippets","text":"from deepeval.test_case import LLMTestCase, LLMTestCaseParams from deepeval.metrics import GEval test_case = LLMTestCase(input=\"input to your LLM\", actual_output=\"your LLM output\") coherence_metric = GEval( name=\"Coherence\", criteria=\"Coherence - the collective quality of all sentences in the actual output\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT], ) coherence_metric.measure(test_case) print(coherence_metric.score) print(coherence_metric.reason)","title":"Code Snippets"},{"location":"evaluation/#dag-model-based-scorer","text":"","title":"DAG (Model-based Scorer)"},{"location":"evaluation/#introduction_2","text":"Deep Acyclic Graph is a LLM-based scorer that relies on a decision tree Each node is an LLM Judgement and each edge is a decision Each leaf node is associated with a hardcoded score","title":"Introduction"},{"location":"evaluation/#advantages","text":"Slightly more deterministic, since there's a certain degree of control in the score determination It can be used to filter away edge cases where LLM output doesn't even meet minimum requirements","title":"Advantages"},{"location":"evaluation/#code-snippets_1","text":"from deepeval.test_case import LLMTestCase from deepeval.metrics.dag import ( DeepAcyclicGraph, TaskNode, BinaryJudgementNode, NonBinaryJudgementNode, VerdictNode, ) from deepeval.metrics import DAGMetric correct_order_node = NonBinaryJudgementNode( criteria=\"Are the summary headings in the correct order: 'intro' => 'body' => 'conclusion'?\", children=[ VerdictNode(verdict=\"Yes\", score=10), VerdictNode(verdict=\"Two are out of order\", score=4), VerdictNode(verdict=\"All out of order\", score=2), ], ) correct_headings_node = BinaryJudgementNode( criteria=\"Does the summary headings contain all three: 'intro', 'body', and 'conclusion'?\", children=[ VerdictNode(verdict=False, score=0), VerdictNode(verdict=True, child=correct_order_node), ], ) extract_headings_node = TaskNode( instructions=\"Extract all headings in `actual_output`\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT], output_label=\"Summary headings\", children=[correct_headings_node, correct_order_node], ) # create the DAG dag = DeepAcyclicGraph(root_nodes=[extract_headings_node]) # create the metric format_correctness = DAGMetric(name=\"Format Correctness\", dag=dag) # create a test case test_case = LLMTestCase(input=\"your-original-text\", actual_output=\"your-summary\") # evaluate format_correctness.measure(test_case) print(format_correctness.score, format_correctness.reason)","title":"Code Snippets"},{"location":"evaluation/#prometheus-model-based-scorer","text":"","title":"Prometheus (Model-based Scorer)"},{"location":"evaluation/#introduction_3","text":"LLM-based evaluation framework use case agnostic Based con Llama-2-chat and fine-tuned for evaluation purposes","title":"Introduction"},{"location":"evaluation/#advantages_1","text":"Evaluation steps are not produced by LLM, but are embedded in the node itself","title":"Advantages"},{"location":"evaluation/#qag-score-hybrid-scorer","text":"","title":"QAG Score (Hybrid Scorer)"},{"location":"evaluation/#introduction_4","text":"QAG (Question Answer Generation) Score uses binary answer (\u2018yes\u2019 or \u2018no\u2019) to close-ended questions (which can be generated or preset) to compute a final metric score. Example: So for this example LLM output: Martin Luther King Jr., the renowned civil rights leader, was assassinated on April 4, 1968, at the Lorraine Motel in Memphis, Tennessee. He was in Memphis to support striking sanitation workers and was fatally shot by James Earl Ray, an escaped convict, while standing on the motel\u2019s second-floor balcony. A claim would be: Martin Luther King Jr. assassinated on the April 4, 1968 And a corresponding close-ended question would be: Was Martin Luther King Jr. assassinated on the April 4, 1968?","title":"Introduction"},{"location":"evaluation/#advantages_2","text":"The score is not directly generated by an LLM","title":"Advantages"},{"location":"evaluation/#gptscore","text":"","title":"GPTScore"},{"location":"evaluation/#introduction_5","text":"It is similar to G-Eval, but the evaluation trask is performed with a form-filling paradigm.","title":"Introduction"},{"location":"evaluation/#selfcheckgpt","text":"","title":"SelfCheckGPT"},{"location":"evaluation/#introduction_6","text":"It samples multiple output in order to detect hallucinations through a model-based approach.","title":"Introduction"},{"location":"evaluation/#rag-specific-metrics","text":"","title":"RAG-Specific Metrics"},{"location":"evaluation/#faithfulness","text":"It evaluates whether the Generator is generating output that factually aligns with the information presented from the Retriever. It is possible to use a QAG Score here. For faithfulness, if you define it as the proportion of truthful claims made in an LLM output with regards to the retrieval context, we can calculate faithfulness using QAG by following this algorithm: Use LLMs to extract all claims made in the output. For each claim, check whether the it agrees or contradicts with each individual node in the retrieval context. In this case, the close-ended question in QAG will be something like: \u201cDoes the given claim agree with the reference text\u201d, where the \u201creference text\u201d will be each individual retrieved node. (Note that you need to confine the answer to either a \u2018yes\u2019, \u2018no\u2019, or \u2018idk\u2019. The \u2018idk\u2019 state represents the edge case where the retrieval context does not contain relevant information to give a yes/no answer.) Add up the total number of truthful claims (\u2018yes\u2019 and \u2018idk\u2019), and divide it by the total number of claims made. from deepeval.metrics import FaithfulnessMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", retrieval_context=[\"...\"] ) metric = FaithfulnessMetric(threshold=0.5) metric.measure(test_case) print(metric.score) print(metric.reason) print(metric.is_successful())","title":"Faithfulness"},{"location":"evaluation/#answer-relevancy","text":"It assesses whether RAG generator outputs concise answers, and can be calculated by determining the proportion of sentences in an LLM output that a relevant to the input. from deepeval.metrics import AnswerRelevancyMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", retrieval_context=[\"...\"] ) metric = AnswerRelevancyMetric(threshold=0.5) metric.measure(test_case) print(metric.score) print(metric.reason) print(metric.is_successful())","title":"Answer Relevancy"},{"location":"evaluation/#contextual-precision","text":"Contextual Precision is a RAG metric that assesses the quality of your RAG pipeline\u2019s retriever. from deepeval.metrics import ContextualPrecisionMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", # Expected output is the \"ideal\" output of your LLM, it is an # extra parameter that's needed for contextual metrics expected_output=\"...\", retrieval_context=[\"...\"] ) metric = ContextualPrecisionMetric(threshold=0.5) # Or ContextualRecallMetric metric.measure(test_case) print(metric.score) print(metric.reason) print(metric.is_successful())","title":"Contextual Precision"},{"location":"evaluation/#agentic-metrics","text":"","title":"Agentic Metrics"},{"location":"evaluation/#tool-correctness","text":"Tool correctness is an agentic metric that assesses the quality of your agentic systems, and is the most unusual metric here because it is based on exact matching and not any LLM-as-a-judge. It is computed by comparing the tools called for a given input to the expected tools that should be called. from deepeval.test_case import LLMTestCase, ToolCall from deepeval.metrics import ToolCorrectnessMetric test_case = LLMTestCase( input=\"What if these shoes don't fit?\", actual_output=\"We offer a 30-day full refund at no extra cost.\", # Replace this with the tools that was actually used by your LLM agent tools_called=[ToolCall(name=\"WebSearch\"), ToolCall(name=\"ToolQuery\")], expected_tools=[ToolCall(name=\"WebSearch\")], ) metric = ToolCorrectnessMetric() metric.measure(test_case) print(metric.score, metric.reason)","title":"Tool Correctness"},{"location":"evaluation/#task-completion","text":"Task completion is an agentic metric that uses LLM-as-a-judge to evaluate whether your LLM agent is able to accomplish its given task. from deepeval.test_case import LLMTestCase from deepeval.metrics import TaskCompletionMetric metric = TaskCompletionMetric( threshold=0.7, model=\"gpt-4o\", include_reason=True ) test_case = LLMTestCase( input=\"Plan a 3-day itinerary for Paris with cultural landmarks and local cuisine.\", actual_output=( \"Day 1: Eiffel Tower, dinner at Le Jules Verne. \" \"Day 2: Louvre Museum, lunch at Angelina Paris. \" \"Day 3: Montmartre, evening at a wine bar.\" ), tools_called=[ ToolCall( name=\"Itinerary Generator\", description=\"Creates travel plans based on destination and duration.\", input_parameters={\"destination\": \"Paris\", \"days\": 3}, output=[ \"Day 1: Eiffel Tower, Le Jules Verne.\", \"Day 2: Louvre Museum, Angelina Paris.\", \"Day 3: Montmartre, wine bar.\", ], ), ToolCall( name=\"Restaurant Finder\", description=\"Finds top restaurants in a city.\", input_parameters={\"city\": \"Paris\"}, output=[\"Le Jules Verne\", \"Angelina Paris\", \"local wine bars\"], ), ], ) metric.measure(test_case) print(metric.score, metric.reason)","title":"Task Completion"},{"location":"evaluation/#llm-metrics","text":"","title":"LLM Metrics"},{"location":"evaluation/#hallucination","text":"from deepeval.metrics import HallucinationMetric from deepeval.test_case import LLMTestCase test_case=LLMTestCase( input=\"...\", actual_output=\"...\", # Note that 'context' is not the same as 'retrieval_context'. # While retrieval context is more concerned with RAG pipelines, # context is the ideal retrieval results for a given input, # and typically resides in the dataset used to fine-tune your LLM context=[\"...\"], ) metric = HallucinationMetric(threshold=0.5) metric.measure(test_case) print(metric.score) print(metric.is_successful())","title":"Hallucination"},{"location":"evaluation/#toxicity","text":"from deepeval.metrics import ToxicityMetric from deepeval.test_case import LLMTestCase metric = ToxicityMetric(threshold=0.5) test_case = LLMTestCase( input=\"What if these shoes don't fit?\", # Replace this with the actual output from your LLM application actual_output = \"We offer a 30-day full refund at no extra cost.\" ) metric.measure(test_case) print(metric.score)","title":"Toxicity"},{"location":"evaluation/#bias","text":"The bias metric evaluates aspects such as political, gender, and social biases in textual content. from deepeval.metrics import GEval from deepeval.test_case import LLMTestCase test_case = LLMTestCase( input=\"What if these shoes don't fit?\", # Replace this with the actual output from your LLM application actual_output = \"We offer a 30-day full refund at no extra cost.\" ) toxicity_metric = GEval( name=\"Bias\", criteria=\"Bias - determine if the actual output contains any racial, gender, or political bias.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT], ) metric.measure(test_case) print(metric.score)","title":"Bias"},{"location":"evaluation/#evaluation-framework-platforms","text":"","title":"Evaluation Framework Platforms"},{"location":"evaluation/#azure-ai-foundry-automated-evaluation-microsoft","text":"Azure AI Foundry is an all-in-one AI platform for building, evaluating, and deploying generative AI solutions and custom copilots.","title":"Azure AI Foundry Automated Evaluation (Microsoft)"},{"location":"evaluation/#openai-evals-openai","text":"The OpenAI Evals framework consists of a framework to evaluate an LLM or a system built on top of an LLM, and an open-source registry of challenging evals. It is based on pre-defined dataset. The process to create a new evaluation dataset is described in custom-eval.md from the OpenAI Evals package. Although it seems pretty rigid and too schematic for the actual standards.","title":"OpenAI Evals (OpenAI)"},{"location":"evaluation/#weights-biases","text":"A Machine Learning platform to quickly track experiments, version and iterate on datasets, evaluate model performance.","title":"Weights &amp; Biases"},{"location":"evaluation/#langsmith-langchain","text":"Trace and evaluate language model applications and intelligent agents.","title":"LangSmith (LangChain)"},{"location":"evaluation/#trulens-truera","text":"TruLens provides a set of tools for developing and monitoring neural nets, including LLMs.","title":"TruLens (TruEra)"},{"location":"evaluation/#vertex-ai-studio-google","text":"You can evaluate the performance of foundation models and your tuned generative AI models on Vertex AI.","title":"Vertex AI Studio (Google)"},{"location":"evaluation/#amazon-bedrock","text":"Amazon Bedrock supports model evaluation jobs.","title":"Amazon Bedrock"},{"location":"evaluation/#deepeval-confident-ai","text":"An open-source LLM evaluation framework for LLM applications. Sample Codes .","title":"DeepEval (Confident AI)"},{"location":"evaluation/#parea-ai","text":"Parea provides tools for debugging, testing, evaluating, and monitoring LLM-powered applications.","title":"Parea AI"},{"location":"evaluation/#optik-by-comet","text":"Opik is an open-source platform by Comet for evaluating, testing, and monitoring Large Language Models (LLMs). It provides flexible tools to track, annotate, and refine LLM applications across development and production environments. from opik.evaluation.metrics import Hallucination metric = Hallucination() score = metric.score( input=\"What is the capital of France?\", output=\"Paris\", context=[\"France is a country in Europe.\"] ) print(score)","title":"Optik by Comet"},{"location":"evaluation/#code-based-llm-evaluations","text":"","title":"Code-Based LLM Evaluations"},{"location":"evaluation/#introduction_7","text":"It involves creating automated CI/CD test cases to evaluate how the LLM performs on specific tasks or datasets The advantage is that it's cost-efficient as it does not introduce token usage or latency","title":"Introduction"},{"location":"evaluation/#use-cases","text":"Test Correct Structure of Output Test Specific Data in Output by verifying that the LLM output contains specific data points","title":"Use Cases"},{"location":"fine_tuning/","text":"Fine-Tuning Terminology Auto-Regression the way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called \u201cAuto-Regression\u201d . This feature is not always incorporated. For example, BERT does not have it. Pre-Training Definition It's the very first step of training a LLM and, in this operation, a huge amount of text data is processed. Steps Data Processing In order to feed the text data in the training process, they have to be converted into tokens by a Tokenizer , which is specifically trained for the task. Its job is to encode and decode text into tokens (and vice versa). The dataset is then pre-processed using the tokenizer's vocabulary, converting the raw text into a format suitable for training the model. This step involves mapping tokens to their corresponding IDs, and incorporating any necessary special tokens or attention masks. Once the dataset is pre-processed, it is ready to be used for the pre-training phase. Training In this step, the model learns either to predict the next token in a sequence, or filling the missing tokens in a given sequence. In this way, the model learn language patterns, grammar, and semantic relationships The task depends on the training algorithm, but it is a supervised-learning algorithm. Learning Algorithms Masked Language Modeling - The model tries to predict certain masked tokens within the input sequence Casual Language Modeling - The model tries to predict the next token given the preceding context Definition LLMs are pre-trained on very extensive text corpus - LLaMa 2 on 2 trillion tokens - BERT on BookCorpus (800M words) and Wikipedia (2500M words) This pre-training is very long and costly. Such pre-trained models are just able to predict tokens and, thus, construct sentences. However, they're not really efficient in answering questions. This is the reason for the Fine-Tuning step: allows us to specialize the model's capabilities and optimize its performance on a narrower, task-specific dataset. Process The goal is to re-train the model's weights for a specific task. The way in which this happens can vary much, depending on the Fine-Tuning algorithm chosen. The whole model's weights can be retrained, just a portion or having another set of weights (LoRA). During this Fine-Tuning Process, all the elements of a normal training are applied: optimizer (e.g., SGD or Adam), learning rate, dropout, weight decay, overfit and early stopping. Dataset Definition The dataset used for the Fine-Tuning should have: Data Diversity - Do not address a single task, but aim for more. Ensure to include all possible conversation scenarios Dataset Size - At least 10MiB. It's not easy to overfit a pre-trained model with fine-tuning, so the more, the better Dataset Quality - Do not feed garbage Performance A pure pre-trained model can be most of the time be out-performed by a fine-tuned model, even if the original pre-trainig was performed on fewer tokens. Techniques Supervised Fine-Tuning (SFT) Models are trained on a dataset of instructions and responses. It adjusts the weights in the LLM to minimize the difference between the generated answers and ground-truth responses, acting as labels. It requires: Good quality instruction dataset Prompt template Reinforcement Learning from Human Feedback (RLHF) Models learn by interacting with their environment and receiving feedback. They are trained to maximize a reward signal (using PPO), which is often derived from human evaluations of model outputs. One example of dataset used in RLHF is Anthropic/hh-rlhf . For each row there is one chosen and one rejected answer. Parameter Efficient Fine-Tuning (PEFT) Since both SFT and RLHF are very costly, the PEFT was a huge step forward. At a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the order of thousands of parameters) that are used to \u2018perturb\u2019 the pre-trained LLM weights. The perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. This has the benefit of training a significantly smaller set of weights, compared to traditional fine-tuning of the entire model. Some PEFT techniques are: Adapter-based fine-tuning - It employs small modules, called adapters, to the pre-trained model. Only adapters' parameters are trained Low-Rank Adaptation (LoRA) - It uses two smaller matrices to approximate the original weight matrix update instead of fine-tuning the whole LLM. This technique freezes the original weights and trains these update matrices, significantly reducing resource requirements with minimum additional inference latency. Additionally, LoRA has improved variants such as QLoRA,48 which uses quantized weights for even greater efficiency. Comparison RLHF is able to better capture humans way of generating responses, but it's harder to implement. LoRA General The Low-Rang Adaptation algorithm is designed for fine-tuning LLMs while keeping memory consumption low. The main concept is the Low Rank : there are very few elements in the weights matrices of an LLM that carry information. So it is required to just add a Low Rank Update matrices in order to capture such valuable information. The main point to fine-tune is the Self-Attention layer, since it's the one having the lowest rank (i.e, the most redundant information) Process LLMs are pre-trained by updating the weights of certain weight matrices, added into the model's architecture. LoRA focuses on pair of Rank-Decomposition Weight Matrices (Update matrices) to the existing pre-training weights. It basically adds new weights matrices on top. The update matrices are placed only for the self-attention layers. It also shows better results when these Update Matrices are applied on the Value Tensors of the self-attention layers. If the rank is equal to the rank of the Self-Attention layer, we are doing a full fine-tuning. Hyperparameters Rank It is the number of independent rows within a matrix. It depends on the complexity of the dataset. Higher the number, higher the complexity and, thus, the memory requirements. To match a full fine-tune, the rank has to be equal to the model's hidden size ( model.config.hidden_size ) Target Modules It determines which weights and matrices have to be targeted. By default, the Query Vector and Value Vector . Such matrices can be usually retrieved as follow: from transformers import AutoModelForCausalLM model_name = \"huggyllama/llama-7b\" # can also be a local directory model = AutoModelForCausalLM.from_pretrained(model_name) layer_names = model.state_dict().keys() for name in layer_names: print(name) \"\"\" Output model.embed_tokens.weight model.layers.0.self_attn.q_proj.weight model.layers.0.self_attn.k_proj.weight model.layers.0.self_attn.v_proj.weight model.layers.0.self_attn.o_proj.weight model.layers.0.self_attn.rotary_emb.inv_freq model.layers.0.mlp.gate_proj.weight model.layers.0.mlp.down_proj.weight model.layers.0.mlp.up_proj.weight model.layers.0.input_layernorm.weight model.layers.0.post_attention_layernorm.weight ... model.norm.weight lm_head.weight \"\"\" Naming convention is: {identifier}.{layer}.{layer_number}.{component}.{module}.{parameter} . Some basic modules are: up_proj : The projection matrix used in the upward (decoder to encoder) attention pass. It projects the decoder's hidden states to the same dimension as the encoder's hidden states for compatibility during attention calculations. down_proj : The projection matrix used in the downward (encoder to decoder) attention pass. It projects the encoder's hidden states to the dimension expected by thr decoder for attention calculations. q_proj : The projection matrix applied to the query vectors in the attention mechanism. Transforms the input hidden states to the desired dimension for effective query representations. v_proj : The projection matrix applied to the value vectors in the attention mechanism. Transforms the input hidden states to the desired dimension for effective value representations. k_proj : The projection matrix applied to the key vectors blah blah. Transforms the input hidden states to the desired dimension for effective value representations. o_proj : The projection matrix applied to the output of the attention mechanism. Transforms the combined attention output to the desired dimension before further processing. Advantages Preserve pre-training weights, minimizing the risk of catastrophic forgetting Update Matrices have far fewer parameters tha pre-training weights, thus are much more portable Update Matrices are incorporated into original attention layers Memory efficiency due to the dimension of Update Matrices QLoRA Definition QLoRA (Quantized Low Rank Adapters) is an efficient fine-tuning approach that reduces memory usage while maintaining high performance for large language models. It enables the fine-tuning of a 65B parameter model on a single 48GB GPU, while preserving full 16-bit fine-tuning task performance. Key Innovations: Backpropagation of gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA) Use of a new data type called 4-bit NormalFloat (NF4), which optimally handles normally distributed weights Double quantization to reduce the average memory footprint by quantising the quantization constants Paged optimizers to effectively manage memory spikes during the fine-tuning process Hyperparameters Batch Size - Number of training sample processed at the same time, before updating the weights Epochs - The number of time the model sees the entire dataset Axolotl Definition Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures. It supports different moodels (e.g., LLaMA, Falcon, etc.) and different Fine-Tuning algorithms, such as LoRA and QLoRA. Advantages Single Configuration File - All parameters used to train an LLM are neatly stored in a yaml config file. This makes it convenient for sharing and reproducing models. Examples in the GitHub repo here . Dataset Flexibility - Different prompt's formats supported Modify Configuration Some parameters that should be usually changed are: base_model base_model_config (Usually the same as the base_model ) hub_model_id (New model name) datasets ( path and type ) Gradient Checkpointing Gradient checkpointing is a technique used in machine learning, particularly in deep learning, to reduce the memory requirements during the training of neural networks, especially those with many layers. In deep learning, during the backpropagation process, gradients are computed and stored for each layer in order to update the model parameters. However, as neural networks become deeper, the memory requirements for storing these gradients can become a limiting factor, especially in memory-constrained environments such as GPUs. Gradient checkpointing addresses this issue by trading off memory consumption with recomputation during the backward pass. Instead of storing the gradients for all layers, only a subset of the layers' activations and intermediate gradients are stored, while the remaining layers' activations are recomputed during the backward pass when needed. Direct Preference Optimisation (DPO) Definition It is RLHF Fine-Tuning technique. PPO vs DPO The Proximal Policy Optimization (PPO) is another reinforcement learning algorithm that aims to improve the policy of an agent. One of the key features of PPO is the use of a \"proximal policy optimization\" approach, which constrains the policy updates to be within a certain \"trust region\" to prevent large policy changes that could lead to instability in learning. It however still very unstable and computationally expensive. It also implies optimizing a surrogate objective function that approximates the true objective. The Direct Preference Optimization (DPO) directly optimizes the preferences of an agent over different actions or policies, rather than optimizing a surrogate objective function. In DPO, the agent learns a preference function that assigns values or scores to different actions or policies based on their expected long-term rewards. Code Examples SFT on Vertex AI Python # Before you start run this command: # pip install --upgrade --user --quiet google-cloud-aiplatform # after running pip install make sure you restart your kernel import vertexai from vertexai.generative_models import GenerativeModel from vertexai.preview.tuning import sft # TODO : Set values as per your requirements # Project and Storage Constants PROJECT_ID = \u2018<project_id>\u2019 REGION = \u2018<region>\u2019 vertexai.init(project=PROJECT_ID, location=REGION) # define training & eval dataset. TRAINING_DATASET = \u2018gs://cloud-samples-data/vertex-ai/model-evaluation/ peft_train_sample.jsonl\u2019 # set base model and specify a name for the tuned model BASE_MODEL = \u2018gemini-1.5-pro-002\u2019 TUNED_MODEL_DISPLAY_NAME = \u2018gemini-fine-tuning-v1\u2019 # start the fine-tuning job sft_tuning_job = sft.train( source_model=BASE_MODEL, train_dataset=TRAINING_DATASET, # # Optional: tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME, ) # Get the tuning job info. sft_tuning_job.to_dict() # tuned model endpoint name tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name # use the tuned model tuned_genai_model = GenerativeModel(tuned_model_endpoint_name) print(tuned_genai_model.generate_content(contents=\u2019What is a LLM?\u2019))","title":"Fine-Tuning"},{"location":"fine_tuning/#fine-tuning","text":"","title":"Fine-Tuning"},{"location":"fine_tuning/#terminology","text":"","title":"Terminology"},{"location":"fine_tuning/#auto-regression","text":"the way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called \u201cAuto-Regression\u201d . This feature is not always incorporated. For example, BERT does not have it.","title":"Auto-Regression"},{"location":"fine_tuning/#pre-training","text":"","title":"Pre-Training"},{"location":"fine_tuning/#definition","text":"It's the very first step of training a LLM and, in this operation, a huge amount of text data is processed.","title":"Definition"},{"location":"fine_tuning/#steps","text":"","title":"Steps"},{"location":"fine_tuning/#data-processing","text":"In order to feed the text data in the training process, they have to be converted into tokens by a Tokenizer , which is specifically trained for the task. Its job is to encode and decode text into tokens (and vice versa). The dataset is then pre-processed using the tokenizer's vocabulary, converting the raw text into a format suitable for training the model. This step involves mapping tokens to their corresponding IDs, and incorporating any necessary special tokens or attention masks. Once the dataset is pre-processed, it is ready to be used for the pre-training phase.","title":"Data Processing"},{"location":"fine_tuning/#training","text":"In this step, the model learns either to predict the next token in a sequence, or filling the missing tokens in a given sequence. In this way, the model learn language patterns, grammar, and semantic relationships The task depends on the training algorithm, but it is a supervised-learning algorithm.","title":"Training"},{"location":"fine_tuning/#learning-algorithms","text":"Masked Language Modeling - The model tries to predict certain masked tokens within the input sequence Casual Language Modeling - The model tries to predict the next token given the preceding context","title":"Learning Algorithms"},{"location":"fine_tuning/#definition_1","text":"LLMs are pre-trained on very extensive text corpus - LLaMa 2 on 2 trillion tokens - BERT on BookCorpus (800M words) and Wikipedia (2500M words) This pre-training is very long and costly. Such pre-trained models are just able to predict tokens and, thus, construct sentences. However, they're not really efficient in answering questions. This is the reason for the Fine-Tuning step: allows us to specialize the model's capabilities and optimize its performance on a narrower, task-specific dataset.","title":"Definition"},{"location":"fine_tuning/#process","text":"The goal is to re-train the model's weights for a specific task. The way in which this happens can vary much, depending on the Fine-Tuning algorithm chosen. The whole model's weights can be retrained, just a portion or having another set of weights (LoRA). During this Fine-Tuning Process, all the elements of a normal training are applied: optimizer (e.g., SGD or Adam), learning rate, dropout, weight decay, overfit and early stopping.","title":"Process"},{"location":"fine_tuning/#dataset-definition","text":"The dataset used for the Fine-Tuning should have: Data Diversity - Do not address a single task, but aim for more. Ensure to include all possible conversation scenarios Dataset Size - At least 10MiB. It's not easy to overfit a pre-trained model with fine-tuning, so the more, the better Dataset Quality - Do not feed garbage","title":"Dataset Definition"},{"location":"fine_tuning/#performance","text":"A pure pre-trained model can be most of the time be out-performed by a fine-tuned model, even if the original pre-trainig was performed on fewer tokens.","title":"Performance"},{"location":"fine_tuning/#techniques","text":"","title":"Techniques"},{"location":"fine_tuning/#supervised-fine-tuning-sft","text":"Models are trained on a dataset of instructions and responses. It adjusts the weights in the LLM to minimize the difference between the generated answers and ground-truth responses, acting as labels. It requires: Good quality instruction dataset Prompt template","title":"Supervised Fine-Tuning (SFT)"},{"location":"fine_tuning/#reinforcement-learning-from-human-feedback-rlhf","text":"Models learn by interacting with their environment and receiving feedback. They are trained to maximize a reward signal (using PPO), which is often derived from human evaluations of model outputs. One example of dataset used in RLHF is Anthropic/hh-rlhf . For each row there is one chosen and one rejected answer.","title":"Reinforcement Learning from Human Feedback (RLHF)"},{"location":"fine_tuning/#parameter-efficient-fine-tuning-peft","text":"Since both SFT and RLHF are very costly, the PEFT was a huge step forward. At a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the order of thousands of parameters) that are used to \u2018perturb\u2019 the pre-trained LLM weights. The perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. This has the benefit of training a significantly smaller set of weights, compared to traditional fine-tuning of the entire model. Some PEFT techniques are: Adapter-based fine-tuning - It employs small modules, called adapters, to the pre-trained model. Only adapters' parameters are trained Low-Rank Adaptation (LoRA) - It uses two smaller matrices to approximate the original weight matrix update instead of fine-tuning the whole LLM. This technique freezes the original weights and trains these update matrices, significantly reducing resource requirements with minimum additional inference latency. Additionally, LoRA has improved variants such as QLoRA,48 which uses quantized weights for even greater efficiency.","title":"Parameter Efficient Fine-Tuning (PEFT)"},{"location":"fine_tuning/#comparison","text":"RLHF is able to better capture humans way of generating responses, but it's harder to implement.","title":"Comparison"},{"location":"fine_tuning/#lora","text":"","title":"LoRA"},{"location":"fine_tuning/#general","text":"The Low-Rang Adaptation algorithm is designed for fine-tuning LLMs while keeping memory consumption low. The main concept is the Low Rank : there are very few elements in the weights matrices of an LLM that carry information. So it is required to just add a Low Rank Update matrices in order to capture such valuable information. The main point to fine-tune is the Self-Attention layer, since it's the one having the lowest rank (i.e, the most redundant information)","title":"General"},{"location":"fine_tuning/#process_1","text":"LLMs are pre-trained by updating the weights of certain weight matrices, added into the model's architecture. LoRA focuses on pair of Rank-Decomposition Weight Matrices (Update matrices) to the existing pre-training weights. It basically adds new weights matrices on top. The update matrices are placed only for the self-attention layers. It also shows better results when these Update Matrices are applied on the Value Tensors of the self-attention layers. If the rank is equal to the rank of the Self-Attention layer, we are doing a full fine-tuning.","title":"Process"},{"location":"fine_tuning/#hyperparameters","text":"","title":"Hyperparameters"},{"location":"fine_tuning/#rank","text":"It is the number of independent rows within a matrix. It depends on the complexity of the dataset. Higher the number, higher the complexity and, thus, the memory requirements. To match a full fine-tune, the rank has to be equal to the model's hidden size ( model.config.hidden_size )","title":"Rank"},{"location":"fine_tuning/#target-modules","text":"It determines which weights and matrices have to be targeted. By default, the Query Vector and Value Vector . Such matrices can be usually retrieved as follow: from transformers import AutoModelForCausalLM model_name = \"huggyllama/llama-7b\" # can also be a local directory model = AutoModelForCausalLM.from_pretrained(model_name) layer_names = model.state_dict().keys() for name in layer_names: print(name) \"\"\" Output model.embed_tokens.weight model.layers.0.self_attn.q_proj.weight model.layers.0.self_attn.k_proj.weight model.layers.0.self_attn.v_proj.weight model.layers.0.self_attn.o_proj.weight model.layers.0.self_attn.rotary_emb.inv_freq model.layers.0.mlp.gate_proj.weight model.layers.0.mlp.down_proj.weight model.layers.0.mlp.up_proj.weight model.layers.0.input_layernorm.weight model.layers.0.post_attention_layernorm.weight ... model.norm.weight lm_head.weight \"\"\" Naming convention is: {identifier}.{layer}.{layer_number}.{component}.{module}.{parameter} . Some basic modules are: up_proj : The projection matrix used in the upward (decoder to encoder) attention pass. It projects the decoder's hidden states to the same dimension as the encoder's hidden states for compatibility during attention calculations. down_proj : The projection matrix used in the downward (encoder to decoder) attention pass. It projects the encoder's hidden states to the dimension expected by thr decoder for attention calculations. q_proj : The projection matrix applied to the query vectors in the attention mechanism. Transforms the input hidden states to the desired dimension for effective query representations. v_proj : The projection matrix applied to the value vectors in the attention mechanism. Transforms the input hidden states to the desired dimension for effective value representations. k_proj : The projection matrix applied to the key vectors blah blah. Transforms the input hidden states to the desired dimension for effective value representations. o_proj : The projection matrix applied to the output of the attention mechanism. Transforms the combined attention output to the desired dimension before further processing.","title":"Target Modules"},{"location":"fine_tuning/#advantages","text":"Preserve pre-training weights, minimizing the risk of catastrophic forgetting Update Matrices have far fewer parameters tha pre-training weights, thus are much more portable Update Matrices are incorporated into original attention layers Memory efficiency due to the dimension of Update Matrices","title":"Advantages"},{"location":"fine_tuning/#qlora","text":"","title":"QLoRA"},{"location":"fine_tuning/#definition_2","text":"QLoRA (Quantized Low Rank Adapters) is an efficient fine-tuning approach that reduces memory usage while maintaining high performance for large language models. It enables the fine-tuning of a 65B parameter model on a single 48GB GPU, while preserving full 16-bit fine-tuning task performance. Key Innovations: Backpropagation of gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA) Use of a new data type called 4-bit NormalFloat (NF4), which optimally handles normally distributed weights Double quantization to reduce the average memory footprint by quantising the quantization constants Paged optimizers to effectively manage memory spikes during the fine-tuning process","title":"Definition"},{"location":"fine_tuning/#hyperparameters_1","text":"Batch Size - Number of training sample processed at the same time, before updating the weights Epochs - The number of time the model sees the entire dataset","title":"Hyperparameters"},{"location":"fine_tuning/#axolotl","text":"","title":"Axolotl"},{"location":"fine_tuning/#definition_3","text":"Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures. It supports different moodels (e.g., LLaMA, Falcon, etc.) and different Fine-Tuning algorithms, such as LoRA and QLoRA.","title":"Definition"},{"location":"fine_tuning/#advantages_1","text":"Single Configuration File - All parameters used to train an LLM are neatly stored in a yaml config file. This makes it convenient for sharing and reproducing models. Examples in the GitHub repo here . Dataset Flexibility - Different prompt's formats supported","title":"Advantages"},{"location":"fine_tuning/#modify-configuration","text":"Some parameters that should be usually changed are: base_model base_model_config (Usually the same as the base_model ) hub_model_id (New model name) datasets ( path and type )","title":"Modify Configuration"},{"location":"fine_tuning/#gradient-checkpointing","text":"Gradient checkpointing is a technique used in machine learning, particularly in deep learning, to reduce the memory requirements during the training of neural networks, especially those with many layers. In deep learning, during the backpropagation process, gradients are computed and stored for each layer in order to update the model parameters. However, as neural networks become deeper, the memory requirements for storing these gradients can become a limiting factor, especially in memory-constrained environments such as GPUs. Gradient checkpointing addresses this issue by trading off memory consumption with recomputation during the backward pass. Instead of storing the gradients for all layers, only a subset of the layers' activations and intermediate gradients are stored, while the remaining layers' activations are recomputed during the backward pass when needed.","title":"Gradient Checkpointing"},{"location":"fine_tuning/#direct-preference-optimisation-dpo","text":"","title":"Direct Preference Optimisation (DPO)"},{"location":"fine_tuning/#definition_4","text":"It is RLHF Fine-Tuning technique.","title":"Definition"},{"location":"fine_tuning/#ppo-vs-dpo","text":"The Proximal Policy Optimization (PPO) is another reinforcement learning algorithm that aims to improve the policy of an agent. One of the key features of PPO is the use of a \"proximal policy optimization\" approach, which constrains the policy updates to be within a certain \"trust region\" to prevent large policy changes that could lead to instability in learning. It however still very unstable and computationally expensive. It also implies optimizing a surrogate objective function that approximates the true objective. The Direct Preference Optimization (DPO) directly optimizes the preferences of an agent over different actions or policies, rather than optimizing a surrogate objective function. In DPO, the agent learns a preference function that assigns values or scores to different actions or policies based on their expected long-term rewards.","title":"PPO vs DPO"},{"location":"fine_tuning/#code-examples","text":"","title":"Code Examples"},{"location":"fine_tuning/#sft-on-vertex-ai","text":"Python # Before you start run this command: # pip install --upgrade --user --quiet google-cloud-aiplatform # after running pip install make sure you restart your kernel import vertexai from vertexai.generative_models import GenerativeModel from vertexai.preview.tuning import sft # TODO : Set values as per your requirements # Project and Storage Constants PROJECT_ID = \u2018<project_id>\u2019 REGION = \u2018<region>\u2019 vertexai.init(project=PROJECT_ID, location=REGION) # define training & eval dataset. TRAINING_DATASET = \u2018gs://cloud-samples-data/vertex-ai/model-evaluation/ peft_train_sample.jsonl\u2019 # set base model and specify a name for the tuned model BASE_MODEL = \u2018gemini-1.5-pro-002\u2019 TUNED_MODEL_DISPLAY_NAME = \u2018gemini-fine-tuning-v1\u2019 # start the fine-tuning job sft_tuning_job = sft.train( source_model=BASE_MODEL, train_dataset=TRAINING_DATASET, # # Optional: tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME, ) # Get the tuning job info. sft_tuning_job.to_dict() # tuned model endpoint name tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name # use the tuned model tuned_genai_model = GenerativeModel(tuned_model_endpoint_name) print(tuned_genai_model.generate_content(contents=\u2019What is a LLM?\u2019))","title":"SFT on Vertex AI"},{"location":"google_llms/","text":"Google LLMs Services AI Studio AI Studio is free to use and only requires a compatible Google account to log in and get started. It is deeply integrated with the Gemini API, which comes with a generous free tier that you can use to run. Python SDK Installation pip install -U -q \"google-generativeai>=0.8.3\" # Import import google.generativeai as genai Setup # Import Standard Libraries import google.generativeai as genai # Set the API Key from AI Studio genai.configure(api_key=GOOGLE_API_KEY)","title":"Google LLMs"},{"location":"google_llms/#google-llms","text":"","title":"Google LLMs"},{"location":"google_llms/#services","text":"","title":"Services"},{"location":"google_llms/#ai-studio","text":"AI Studio is free to use and only requires a compatible Google account to log in and get started. It is deeply integrated with the Gemini API, which comes with a generous free tier that you can use to run.","title":"AI Studio"},{"location":"google_llms/#python-sdk","text":"","title":"Python SDK"},{"location":"google_llms/#installation","text":"pip install -U -q \"google-generativeai>=0.8.3\" # Import import google.generativeai as genai","title":"Installation"},{"location":"google_llms/#setup","text":"# Import Standard Libraries import google.generativeai as genai # Set the API Key from AI Studio genai.configure(api_key=GOOGLE_API_KEY)","title":"Setup"},{"location":"tokenizers/","text":"Tokenizer Resources YouTube Let's build the GPT Tokenizer Playground TikTokenizer Libraries TikToken SentencePiece Definition It is used the main element while using LLMs, and it converts strings into vectors. Strings and Unicode in Python In Python you can retrieve the Unicode value of a character through ord('<char>) . The Python interpreter does not therefore see characters, but those numbers instead. UTF-8/16/32 are specific encoding of Unicode: they define how to represent Unicode characeters in bytes. For example: 'hello'.encode('utf-8') would return the bytes for those 5 characters. list('Hello'.encode('utf-8'))# [72, 101, 108, 108, 111] -> List of raw bytes UTF-8 uses dynamic number of bytes up to 4, while UTF-16 and UTF-32 use always the same amount of 4 bytes, leading to many 0s in their encodings, especially for single english characters. It would be amazing to directly feed bytes into an LLM, but the context window would be too high! Therefore, we still need the tokenization process. Common Problems Diluted Tokens The reason why LLMs work in different ways between different languages and/or topics, is because of the Tokenizer (most of the time). This can be due to different reasons, but if we have a look on how GPT2 Tokenizer behaves with Korean with respect to english, we can see an interesting pattern. The tokens required to tokenize a Korean text are much more than the ones required to tokenize a similar english text. This is because the tokenizer has been trained on english text mainly. The effect is that the output tokens for the Korean text are much more sparse, and therefore the LLM does not work very well. Whitespace Character One key component while tokenizing code text is how to tokenize space. GPT2 use a single token for each space, leading to a Diluted Tokens problem. GPT4o Tokenizer (cl100k_base) improved this behaviour by using less tokens for the indentation. Characteristics Tokens Vocabulary It represents the tokens space through which the tokenizer can convert the text. The bigger is this vocabulary, the lesser tokens would be required to tokenize a text. More tokens in the vocabulary means that the tokenizer would be able to better represent the meaning of a text, fighting the \"Diluted Tokens\" problem. However, too big vocabulary might influence the LLM softmax function for the token sampling while constructing its output. Splitting The very first step of any Encoder is the text split. GPT-2, for example, does that through a REGEX, which can work differently between lowercase and uppercase. Also, the way in which the sentence is separated before encoding it can affect the final result. There are also rules when need to split text that includes, for example, code. This is the main change between GPT-2 and GPT-4 Tokenizers. Techniques Byte Pair Encoding (BPE) It is a technique that wants to compress the output encoding by encoding together the pairs of most common bytes. It is essentially a token embeddings' algorithm. For example: [aaabbaabaacaa] \u2192 The sequence \"aa\" is the most common \u2192 Z = aa \u2192 [ZabbZbZcZ] This would reduce the length of the output sequence. This can be done recursively and shortening each time the output sequence. It is possible to perform a hyperparameter tuning process in order to understand which is the best Vocabulary size that has the best compression (i.e, the number of times we repeat the Byte Pair Encoding). Special Tokens With the TikToken library it is possible to add special tokens to the Encoder. For example the <endoftext> and assign it with an unused index. Training General The Tokenizer has its own training set, separated from the LLM's training. Taking into account the training dataset and the \"Diluted Tokens\" problem, it becomes clear that, the more words in the Tokenizer sees in the training dataset that are, for example, in Japanese, the better the Tokenizer would group up these words into the same token. In this way, it would represent a Japanese sentence with far less tokens. Libraries SentencePiece Unlike TikToken, SentencePiece is pretty good with both training and inference: TikToken - Encode to UTF-8 and then applied BPE SentencePiece - Apply BPE directly on the code points (and eventually falls back to UTF-8 for certain code points)","title":"Tokenizer"},{"location":"tokenizers/#tokenizer","text":"","title":"Tokenizer"},{"location":"tokenizers/#resources","text":"","title":"Resources"},{"location":"tokenizers/#youtube","text":"Let's build the GPT Tokenizer","title":"YouTube"},{"location":"tokenizers/#playground","text":"TikTokenizer","title":"Playground"},{"location":"tokenizers/#libraries","text":"TikToken SentencePiece","title":"Libraries"},{"location":"tokenizers/#definition","text":"It is used the main element while using LLMs, and it converts strings into vectors.","title":"Definition"},{"location":"tokenizers/#strings-and-unicode-in-python","text":"In Python you can retrieve the Unicode value of a character through ord('<char>) . The Python interpreter does not therefore see characters, but those numbers instead. UTF-8/16/32 are specific encoding of Unicode: they define how to represent Unicode characeters in bytes. For example: 'hello'.encode('utf-8') would return the bytes for those 5 characters. list('Hello'.encode('utf-8'))# [72, 101, 108, 108, 111] -> List of raw bytes UTF-8 uses dynamic number of bytes up to 4, while UTF-16 and UTF-32 use always the same amount of 4 bytes, leading to many 0s in their encodings, especially for single english characters. It would be amazing to directly feed bytes into an LLM, but the context window would be too high! Therefore, we still need the tokenization process.","title":"Strings and Unicode in Python"},{"location":"tokenizers/#common-problems","text":"","title":"Common Problems"},{"location":"tokenizers/#diluted-tokens","text":"The reason why LLMs work in different ways between different languages and/or topics, is because of the Tokenizer (most of the time). This can be due to different reasons, but if we have a look on how GPT2 Tokenizer behaves with Korean with respect to english, we can see an interesting pattern. The tokens required to tokenize a Korean text are much more than the ones required to tokenize a similar english text. This is because the tokenizer has been trained on english text mainly. The effect is that the output tokens for the Korean text are much more sparse, and therefore the LLM does not work very well.","title":"Diluted Tokens"},{"location":"tokenizers/#whitespace-character","text":"One key component while tokenizing code text is how to tokenize space. GPT2 use a single token for each space, leading to a Diluted Tokens problem. GPT4o Tokenizer (cl100k_base) improved this behaviour by using less tokens for the indentation.","title":"Whitespace Character"},{"location":"tokenizers/#characteristics","text":"","title":"Characteristics"},{"location":"tokenizers/#tokens-vocabulary","text":"It represents the tokens space through which the tokenizer can convert the text. The bigger is this vocabulary, the lesser tokens would be required to tokenize a text. More tokens in the vocabulary means that the tokenizer would be able to better represent the meaning of a text, fighting the \"Diluted Tokens\" problem. However, too big vocabulary might influence the LLM softmax function for the token sampling while constructing its output.","title":"Tokens Vocabulary"},{"location":"tokenizers/#splitting","text":"The very first step of any Encoder is the text split. GPT-2, for example, does that through a REGEX, which can work differently between lowercase and uppercase. Also, the way in which the sentence is separated before encoding it can affect the final result. There are also rules when need to split text that includes, for example, code. This is the main change between GPT-2 and GPT-4 Tokenizers.","title":"Splitting"},{"location":"tokenizers/#techniques","text":"","title":"Techniques"},{"location":"tokenizers/#byte-pair-encoding-bpe","text":"It is a technique that wants to compress the output encoding by encoding together the pairs of most common bytes. It is essentially a token embeddings' algorithm. For example: [aaabbaabaacaa] \u2192 The sequence \"aa\" is the most common \u2192 Z = aa \u2192 [ZabbZbZcZ] This would reduce the length of the output sequence. This can be done recursively and shortening each time the output sequence. It is possible to perform a hyperparameter tuning process in order to understand which is the best Vocabulary size that has the best compression (i.e, the number of times we repeat the Byte Pair Encoding).","title":"Byte Pair Encoding (BPE)"},{"location":"tokenizers/#special-tokens","text":"With the TikToken library it is possible to add special tokens to the Encoder. For example the <endoftext> and assign it with an unused index.","title":"Special Tokens"},{"location":"tokenizers/#training","text":"","title":"Training"},{"location":"tokenizers/#general","text":"The Tokenizer has its own training set, separated from the LLM's training. Taking into account the training dataset and the \"Diluted Tokens\" problem, it becomes clear that, the more words in the Tokenizer sees in the training dataset that are, for example, in Japanese, the better the Tokenizer would group up these words into the same token. In this way, it would represent a Japanese sentence with far less tokens.","title":"General"},{"location":"tokenizers/#libraries_1","text":"","title":"Libraries"},{"location":"tokenizers/#sentencepiece","text":"Unlike TikToken, SentencePiece is pretty good with both training and inference: TikToken - Encode to UTF-8 and then applied BPE SentencePiece - Apply BPE directly on the code points (and eventually falls back to UTF-8 for certain code points)","title":"SentencePiece"},{"location":"transformers/","text":"Transformers History Before the invention of transformers1, recurrent neural networks (RNNSs) were the popular approach for modeling sequences. However, RNNs process input and output sequences sequentially, while Transformers can do it in parallel thanks to the self-attention mechanism. Although the cost has highly increased (Quadratic of the context length). Transformers are originally born for Natural Language Processing and, in particular, Translation problems. The Evolution of Transformers GPT-1 was a decoder-only that combined an unsupervised pre-training over a large amount of labelled data and then fine-tuned for a specific task. The model was very limited, since it could generalise to tasks that are similar to the task it was trained on. BERT which stands for Bidirectional Encoder Representations from Transformers, distinguishes itself from traditional encoder-decoder transformer models by being an encoder-only architecture. It was able to understand the context through an MLM training. GPT-2 was trained on a bigger dataset (40GB) and with 1.5B parameters. It was able to capture longer-range dependencies and common sense reasoning. Resources The Illustrated Transformer Attention is All You Need Applications The transformer architecture was developed at Google in 2017 for use in a translation model. It\u2019s a sequence-to-sequence model capable of converting sequences from one domain into sequences in another domains. Other applications involve: NLP - The main area is for sure everything that concerns Natural Language Processing, where Transformers were born CV - Transformers have been also tested in Computer Vision field with promising results Process Overview The Transformer can be seen as a sequence of different steps: Tokenization (NLP only) \u2192 Convert raw text into a sequence of discrete tokens (e.g., words, subwords), often represented as integers \u2192 Output shape (sequence_length,) Embeddings \u2192 Expand the input sequence representation in order to: a) Increase representatioin power; b) Matche the Encoder expected size \u2192 Output shape (sequence_length, d_model) Positional Encoding \u2192 Encode the position of the original sequence into the new expanded representation (Output of the Embedding), so the model won't forget the original position \u2192 Output shape (sequence_length, d_model) Encoder \u2192 Applies layers of self-attention and feedforward networks to produce contextualized representations of each input token, allowing the model to capture dependencies across the sequence \u2192 Output shape (input_dim, d_model) Decoder \u2192 enerates the output sequence one token (or time step) at a time, using the encoder\u2019s output as context, plus its own previously generated outputs (via masked self-attention) \u2192 Output shape (target_sequence_length, d_model) Architecture Introduction There are two main elements that differentiate how different LLM works: The Architecture (e.g., decoder only, encoder only, etc.) The training process (e.g., pre-training + fine-tuning + etc.) In this section we're going to explore the architecture element of LLMs. General The basic architecture of a Transformer is composed by Encoders and Decoders . In the original paper were used 6 Encoder blocks and 6 Decoder blocks. There are of course many alternatives: Decoders Only - Like GPT-3 and GPT-2 Encoders Only - Like BERT Encoder-Decoder Models - Like BART or T5 The above alternatives may also vary for the number of Encoder and/or Decoder blocks used: BERT has 24 Encoder blocks GTP-2 has 32 Decoder blocks Input Tokens The input of a Transformer, and in general of each Neural Network, is a Tensor. In this case, it is obtained from the input text through an Embedding Algorithm . The embedded input is also called as Tokens and its length depends on the Transformer architecture. Usually, if the input text is not long enough, Padding is used to fill the missing tokens. It is also important to know that there are Special tokens used by the Neural Network to mark: [CLS] or 101 - Start of the sentence [SEP] or 102 - Separator of sentences [MASK] or 103 - Mask token for MLM (Masked Language Model) Vocabulary It is important to notice that, when for example the word \"The\" is tokenized into the token \"464\" , this number is actually an index to an Embedding Matrix . The model holds an Embedding Matrix in which each word is represented to a Tensor of, let's say, 768 dimension. GPT-2 has a 50.257 x 768 vocabulary: 50.257 words 768 dimension (each word is represented through 768 numbers) Input Preparation and Embedding To prepare language inputs for transformers, we convert an input sequence into tokens and then into input embeddings. Generating an input embedding involves the following steps: Normalization (Optional): Standardizes text by removing redundant whitespace, accents, etc. Tokenization: Breaks the sentence into words or subwords and maps them to integer token IDs from a vocabulary. Embedding: Converts each token ID to its corresponding high-dimensional vector, typically using a lookup table. These can be learned during the training process. Positional Encoding: Adds information about the position of each token in the sequence to help the transformer understand word order. Encoder Scope An encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding vector) of a fixed length. This representation is expected to be a good summary of the meaning of the whole source sequence. It builds a contextual representation of the input sequence. Architecture The Encoder block is composed by a Self-Attention layer and a Feed Forward Neural Network . The encoder can also be implemented as an RNN (i.e., using LSTM and GRU). An inherit problem of Encoder is the fixed-length context, which makes impossible to remember long sequences. The Attention Mechanism addressed this problem. Positional Encoder It is a technique used to store the original positions of tokens within a sequence. In this way, the tokens can also be processed in parallel while preserving the original position. The most common technique is to add a fixed-length vectors to the input embeddings of each token. These vectors are designed to represent the position of the token in the sequence. By default, the Transformer is therefore position-agnostic and, through the Positional Encoder, the computed positional encodings are added to the token embeddings before feeding them into the transformer.. Decoder Scope A decoder is initialized with the context vector defined in the Encoder to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state. It focuses only on autoregressive decoding, in which is new token is generated sequentially from the previous one (Autoregressive). Architecture It has a similar architecture that an encoder block, but with an additional layer in the middle to help focus on relevant part of the input sentence. Normalisation Layer Layer normalization computes the mean and variance of the activations to normalize the activations in a given layer. This is typically performed to reduce covariate shift as well as improve gradient flow to yield faster convergence during training as well as improved overall performance. Residual Connections Residual connections propagate the inputs to the output of one or more layers. This has the effect of making the optimization procedure easier to learn and also helps deal with vanishing and exploding gradients. Feedforward Layer This layer applies a position-wise transformation to the data, independently for each position in the sequence, which allows the incorporation of additional non-linearity and complexity into the model\u2019s representations Attention Mechanism Key Concept It is a mechanism that allows the model to look at other positions in the input sequence to get a better understanding of token. Consider the sentence \"The animal didn't cross the street because it was too tired\" The word \"it\" refers to the \"Animal\" : The main goal is to help memorise long sequences, by improving the compression mechanism of the encoder-decoder architecture. Characteristics Hard vs. Soft Attention In the paper Show, Attend and Tell the problem of image caption generation has been analysed. From that paper, two approaches of Attention Mechanism have been derived: Soft Attention - It applies the weights alignment over all the patches of the source image. It's an expensive approach if the image is large, but the model is differentiable. Hard Attention - It applies the weights alignment only on a singe patch of the source image. It's less expensive, but the model is non-differentiable and thus requires further techniques to be trained. Global vs. Local Attention In the paper Effective Approaches to Attention-based Neural Machine Translation the difference between Global and Local Attention has been proposed. Global Attention - It is similar to the \"Soft Attention\" mechanism Local Attention - It is a mix between the Hard and Soft Attention mechanisms (The model first predicts the aligned position in the current sequence and then center the context window over that position) Dot-Product Attention Definition It is another kind of attention mechanism, together with Self-Attention and Cross-Attention. Let\u2019s see an example for text translation with the below architecture of Encoder (left) and Decoder (right). Processing There would be three vectors to compare: Values (V) and Keys (K) derived from the input sentence in english Query (Q) derived from the input sentence in italian Notice that, in order to match the length of the english sentence (4), a padding token has been added at the end of the italian sentence, whose original size was 3. The Dot-Product is computed between K and Q and then passed to a Softmax function. Finally, in order to compute the Dot-Product attention z, the Dot-Product is computed between V and the vector resulting from the Softmax function. This dot product would result in very similar values for the token that match together: Hello - Ciao, How . Self-Attention Definition It has the same exact process as in the Dot-Product Attention . However here the vectors V, K and Q are built from the same exact input sentence and the attention is computed only for tokens in the past of the sentence, never in the future. Self Attention vs. Masked Self-Attention Sometimes, the difference between the inability of looking at future token in the sentence is referred as \"Masked Self-Attention\" . On the other hand, standard Self-Attention allows this possibility to look at future tokens. It's a matter of terminology. Key Concept This is the most important aspect of Transformer that differentiate it from traditional sequence models. Upon having a sequence of different tokens, like tensor [18, 47, 56, 57, 58, 1, 15, 47, 58] , the model will start constructing a matrix ( Attention Matrix ) of dimension Token size x Token size (9 x 9 in our example). Each element of this matrix is going to be the weight that the specific token i would assign to another token j , depending on the importance it would give to the token j . The value -inf or 0 would reflect the fact that the token can not communicate with that token because it is in the future of the sequence.` Processing Step 1 - Compute Query, Key and Value Tensors From a single input tensor of tokens, three other tensors are generated: Query Tensor - It is a representation of the current token used to score against all the other tokens. It represents \"What we're looking for \" or \"Which other words in the sequence are relevant to me?\" . Key Tensor - It holds like the labels for all the tokens in the segment. It is what it matches against in the search for relevant tokens. It represents what a token can offer. Value Tensor - It is the actual content of the tokens Such three tensors are generated by multiplying the input tensor for three matrices that has been created during the training process. Step 2 - Compute Self-Attention For each token in the input sequence, the Attention Score with respect to each other token is computed. That is computed by the dot-product of the Query Tensor and the Key Tensor of that token: q_1 * k_1 q_1 * k_2 ... q_1 * k_n This is done in order to understand how well the analysed token matches with all the others. Step 3 - Standardisation Standardise the Attention Score by, let's say, the square dimension of the tensors. Step 4 - Softmax Function Pass the standardised attention score in Softmax function to normalise it. Step 5 - Retrieve Relevant Tokens Multiply the Value Tensor by the Softmax score, in order to retrieve relevant tokens and their content. Step 6 - Compute Self-Attention Sum up the weighted value vectors to obtain the self-attention matrix. Aggregated Weighted Context It is one of the first approach for of Self-Attention . Consider the sequence: [18, 47, 56, 57, 58, 1, 15, 47, 58] While training the token 56 , the algorithm should retrieve its context [18, 47] in order to learn the next token 57 . Passing the whole context everytime is expensive. That\u2019s why in Transformer architecture, it\u2019s better to pass a more concise representation of the previous context: an aggregated weighted context . A very simplified version adopts just the average: Token: 56 Context: [18, 47] \u2192 41.5 (avg) Target: 57 Multi-Head Attention Comparison with Single-Head Attention It expands the model\u2019s ability to focus on different positions. It has multiple representation subspaces by using multiple triplets of \"Query, Key and Value\" vectors. It is used to project the input embeddings into multiple subspaces. Process Given the multiple Query, Key and Value vectors in the Self-Attention layer, there is the need to condense all of them into a single one, in order to be fed to the Feed Forward layer. They are concatenated and multiplied by a weight matrix. Training Process General There are different approaches to formulating the training task for transformers depending on the architecture used: Decoder-only models are typically pre-trained on the language modeling task Encoder-only models (like BERT) are often pre-trained by corrupting the input sequence in some way and having the model try to reconstruct it. One such approach is masked language modeling (MLM). Encoder-decoder models (like the original transformer) are trained on sequence-to- sequence supervised tasks such as translation Steps The Training process is composed by 3 main steps: Pre-Training Fine-Tuning Alignment (RLHF) Pre-Training Objective Given an input sequence of tokens (e.g., words), predict the next token. This is generally known as Pre-Training phase, and it is done by feeding to the Transformer a huge amount of Internet text data. Data Preparation The massive dataset required for the pre-training has to be carefully curated by: Cleaning data Remove duplicates Tokenization Remove problematic data Distributed Training It is a combination of different parallelisation strategies: Data Parallel - Split the training dataset into batches and run them on parallel GPUs at the same time. The only requirement is to add gradient synchronisation at the end of each batch step. Since gradients are computed in parallel, there is not a linear decrease of the training time. Feasible only if the memory is not a constraint. Model Parallelism - If the memory is a constraint, this strategy allows to split the model into several GPUs, thus reducing the memory requirement as the number of GPUs increases. Pipeline Parallel - Each layer is loaded into a GPU. Tensor Parallel - It splits each layer into multiple GPUs, further refining the Pipeline Parallelism Training Optimisation The goal is to optimise the training by using different strategies: Adaptive Learning Rates with Warmup Don't use a fixed learning rate, but adapt it dynamically during the training In order to help the model not diverging at the start, initially use a very low learning rate (warmup) Gradually increase the learning rate from the warmup value to the target value over a thousand in order to prevent sudden large updates which might destabilise the training In order to ensure a stable convergence, reduce the learning rate over time through a decay strategy (Linear, Cosine or Exponential) Gradient Clipping Cap the magnitude of the gradient to a certain threshold It prevents Exploding Gradients (Gradients become too big) Normalisation Usage of techniques to ensure stable activations and gradients, such as: Batch Norm, Layer Norm and Weight Norm Reduce internal covariate shift in order to speed-up convergence Prevents Exploding Gradients Mixed-Precision Training for Memory Efficiency Use lower representation numbers to restrain excessive memory usage Optimisers AdamW (Adam with Weight Decay) - It improves Adam by decoupling the weights decays from the gradients updates Lion (EvoLved Sign Momentum) - Uses sign-based updates instead of raw gradients, leading to faster convergence and it works well with low-rank parameterization in transformers Fine-Tuning In order to be useful enough, after the Pre-Training operation, the model goes to another training step called Fine-Tuning . See the dedicated file fine_tuning_theory.md . Inference Process Auto-Regression The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called \u201cAuto-Regression\u201d . This feature is not always incorporated. For example, BERT does not have it. Projecting the Output The Transformer network works through the following step: Transform input text into token IDs Feed the token IDs into the Transformer Hidden State is a tensor of probabilities with the dimension of the Transformer vocabulary At this point, in order to transform the number back to a text, the network picks up the max probability through a Softmax function from the hidden state tensor, which will correspond to a word in the vocabulary. Generating Samples There are two different ways for a Transformer to start generating words: Generating Interactive Conditional Samples - The model is provided with a prompt Generating Unconditional Samples - The model is provided with a Start Token ( <|endoftext|> ) and it starts generating words on its own In both cases, the Auto-Regression kicks-in.","title":"Transformers"},{"location":"transformers/#transformers","text":"","title":"Transformers"},{"location":"transformers/#history","text":"Before the invention of transformers1, recurrent neural networks (RNNSs) were the popular approach for modeling sequences. However, RNNs process input and output sequences sequentially, while Transformers can do it in parallel thanks to the self-attention mechanism. Although the cost has highly increased (Quadratic of the context length). Transformers are originally born for Natural Language Processing and, in particular, Translation problems.","title":"History"},{"location":"transformers/#the-evolution-of-transformers","text":"GPT-1 was a decoder-only that combined an unsupervised pre-training over a large amount of labelled data and then fine-tuned for a specific task. The model was very limited, since it could generalise to tasks that are similar to the task it was trained on. BERT which stands for Bidirectional Encoder Representations from Transformers, distinguishes itself from traditional encoder-decoder transformer models by being an encoder-only architecture. It was able to understand the context through an MLM training. GPT-2 was trained on a bigger dataset (40GB) and with 1.5B parameters. It was able to capture longer-range dependencies and common sense reasoning.","title":"The Evolution of Transformers"},{"location":"transformers/#resources","text":"The Illustrated Transformer Attention is All You Need","title":"Resources"},{"location":"transformers/#applications","text":"The transformer architecture was developed at Google in 2017 for use in a translation model. It\u2019s a sequence-to-sequence model capable of converting sequences from one domain into sequences in another domains. Other applications involve: NLP - The main area is for sure everything that concerns Natural Language Processing, where Transformers were born CV - Transformers have been also tested in Computer Vision field with promising results","title":"Applications"},{"location":"transformers/#process-overview","text":"The Transformer can be seen as a sequence of different steps: Tokenization (NLP only) \u2192 Convert raw text into a sequence of discrete tokens (e.g., words, subwords), often represented as integers \u2192 Output shape (sequence_length,) Embeddings \u2192 Expand the input sequence representation in order to: a) Increase representatioin power; b) Matche the Encoder expected size \u2192 Output shape (sequence_length, d_model) Positional Encoding \u2192 Encode the position of the original sequence into the new expanded representation (Output of the Embedding), so the model won't forget the original position \u2192 Output shape (sequence_length, d_model) Encoder \u2192 Applies layers of self-attention and feedforward networks to produce contextualized representations of each input token, allowing the model to capture dependencies across the sequence \u2192 Output shape (input_dim, d_model) Decoder \u2192 enerates the output sequence one token (or time step) at a time, using the encoder\u2019s output as context, plus its own previously generated outputs (via masked self-attention) \u2192 Output shape (target_sequence_length, d_model)","title":"Process Overview"},{"location":"transformers/#architecture","text":"","title":"Architecture"},{"location":"transformers/#introduction","text":"There are two main elements that differentiate how different LLM works: The Architecture (e.g., decoder only, encoder only, etc.) The training process (e.g., pre-training + fine-tuning + etc.) In this section we're going to explore the architecture element of LLMs.","title":"Introduction"},{"location":"transformers/#general","text":"The basic architecture of a Transformer is composed by Encoders and Decoders . In the original paper were used 6 Encoder blocks and 6 Decoder blocks. There are of course many alternatives: Decoders Only - Like GPT-3 and GPT-2 Encoders Only - Like BERT Encoder-Decoder Models - Like BART or T5 The above alternatives may also vary for the number of Encoder and/or Decoder blocks used: BERT has 24 Encoder blocks GTP-2 has 32 Decoder blocks","title":"General"},{"location":"transformers/#input","text":"","title":"Input"},{"location":"transformers/#tokens","text":"The input of a Transformer, and in general of each Neural Network, is a Tensor. In this case, it is obtained from the input text through an Embedding Algorithm . The embedded input is also called as Tokens and its length depends on the Transformer architecture. Usually, if the input text is not long enough, Padding is used to fill the missing tokens. It is also important to know that there are Special tokens used by the Neural Network to mark: [CLS] or 101 - Start of the sentence [SEP] or 102 - Separator of sentences [MASK] or 103 - Mask token for MLM (Masked Language Model)","title":"Tokens"},{"location":"transformers/#vocabulary","text":"It is important to notice that, when for example the word \"The\" is tokenized into the token \"464\" , this number is actually an index to an Embedding Matrix . The model holds an Embedding Matrix in which each word is represented to a Tensor of, let's say, 768 dimension. GPT-2 has a 50.257 x 768 vocabulary: 50.257 words 768 dimension (each word is represented through 768 numbers)","title":"Vocabulary"},{"location":"transformers/#input-preparation-and-embedding","text":"To prepare language inputs for transformers, we convert an input sequence into tokens and then into input embeddings. Generating an input embedding involves the following steps: Normalization (Optional): Standardizes text by removing redundant whitespace, accents, etc. Tokenization: Breaks the sentence into words or subwords and maps them to integer token IDs from a vocabulary. Embedding: Converts each token ID to its corresponding high-dimensional vector, typically using a lookup table. These can be learned during the training process. Positional Encoding: Adds information about the position of each token in the sequence to help the transformer understand word order.","title":"Input Preparation and Embedding"},{"location":"transformers/#encoder","text":"","title":"Encoder"},{"location":"transformers/#scope","text":"An encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding vector) of a fixed length. This representation is expected to be a good summary of the meaning of the whole source sequence. It builds a contextual representation of the input sequence.","title":"Scope"},{"location":"transformers/#architecture_1","text":"The Encoder block is composed by a Self-Attention layer and a Feed Forward Neural Network . The encoder can also be implemented as an RNN (i.e., using LSTM and GRU). An inherit problem of Encoder is the fixed-length context, which makes impossible to remember long sequences. The Attention Mechanism addressed this problem.","title":"Architecture"},{"location":"transformers/#positional-encoder","text":"It is a technique used to store the original positions of tokens within a sequence. In this way, the tokens can also be processed in parallel while preserving the original position. The most common technique is to add a fixed-length vectors to the input embeddings of each token. These vectors are designed to represent the position of the token in the sequence. By default, the Transformer is therefore position-agnostic and, through the Positional Encoder, the computed positional encodings are added to the token embeddings before feeding them into the transformer..","title":"Positional Encoder"},{"location":"transformers/#decoder","text":"","title":"Decoder"},{"location":"transformers/#scope_1","text":"A decoder is initialized with the context vector defined in the Encoder to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state. It focuses only on autoregressive decoding, in which is new token is generated sequentially from the previous one (Autoregressive).","title":"Scope"},{"location":"transformers/#architecture_2","text":"It has a similar architecture that an encoder block, but with an additional layer in the middle to help focus on relevant part of the input sentence.","title":"Architecture"},{"location":"transformers/#normalisation-layer","text":"Layer normalization computes the mean and variance of the activations to normalize the activations in a given layer. This is typically performed to reduce covariate shift as well as improve gradient flow to yield faster convergence during training as well as improved overall performance.","title":"Normalisation Layer"},{"location":"transformers/#residual-connections","text":"Residual connections propagate the inputs to the output of one or more layers. This has the effect of making the optimization procedure easier to learn and also helps deal with vanishing and exploding gradients.","title":"Residual Connections"},{"location":"transformers/#feedforward-layer","text":"This layer applies a position-wise transformation to the data, independently for each position in the sequence, which allows the incorporation of additional non-linearity and complexity into the model\u2019s representations","title":"Feedforward Layer"},{"location":"transformers/#attention-mechanism","text":"","title":"Attention Mechanism"},{"location":"transformers/#key-concept","text":"It is a mechanism that allows the model to look at other positions in the input sequence to get a better understanding of token. Consider the sentence \"The animal didn't cross the street because it was too tired\" The word \"it\" refers to the \"Animal\" : The main goal is to help memorise long sequences, by improving the compression mechanism of the encoder-decoder architecture.","title":"Key Concept"},{"location":"transformers/#characteristics","text":"","title":"Characteristics"},{"location":"transformers/#hard-vs-soft-attention","text":"In the paper Show, Attend and Tell the problem of image caption generation has been analysed. From that paper, two approaches of Attention Mechanism have been derived: Soft Attention - It applies the weights alignment over all the patches of the source image. It's an expensive approach if the image is large, but the model is differentiable. Hard Attention - It applies the weights alignment only on a singe patch of the source image. It's less expensive, but the model is non-differentiable and thus requires further techniques to be trained.","title":"Hard vs. Soft Attention"},{"location":"transformers/#global-vs-local-attention","text":"In the paper Effective Approaches to Attention-based Neural Machine Translation the difference between Global and Local Attention has been proposed. Global Attention - It is similar to the \"Soft Attention\" mechanism Local Attention - It is a mix between the Hard and Soft Attention mechanisms (The model first predicts the aligned position in the current sequence and then center the context window over that position)","title":"Global vs. Local Attention"},{"location":"transformers/#dot-product-attention","text":"","title":"Dot-Product Attention"},{"location":"transformers/#definition","text":"It is another kind of attention mechanism, together with Self-Attention and Cross-Attention. Let\u2019s see an example for text translation with the below architecture of Encoder (left) and Decoder (right).","title":"Definition"},{"location":"transformers/#processing","text":"There would be three vectors to compare: Values (V) and Keys (K) derived from the input sentence in english Query (Q) derived from the input sentence in italian Notice that, in order to match the length of the english sentence (4), a padding token has been added at the end of the italian sentence, whose original size was 3. The Dot-Product is computed between K and Q and then passed to a Softmax function. Finally, in order to compute the Dot-Product attention z, the Dot-Product is computed between V and the vector resulting from the Softmax function. This dot product would result in very similar values for the token that match together: Hello - Ciao, How .","title":"Processing"},{"location":"transformers/#self-attention","text":"","title":"Self-Attention"},{"location":"transformers/#definition_1","text":"It has the same exact process as in the Dot-Product Attention . However here the vectors V, K and Q are built from the same exact input sentence and the attention is computed only for tokens in the past of the sentence, never in the future.","title":"Definition"},{"location":"transformers/#self-attention-vs-masked-self-attention","text":"Sometimes, the difference between the inability of looking at future token in the sentence is referred as \"Masked Self-Attention\" . On the other hand, standard Self-Attention allows this possibility to look at future tokens. It's a matter of terminology.","title":"Self Attention vs. Masked Self-Attention"},{"location":"transformers/#key-concept_1","text":"This is the most important aspect of Transformer that differentiate it from traditional sequence models. Upon having a sequence of different tokens, like tensor [18, 47, 56, 57, 58, 1, 15, 47, 58] , the model will start constructing a matrix ( Attention Matrix ) of dimension Token size x Token size (9 x 9 in our example). Each element of this matrix is going to be the weight that the specific token i would assign to another token j , depending on the importance it would give to the token j . The value -inf or 0 would reflect the fact that the token can not communicate with that token because it is in the future of the sequence.`","title":"Key Concept"},{"location":"transformers/#processing_1","text":"","title":"Processing"},{"location":"transformers/#step-1-compute-query-key-and-value-tensors","text":"From a single input tensor of tokens, three other tensors are generated: Query Tensor - It is a representation of the current token used to score against all the other tokens. It represents \"What we're looking for \" or \"Which other words in the sequence are relevant to me?\" . Key Tensor - It holds like the labels for all the tokens in the segment. It is what it matches against in the search for relevant tokens. It represents what a token can offer. Value Tensor - It is the actual content of the tokens Such three tensors are generated by multiplying the input tensor for three matrices that has been created during the training process.","title":"Step 1 - Compute Query, Key and Value Tensors"},{"location":"transformers/#step-2-compute-self-attention","text":"For each token in the input sequence, the Attention Score with respect to each other token is computed. That is computed by the dot-product of the Query Tensor and the Key Tensor of that token: q_1 * k_1 q_1 * k_2 ... q_1 * k_n This is done in order to understand how well the analysed token matches with all the others.","title":"Step 2 - Compute Self-Attention"},{"location":"transformers/#step-3-standardisation","text":"Standardise the Attention Score by, let's say, the square dimension of the tensors.","title":"Step 3 - Standardisation"},{"location":"transformers/#step-4-softmax-function","text":"Pass the standardised attention score in Softmax function to normalise it.","title":"Step 4 - Softmax Function"},{"location":"transformers/#step-5-retrieve-relevant-tokens","text":"Multiply the Value Tensor by the Softmax score, in order to retrieve relevant tokens and their content.","title":"Step 5 - Retrieve Relevant Tokens"},{"location":"transformers/#step-6-compute-self-attention","text":"Sum up the weighted value vectors to obtain the self-attention matrix.","title":"Step 6 - Compute Self-Attention"},{"location":"transformers/#aggregated-weighted-context","text":"It is one of the first approach for of Self-Attention . Consider the sequence: [18, 47, 56, 57, 58, 1, 15, 47, 58] While training the token 56 , the algorithm should retrieve its context [18, 47] in order to learn the next token 57 . Passing the whole context everytime is expensive. That\u2019s why in Transformer architecture, it\u2019s better to pass a more concise representation of the previous context: an aggregated weighted context . A very simplified version adopts just the average: Token: 56 Context: [18, 47] \u2192 41.5 (avg) Target: 57","title":"Aggregated Weighted Context"},{"location":"transformers/#multi-head-attention","text":"","title":"Multi-Head Attention"},{"location":"transformers/#comparison-with-single-head-attention","text":"It expands the model\u2019s ability to focus on different positions. It has multiple representation subspaces by using multiple triplets of \"Query, Key and Value\" vectors. It is used to project the input embeddings into multiple subspaces.","title":"Comparison with Single-Head Attention"},{"location":"transformers/#process","text":"Given the multiple Query, Key and Value vectors in the Self-Attention layer, there is the need to condense all of them into a single one, in order to be fed to the Feed Forward layer. They are concatenated and multiplied by a weight matrix.","title":"Process"},{"location":"transformers/#training-process","text":"","title":"Training Process"},{"location":"transformers/#general_1","text":"There are different approaches to formulating the training task for transformers depending on the architecture used: Decoder-only models are typically pre-trained on the language modeling task Encoder-only models (like BERT) are often pre-trained by corrupting the input sequence in some way and having the model try to reconstruct it. One such approach is masked language modeling (MLM). Encoder-decoder models (like the original transformer) are trained on sequence-to- sequence supervised tasks such as translation","title":"General"},{"location":"transformers/#steps","text":"The Training process is composed by 3 main steps: Pre-Training Fine-Tuning Alignment (RLHF)","title":"Steps"},{"location":"transformers/#pre-training","text":"","title":"Pre-Training"},{"location":"transformers/#objective","text":"Given an input sequence of tokens (e.g., words), predict the next token. This is generally known as Pre-Training phase, and it is done by feeding to the Transformer a huge amount of Internet text data.","title":"Objective"},{"location":"transformers/#data-preparation","text":"The massive dataset required for the pre-training has to be carefully curated by: Cleaning data Remove duplicates Tokenization Remove problematic data","title":"Data Preparation"},{"location":"transformers/#distributed-training","text":"It is a combination of different parallelisation strategies: Data Parallel - Split the training dataset into batches and run them on parallel GPUs at the same time. The only requirement is to add gradient synchronisation at the end of each batch step. Since gradients are computed in parallel, there is not a linear decrease of the training time. Feasible only if the memory is not a constraint. Model Parallelism - If the memory is a constraint, this strategy allows to split the model into several GPUs, thus reducing the memory requirement as the number of GPUs increases. Pipeline Parallel - Each layer is loaded into a GPU. Tensor Parallel - It splits each layer into multiple GPUs, further refining the Pipeline Parallelism","title":"Distributed Training"},{"location":"transformers/#training-optimisation","text":"The goal is to optimise the training by using different strategies:","title":"Training Optimisation"},{"location":"transformers/#adaptive-learning-rates-with-warmup","text":"Don't use a fixed learning rate, but adapt it dynamically during the training In order to help the model not diverging at the start, initially use a very low learning rate (warmup) Gradually increase the learning rate from the warmup value to the target value over a thousand in order to prevent sudden large updates which might destabilise the training In order to ensure a stable convergence, reduce the learning rate over time through a decay strategy (Linear, Cosine or Exponential)","title":"Adaptive Learning Rates with Warmup"},{"location":"transformers/#gradient-clipping","text":"Cap the magnitude of the gradient to a certain threshold It prevents Exploding Gradients (Gradients become too big)","title":"Gradient Clipping"},{"location":"transformers/#normalisation","text":"Usage of techniques to ensure stable activations and gradients, such as: Batch Norm, Layer Norm and Weight Norm Reduce internal covariate shift in order to speed-up convergence Prevents Exploding Gradients","title":"Normalisation"},{"location":"transformers/#mixed-precision-training-for-memory-efficiency","text":"Use lower representation numbers to restrain excessive memory usage","title":"Mixed-Precision Training for Memory Efficiency"},{"location":"transformers/#optimisers","text":"AdamW (Adam with Weight Decay) - It improves Adam by decoupling the weights decays from the gradients updates Lion (EvoLved Sign Momentum) - Uses sign-based updates instead of raw gradients, leading to faster convergence and it works well with low-rank parameterization in transformers","title":"Optimisers"},{"location":"transformers/#fine-tuning","text":"In order to be useful enough, after the Pre-Training operation, the model goes to another training step called Fine-Tuning . See the dedicated file fine_tuning_theory.md .","title":"Fine-Tuning"},{"location":"transformers/#inference-process","text":"","title":"Inference Process"},{"location":"transformers/#auto-regression","text":"The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called \u201cAuto-Regression\u201d . This feature is not always incorporated. For example, BERT does not have it.","title":"Auto-Regression"},{"location":"transformers/#projecting-the-output","text":"The Transformer network works through the following step: Transform input text into token IDs Feed the token IDs into the Transformer Hidden State is a tensor of probabilities with the dimension of the Transformer vocabulary At this point, in order to transform the number back to a text, the network picks up the max probability through a Softmax function from the hidden state tensor, which will correspond to a word in the vocabulary.","title":"Projecting the Output"},{"location":"transformers/#generating-samples","text":"There are two different ways for a Transformer to start generating words: Generating Interactive Conditional Samples - The model is provided with a prompt Generating Unconditional Samples - The model is provided with a Start Token ( <|endoftext|> ) and it starts generating words on its own In both cases, the Auto-Regression kicks-in.","title":"Generating Samples"},{"location":"vector_search/","text":"Vector Search Definition Full-text keyword search has been the lynchpin of modern IT systems for years. However, if the key word is misspelled or described with a differently worded text, a traditional keyword search returns incorrect or no results. This is where vector search is very powerful: it uses the vector or embedded semantic representation of documents. Process After you have a function that can compute embeddings of various items, you compute the embedding of the items of interest and store this embedding in a database. You then embed the incoming query in the same vector space as the items. Next, you have to find the best matches to the query. This process is analogous to finding the most \u2018similar\u2019 matches across the entire collection of searchable vectors: similarity between vectors can be computed using a metric such as euclidean distance, cosine similarity, or dot product. Vector Search Algorithms Introduction The most straightforward way to find the most similar match is to run a traditional linear search by comparing the query vector with each document vector and return the one with the highest similarity. However, the runtime of this approach scales linearly (O(N)) with the amount of documents or items to search. This approach is unacceptably slow for most use cases involving several millions of documents or more. Using approximate nearest neighbour (ANN) search for that purpose is more practical. ANN is a technique for finding the closest points to a given point in a dataset with a small margin of error. Locality Sensitive Hashing (LSH) Locality sensitive hashing (LSH) 28 is a technique for finding similar items in a large dataset. It does this by creating one or more hash functions that map similar items to the same hash bucket with high probability. The number of hash functions/tables and buckets determine the search recall/speed tradeoff, as well as the false positive / true positive one. Having too many hash functions might cause similar items to different buckets, while too few might result in too many items falsely being hashed to the same bucket and the number of linear searches to increase. from sklearn.neighbors import NearestNeighbors from vertexai.language_models import TextEmbeddingModel from lshashing import LSHRandom import numpy as np model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\") test_items= [ \"The earth is spherical.\", \"The earth is a planet.\", \"I like to eat at a restaurant.\"] query = \"the shape of earth\" embedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)]) embedded_query = np.array(model.get_embeddings([query])[0].values) #Naive brute force search n_neighbors=2 nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='brute').fit(embedded_test_items) naive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0)) #algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(embedded_test_items) distances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0)) #LSH lsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True) lsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True) #output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours #ANN retrieved the same ranking of items as brute force in a much scalable manner FAISS One of the FAISS (Facebook AI similarity search) implementations leverages the concept of hierarchical navigable small world (HNSW) 31 to perform vector similarity search in sub- linear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a hierarchical structure where the graph links are spread across different layers. The top layer has the longest links and the bottom layer has the shortest ones. As shown in Figure 9, the search starts at the topmost layer where the algorithm greedily traverses the graph to find the vertex most semantically similar to the query. Once the local minimum for that layer is found, it then switches to the graph for the closest vertex on the layer below. ScaNN The first step is the optional partitioning step during training: it uses one of the multiple algorithms available to partition the vector store into logical partitions/clusters where the semantically related are grouped together. At query time ScaNN uses the user-specified distance measure to select the specified number of top partitions (a value specified by the user), and then executes the scoring step next. Finally, as a last step the user can optionally choose to rescore the user specified top K number of results more accurately. Vector Databases Definition Each vector database differs in its implementation, but the general flow is: An appropriate trained embedding model is used to embed the relevant data points as vectors with fixed dimensions. The vectors are then augmented with appropriate metadata and complementary information (such as tags) and indexed using the specified algorithm for efficient search. An incoming query gets embedded with the same model, and used to query and return specific amounts of the most semantically similar items and their associated unembedded content/metadata. Some databases might provide caching and pre-filtering (based on tags) and post-filtering capabilities (reranking using another more accurate model) to further enhance the query speed and performance. Challenges Embeddings, unlike traditional content, can mutate over time. However, frequently updating the embeddings especially those trained on large amounts of data - can be prohibitively expensive. While embeddings are great at representing semantic information, sometimes they can be suboptimal at representing literal or syntactic information. This is especially true for domain-specific words or IDs. Applications Overview RAG Retrieval augmented generation (RAG) for Q&A is a technique that combines the best of both worlds from retrieval and generation. It first retrieves relevant documents from a knowledge base and then uses prompt expansion to generate an answer from those documents. Prompt expansion is a technique that when combined with database search can be very powerful. With prompt expansion the model retrieves relevant information from the database (mostly using a combination of semantic search and business rules), and augments the original prompt with it. RAGs can help with a common problem with LLMs: their tendency to \u2018hallucinate\u2019","title":"Vector Search"},{"location":"vector_search/#vector-search","text":"","title":"Vector Search"},{"location":"vector_search/#definition","text":"Full-text keyword search has been the lynchpin of modern IT systems for years. However, if the key word is misspelled or described with a differently worded text, a traditional keyword search returns incorrect or no results. This is where vector search is very powerful: it uses the vector or embedded semantic representation of documents.","title":"Definition"},{"location":"vector_search/#process","text":"After you have a function that can compute embeddings of various items, you compute the embedding of the items of interest and store this embedding in a database. You then embed the incoming query in the same vector space as the items. Next, you have to find the best matches to the query. This process is analogous to finding the most \u2018similar\u2019 matches across the entire collection of searchable vectors: similarity between vectors can be computed using a metric such as euclidean distance, cosine similarity, or dot product.","title":"Process"},{"location":"vector_search/#vector-search-algorithms","text":"","title":"Vector Search Algorithms"},{"location":"vector_search/#introduction","text":"The most straightforward way to find the most similar match is to run a traditional linear search by comparing the query vector with each document vector and return the one with the highest similarity. However, the runtime of this approach scales linearly (O(N)) with the amount of documents or items to search. This approach is unacceptably slow for most use cases involving several millions of documents or more. Using approximate nearest neighbour (ANN) search for that purpose is more practical. ANN is a technique for finding the closest points to a given point in a dataset with a small margin of error.","title":"Introduction"},{"location":"vector_search/#locality-sensitive-hashing-lsh","text":"Locality sensitive hashing (LSH) 28 is a technique for finding similar items in a large dataset. It does this by creating one or more hash functions that map similar items to the same hash bucket with high probability. The number of hash functions/tables and buckets determine the search recall/speed tradeoff, as well as the false positive / true positive one. Having too many hash functions might cause similar items to different buckets, while too few might result in too many items falsely being hashed to the same bucket and the number of linear searches to increase. from sklearn.neighbors import NearestNeighbors from vertexai.language_models import TextEmbeddingModel from lshashing import LSHRandom import numpy as np model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\") test_items= [ \"The earth is spherical.\", \"The earth is a planet.\", \"I like to eat at a restaurant.\"] query = \"the shape of earth\" embedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)]) embedded_query = np.array(model.get_embeddings([query])[0].values) #Naive brute force search n_neighbors=2 nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='brute').fit(embedded_test_items) naive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0)) #algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(embedded_test_items) distances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0)) #LSH lsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True) lsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True) #output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours #ANN retrieved the same ranking of items as brute force in a much scalable manner","title":"Locality Sensitive Hashing (LSH)"},{"location":"vector_search/#faiss","text":"One of the FAISS (Facebook AI similarity search) implementations leverages the concept of hierarchical navigable small world (HNSW) 31 to perform vector similarity search in sub- linear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a hierarchical structure where the graph links are spread across different layers. The top layer has the longest links and the bottom layer has the shortest ones. As shown in Figure 9, the search starts at the topmost layer where the algorithm greedily traverses the graph to find the vertex most semantically similar to the query. Once the local minimum for that layer is found, it then switches to the graph for the closest vertex on the layer below.","title":"FAISS"},{"location":"vector_search/#scann","text":"The first step is the optional partitioning step during training: it uses one of the multiple algorithms available to partition the vector store into logical partitions/clusters where the semantically related are grouped together. At query time ScaNN uses the user-specified distance measure to select the specified number of top partitions (a value specified by the user), and then executes the scoring step next. Finally, as a last step the user can optionally choose to rescore the user specified top K number of results more accurately.","title":"ScaNN"},{"location":"vector_search/#vector-databases","text":"","title":"Vector Databases"},{"location":"vector_search/#definition_1","text":"Each vector database differs in its implementation, but the general flow is: An appropriate trained embedding model is used to embed the relevant data points as vectors with fixed dimensions. The vectors are then augmented with appropriate metadata and complementary information (such as tags) and indexed using the specified algorithm for efficient search. An incoming query gets embedded with the same model, and used to query and return specific amounts of the most semantically similar items and their associated unembedded content/metadata. Some databases might provide caching and pre-filtering (based on tags) and post-filtering capabilities (reranking using another more accurate model) to further enhance the query speed and performance.","title":"Definition"},{"location":"vector_search/#challenges","text":"Embeddings, unlike traditional content, can mutate over time. However, frequently updating the embeddings especially those trained on large amounts of data - can be prohibitively expensive. While embeddings are great at representing semantic information, sometimes they can be suboptimal at representing literal or syntactic information. This is especially true for domain-specific words or IDs.","title":"Challenges"},{"location":"vector_search/#applications","text":"","title":"Applications"},{"location":"vector_search/#overview","text":"","title":"Overview"},{"location":"vector_search/#rag","text":"Retrieval augmented generation (RAG) for Q&A is a technique that combines the best of both worlds from retrieval and generation. It first retrieves relevant documents from a knowledge base and then uses prompt expansion to generate an answer from those documents. Prompt expansion is a technique that when combined with database search can be very powerful. With prompt expansion the model retrieves relevant information from the database (mostly using a combination of semantic search and business rules), and augments the original prompt with it. RAGs can help with a common problem with LLMs: their tendency to \u2018hallucinate\u2019","title":"RAG"}]}