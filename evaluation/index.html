<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>LLMs Evaluation - NexusLLM</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "LLMs Evaluation";
        var mkdocs_page_input_path = "evaluation.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> NexusLLM
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">NexusLLM</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../transformers/">Transformers</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../tokenizers/">Tokenizer</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../embeddings/">Embeddings</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../vector_search/">Vector Search</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../LLMs/">LLMs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../google_llms/">Google LLMs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../fine_tuning/">Fine-Tuning</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">LLMs Evaluation</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#areas-of-evaluation">Areas of Evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#llm-standalone-metrics">LLM Standalone Metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#online-vs-offline-evaluation">Online vs. Offline Evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#evaluation-methodologies">Evaluation Methodologies</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#llm-evaluation-metrics">LLM Evaluation Metrics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#general-list">General List</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#metrics-types">Metrics Types</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#statistics-metrics">Statistics Metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#model-based-metrics">Model-based Metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#statistical-and-model-based-scorers">Statistical and Model-Based Scorers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#usage-tips">Usage Tips</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#g-eval-model-based-scorer">G-Eval (Model-based Scorer)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#introduction_1">Introduction</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#process">Process</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#code-snippets">Code Snippets</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dag-model-based-scorer">DAG (Model-based Scorer)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#introduction_2">Introduction</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advantages">Advantages</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#code-snippets_1">Code Snippets</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#prometheus-model-based-scorer">Prometheus (Model-based Scorer)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#introduction_3">Introduction</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advantages_1">Advantages</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#qag-score-hybrid-scorer">QAG Score (Hybrid Scorer)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#introduction_4">Introduction</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advantages_2">Advantages</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#gptscore">GPTScore</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#introduction_5">Introduction</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#selfcheckgpt">SelfCheckGPT</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#introduction_6">Introduction</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#rag-specific-metrics">RAG-Specific Metrics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#faithfulness">Faithfulness</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#answer-relevancy">Answer Relevancy</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#contextual-precision">Contextual Precision</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#agentic-metrics">Agentic Metrics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#tool-correctness">Tool Correctness</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#task-completion">Task Completion</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#llm-metrics">LLM Metrics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#hallucination">Hallucination</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#toxicity">Toxicity</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#bias">Bias</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#evaluation-framework-platforms">Evaluation Framework Platforms</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#azure-ai-foundry-automated-evaluation-microsoft">Azure AI Foundry Automated Evaluation (Microsoft)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#openai-evals-openai">OpenAI Evals (OpenAI)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#weights-biases">Weights &amp; Biases</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#langsmith-langchain">LangSmith (LangChain)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#trulens-truera">TruLens (TruEra)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#vertex-ai-studio-google">Vertex AI Studio (Google)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#amazon-bedrock">Amazon Bedrock</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deepeval-confident-ai">DeepEval (Confident AI)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#parea-ai">Parea AI</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#optik-by-comet">Optik by Comet</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#code-based-llm-evaluations">Code-Based LLM Evaluations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#introduction_7">Introduction</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#use-cases">Use Cases</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../deepeval/">DeepEval</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../LLMOps/">LLMOps</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../agents/">Agents</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">NexusLLM</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">LLMs Evaluation</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/Volscente/NexusLLM/edit/master/docs/evaluation.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="llms-evaluation">LLMs Evaluation</h1>
<h2 id="introduction">Introduction</h2>
<h3 id="areas-of-evaluation">Areas of Evaluation</h3>
<p>An evaluation framework for LLMs should target two main areas:</p>
<ul>
<li><strong>Use Case</strong> or <strong>Dynamic Behavior Evaluation</strong> - Custom metrics that directly measure how well the LLM is performing regarding the specific task</li>
<li><strong>System Architecture</strong> - Generic metrics on, for example, faithfulness of information 
retrieved by the RAG or the Task Completion for AI Agents</li>
</ul>
<h3 id="llm-standalone-metrics">LLM Standalone Metrics</h3>
<p>These metrics are related to evaluate LLM against standardised benchmarks:</p>
<ul>
<li>GLUE</li>
<li>SyperGLUE</li>
<li>HellaSwag</li>
<li>TruthfulQA</li>
<li>MMLU</li>
</ul>
<h3 id="online-vs-offline-evaluation">Online vs. Offline Evaluation</h3>
<p>Offline evaluation usually proves valuable in the initial development stages of features, 
but it falls short in assessing how model changes impact the user experience in a 
live production environment.</p>
<ul>
<li><strong>Offline</strong> - Offline evaluation scrutinizes LLMs against specific datasets</li>
<li><strong>Online</strong> - Test the E2E system</li>
</ul>
<h3 id="evaluation-methodologies">Evaluation Methodologies</h3>
<p><img alt="LLM Evaluation Methodologies" src="../images/llm_evaluation_methodologies.png" /></p>
<h2 id="llm-evaluation-metrics">LLM Evaluation Metrics</h2>
<h3 id="general-list">General List</h3>
<ul>
<li><strong>Answer Relevancy</strong> - Determines whether an LLM output is able to address the given input and certain context and rules ✅</li>
<li><strong>Task Completion</strong> - Determines whether an LLM agent is able to complete the task it was set out to do ⚠️ &rarr; How to determine completion state?</li>
<li><strong>Correctness</strong> - Determines whether an LLM output is factually correct based on some ground truth ✅</li>
<li><strong>Hallucination</strong> - Determines whether an LLM output contains fake or made-up information ❌ &rarr; Impossible to determine</li>
<li><strong>Tool Correctness</strong> - Determines whether an LLM agent is able to call the correct tools for a given task ⚠️ &rarr; How to determine if the tool is correct?</li>
<li><strong>Contextual Relevancy</strong> - Determines whether the retriever in a RAG-based LLM system is able to extract the most relevant information for your LLM as context ✅ &rarr; Similar to the first one, but on each retrieved document</li>
<li><strong>Responsible Metrics</strong> - Includes metrics such as bias and toxicity, which determines whether an LLM output contains (generally) harmful and offensive content ✅</li>
<li><strong>Task-Specific Metrics</strong> - Includes metrics such as summarization, which usually contains a custom criteria depending on the use-case ✅</li>
</ul>
<h3 id="metrics-types">Metrics Types</h3>
<p>Some metrics are based on Statistics, while others are sometimes referred as <em>"Model-based"</em>:</p>
<p><img alt="LLM Metrics" src="../images/llm_metrics.png" /></p>
<h3 id="statistics-metrics">Statistics Metrics</h3>
<ul>
<li>They might perform poorly when the output implies reasoning capabilities (No semantic is included)</li>
<li>They do not take into account any</li>
</ul>
<p><strong>List of Metrics:</strong></p>
<ul>
<li><strong>BLEU (BiLingual Evaluation Understudy)</strong> - It evaluates the output of the LLM application against annotated ground truths. It calculates the precision for each matching n-gram (n consecutive words) between an LLM output and expected output to calculate their geometric mean and applies a brevity penalty if needed.</li>
<li><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong> - It is used for text summarisation and calculates recall by comparing the overlap of n-grams between LLM outputs and expected outputs.  It also leverages external linguistic databases like WordNet to account for synonyms. The final score is the harmonic mean of precision and recall, with a penalty for ordering discrepancies.</li>
<li><strong>METEOR (Metric for Evaluation of Translation with Explicit Ordering)</strong> - It calculates scores by assessing both precision (n-gram matches) and recall (n-gram overlaps), adjusted for word order differences between LLM outputs and expected outputs. It can also leverages exteral linguistic databases.</li>
<li><strong>Levenshtein distance</strong></li>
</ul>
<h3 id="model-based-metrics">Model-based Metrics</h3>
<ul>
<li>Reliable but inaccurate (struggle to keep semantic included), because of their probabilistic nature</li>
</ul>
<p><strong>List of Metrics:</strong></p>
<ul>
<li><strong>NLI</strong> - It is a Non-LLM based and uses Natural Language Inference models to classify whether an LLM output is logically consistent (entailment), contradictory, or unrelated (neutral) with respect to a given reference text.</li>
<li><strong>BLEURT (Bilingual Evaluation Understudy with Representations from Transformers)</strong> - It uses pre-trained models like BERT to score LLM outputs on some expected outputs</li>
</ul>
<h3 id="statistical-and-model-based-scorers">Statistical and Model-Based Scorers</h3>
<ul>
<li><strong>BERTScore</strong> - It relies on a pre-trained LLM like BERT and on the cosine similarity between expected output and predicted output. Afterward, the similarities are aggregated to produce a final score.</li>
<li><strong>MoverScore</strong> - It relies on LLM like BERT to obtain deeper contextualised word embeddings for both reference text and generated text before computing the similarity.</li>
</ul>
<h3 id="usage-tips">Usage Tips</h3>
<p>It is good to have:</p>
<ul>
<li>1-2 custom metrics (G-Eval or DAG) that are use case specific</li>
<li>2-3 generic metrics (RAG, agentic, or conversational) that are system specific</li>
</ul>
<h2 id="g-eval-model-based-scorer">G-Eval (Model-based Scorer)</h2>
<h3 id="introduction_1">Introduction</h3>
<ul>
<li>It is an LLM-based Scorer (<a href="https://arxiv.org/pdf/2303.16634.pdf">Paper</a>)</li>
<li>Documentation from <a href="https://www.deepeval.com/docs/metrics-llm-evals">DeepEval</a></li>
</ul>
<p><img alt="G-Eval" src="../images/g_eval.png" /></p>
<h3 id="process">Process</h3>
<ol>
<li>Prompt with the following information: 1) Task Introduction; 2) Evaluation Criteria</li>
<li>Generate through the previous output the list of Evaluation Steps through the <em>"Auto Chain of Thoughts"</em></li>
<li>Prompt the Scorer LLM with Evaluation Steps, Input Context and Input Target</li>
<li>(Optional) Normalise the output score by the probabilities of the output tokens</li>
</ol>
<h3 id="code-snippets">Code Snippets</h3>
<pre><code class="language-python">from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval

test_case = LLMTestCase(input=&quot;input to your LLM&quot;, actual_output=&quot;your LLM output&quot;)
coherence_metric = GEval(
    name=&quot;Coherence&quot;,
    criteria=&quot;Coherence - the collective quality of all sentences in the actual output&quot;,
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
)

coherence_metric.measure(test_case)
print(coherence_metric.score)
print(coherence_metric.reason)
</code></pre>
<h2 id="dag-model-based-scorer">DAG (Model-based Scorer)</h2>
<h3 id="introduction_2">Introduction</h3>
<ul>
<li>Deep Acyclic Graph is a LLM-based scorer that relies on a decision tree</li>
<li>Each node is an LLM Judgement and each edge is a decision</li>
<li>Each leaf node is associated with a hardcoded score</li>
</ul>
<p><img alt="DAG LLM Scorer" src="../images/dag_llm.png" /></p>
<h3 id="advantages">Advantages</h3>
<ul>
<li>Slightly more deterministic, since there's a certain degree of control in the score determination</li>
<li>It can be used to filter away edge cases where LLM output doesn't even meet minimum requirements</li>
</ul>
<h3 id="code-snippets_1">Code Snippets</h3>
<pre><code class="language-python">from deepeval.test_case import LLMTestCase
from deepeval.metrics.dag import (
    DeepAcyclicGraph,
    TaskNode,
    BinaryJudgementNode,
    NonBinaryJudgementNode,
    VerdictNode,
)
from deepeval.metrics import DAGMetric

correct_order_node = NonBinaryJudgementNode(
    criteria=&quot;Are the summary headings in the correct order: 'intro' =&gt; 'body' =&gt; 'conclusion'?&quot;,
    children=[
        VerdictNode(verdict=&quot;Yes&quot;, score=10),
        VerdictNode(verdict=&quot;Two are out of order&quot;, score=4),
        VerdictNode(verdict=&quot;All out of order&quot;, score=2),
    ],
)

correct_headings_node = BinaryJudgementNode(
    criteria=&quot;Does the summary headings contain all three: 'intro', 'body', and 'conclusion'?&quot;,
    children=[
        VerdictNode(verdict=False, score=0),
        VerdictNode(verdict=True, child=correct_order_node),
    ],
)

extract_headings_node = TaskNode(
    instructions=&quot;Extract all headings in `actual_output`&quot;,
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
    output_label=&quot;Summary headings&quot;,
    children=[correct_headings_node, correct_order_node],
)

# create the DAG
dag = DeepAcyclicGraph(root_nodes=[extract_headings_node])

# create the metric
format_correctness = DAGMetric(name=&quot;Format Correctness&quot;, dag=dag)

# create a test case
test_case = LLMTestCase(input=&quot;your-original-text&quot;, actual_output=&quot;your-summary&quot;)

# evaluate
format_correctness.measure(test_case)
print(format_correctness.score, format_correctness.reason)
</code></pre>
<h2 id="prometheus-model-based-scorer">Prometheus (Model-based Scorer)</h2>
<h3 id="introduction_3">Introduction</h3>
<ul>
<li>LLM-based evaluation framework use case agnostic</li>
<li>Based con Llama-2-chat and fine-tuned for evaluation purposes</li>
</ul>
<h3 id="advantages_1">Advantages</h3>
<ul>
<li>Evaluation steps are not produced by LLM, but are embedded in the node itself</li>
</ul>
<h2 id="qag-score-hybrid-scorer">QAG Score (Hybrid Scorer)</h2>
<h3 id="introduction_4">Introduction</h3>
<p>QAG (Question Answer Generation) Score uses binary answer (‘yes’ or ‘no’) to close-ended questions (which can be generated or preset) to compute a final metric score.</p>
<p><strong>Example:</strong></p>
<pre><code>So for this example LLM output:

Martin Luther King Jr., the renowned civil rights leader, was assassinated on April 4, 1968, at the Lorraine Motel in Memphis, Tennessee. He was in Memphis to support striking sanitation workers and was fatally shot by James Earl Ray, an escaped convict, while standing on the motel’s second-floor balcony.
A claim would be:

Martin Luther King Jr. assassinated on the April 4, 1968
And a corresponding close-ended question would be:

Was Martin Luther King Jr. assassinated on the April 4, 1968?
</code></pre>
<h3 id="advantages_2">Advantages</h3>
<ul>
<li>The score is not directly generated by an LLM</li>
</ul>
<h2 id="gptscore">GPTScore</h2>
<h3 id="introduction_5">Introduction</h3>
<p>It is similar to G-Eval, but the evaluation trask is performed with a form-filling paradigm.</p>
<p><img alt="GPTScore" src="../images/gptscore.png" /></p>
<h2 id="selfcheckgpt">SelfCheckGPT</h2>
<h3 id="introduction_6">Introduction</h3>
<p>It samples multiple output in order to detect hallucinations through a model-based approach.</p>
<p><img alt="SelfCheckGPT" src="../images/selfcheckgpt.png" /></p>
<h2 id="rag-specific-metrics">RAG-Specific Metrics</h2>
<h3 id="faithfulness">Faithfulness</h3>
<p>It evaluates whether the Generator is generating output that factually 
aligns with the information presented from the Retriever.</p>
<p>It is possible to use a QAG Score here.</p>
<p>For faithfulness, if you define it as the proportion of truthful claims
made in an LLM output with regards to the retrieval context, 
we can calculate faithfulness using QAG by following this algorithm:</p>
<ol>
<li>Use LLMs to extract all claims made in the output.</li>
<li>For each claim, check whether the it agrees or contradicts with each individual node in the retrieval context. In this case, the close-ended question in QAG will be something like: “Does the given claim agree with the reference text”, where the “reference text” will be each individual retrieved node. (Note that you need to confine the answer to either a ‘yes’, ‘no’, or ‘idk’. The ‘idk’ state represents the edge case where the retrieval context does not contain relevant information to give a yes/no answer.)</li>
<li>Add up the total number of truthful claims (‘yes’ and ‘idk’), and divide it by the total number of claims made.</li>
</ol>
<pre><code class="language-python">from deepeval.metrics import FaithfulnessMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input=&quot;...&quot;, 
  actual_output=&quot;...&quot;,
  retrieval_context=[&quot;...&quot;]
)
metric = FaithfulnessMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
</code></pre>
<h3 id="answer-relevancy">Answer Relevancy</h3>
<p>It assesses whether RAG generator outputs concise answers, 
and can be calculated by determining the proportion of sentences in an LLM output 
that a relevant to the input.</p>
<pre><code class="language-python">from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input=&quot;...&quot;, 
  actual_output=&quot;...&quot;,
  retrieval_context=[&quot;...&quot;]
)
metric = AnswerRelevancyMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
</code></pre>
<h3 id="contextual-precision">Contextual Precision</h3>
<p>Contextual Precision is a RAG metric that assesses the quality of your RAG pipeline’s retriever.</p>
<pre><code class="language-python">from deepeval.metrics import ContextualPrecisionMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input=&quot;...&quot;, 
  actual_output=&quot;...&quot;,
  # Expected output is the &quot;ideal&quot; output of your LLM, it is an
  # extra parameter that's needed for contextual metrics
  expected_output=&quot;...&quot;,
  retrieval_context=[&quot;...&quot;]
)
metric = ContextualPrecisionMetric(threshold=0.5) # Or ContextualRecallMetric

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
</code></pre>
<h2 id="agentic-metrics">Agentic Metrics</h2>
<h3 id="tool-correctness">Tool Correctness</h3>
<p>Tool correctness is an agentic metric that assesses the quality of your agentic systems, 
and is the most unusual metric here because it is based on exact matching and not any LLM-as-a-judge. It is computed by comparing the tools called for a given input to the expected tools that should be called.</p>
<pre><code class="language-python">from deepeval.test_case import LLMTestCase, ToolCall
from deepeval.metrics import ToolCorrectnessMetric

test_case = LLMTestCase(
    input=&quot;What if these shoes don't fit?&quot;,
    actual_output=&quot;We offer a 30-day full refund at no extra cost.&quot;,
    # Replace this with the tools that was actually used by your LLM agent
    tools_called=[ToolCall(name=&quot;WebSearch&quot;), ToolCall(name=&quot;ToolQuery&quot;)],
    expected_tools=[ToolCall(name=&quot;WebSearch&quot;)],
)
metric = ToolCorrectnessMetric()

metric.measure(test_case)
print(metric.score, metric.reason)
</code></pre>
<h3 id="task-completion">Task Completion</h3>
<p>Task completion is an agentic metric that uses LLM-as-a-judge to evaluate whether your
LLM agent is able to accomplish its given task.</p>
<pre><code class="language-python">from deepeval.test_case import LLMTestCase
from deepeval.metrics import TaskCompletionMetric

metric = TaskCompletionMetric(
    threshold=0.7,
    model=&quot;gpt-4o&quot;,
    include_reason=True
)
test_case = LLMTestCase(
    input=&quot;Plan a 3-day itinerary for Paris with cultural landmarks and local cuisine.&quot;,
    actual_output=(
        &quot;Day 1: Eiffel Tower, dinner at Le Jules Verne. &quot;
        &quot;Day 2: Louvre Museum, lunch at Angelina Paris. &quot;
        &quot;Day 3: Montmartre, evening at a wine bar.&quot;
    ),
    tools_called=[
        ToolCall(
            name=&quot;Itinerary Generator&quot;,
            description=&quot;Creates travel plans based on destination and duration.&quot;,
            input_parameters={&quot;destination&quot;: &quot;Paris&quot;, &quot;days&quot;: 3},
            output=[
                &quot;Day 1: Eiffel Tower, Le Jules Verne.&quot;,
                &quot;Day 2: Louvre Museum, Angelina Paris.&quot;,
                &quot;Day 3: Montmartre, wine bar.&quot;,
            ],
        ),
        ToolCall(
            name=&quot;Restaurant Finder&quot;,
            description=&quot;Finds top restaurants in a city.&quot;,
            input_parameters={&quot;city&quot;: &quot;Paris&quot;},
            output=[&quot;Le Jules Verne&quot;, &quot;Angelina Paris&quot;, &quot;local wine bars&quot;],
        ),
    ],
)

metric.measure(test_case)
print(metric.score, metric.reason)
</code></pre>
<h2 id="llm-metrics">LLM Metrics</h2>
<h3 id="hallucination">Hallucination</h3>
<pre><code class="language-python">from deepeval.metrics import HallucinationMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input=&quot;...&quot;, 
  actual_output=&quot;...&quot;,
  # Note that 'context' is not the same as 'retrieval_context'.
  # While retrieval context is more concerned with RAG pipelines,
  # context is the ideal retrieval results for a given input,
  # and typically resides in the dataset used to fine-tune your LLM
  context=[&quot;...&quot;],
)
metric = HallucinationMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.is_successful())
</code></pre>
<h3 id="toxicity">Toxicity</h3>
<pre><code class="language-python">from deepeval.metrics import ToxicityMetric
from deepeval.test_case import LLMTestCase

metric = ToxicityMetric(threshold=0.5)
test_case = LLMTestCase(
    input=&quot;What if these shoes don't fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output = &quot;We offer a 30-day full refund at no extra cost.&quot;
)

metric.measure(test_case)
print(metric.score)
</code></pre>
<h3 id="bias">Bias</h3>
<p>The bias metric evaluates aspects such as political, gender, and social biases in textual content.</p>
<pre><code class="language-python">from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(
    input=&quot;What if these shoes don't fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output = &quot;We offer a 30-day full refund at no extra cost.&quot;
)
toxicity_metric = GEval(
    name=&quot;Bias&quot;,
    criteria=&quot;Bias - determine if the actual output contains any racial, gender, or political bias.&quot;,
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
)

metric.measure(test_case)
print(metric.score)
</code></pre>
<h2 id="evaluation-framework-platforms">Evaluation Framework Platforms</h2>
<h3 id="azure-ai-foundry-automated-evaluation-microsoft">Azure AI Foundry Automated Evaluation (Microsoft)</h3>
<p>Azure AI Foundry is an all-in-one AI platform for building, evaluating, 
and deploying generative AI solutions and custom copilots.</p>
<h3 id="openai-evals-openai">OpenAI Evals (OpenAI)</h3>
<p>The OpenAI Evals framework consists of a framework to evaluate an LLM or 
a system built on top of an LLM, 
and an open-source registry of challenging evals.</p>
<p>It is based on pre-defined dataset.</p>
<p>The process to create a new evaluation dataset is described in <a href="https://github.com/openai/evals/blob/main/docs/custom-eval.md">custom-eval.md</a>
from the OpenAI Evals package. Although it seems pretty rigid and too schematic for the actual standards.</p>
<h3 id="weights-biases">Weights &amp; Biases</h3>
<p>A Machine Learning platform to quickly track experiments, version and iterate on datasets, 
evaluate model performance.</p>
<h3 id="langsmith-langchain">LangSmith (LangChain)</h3>
<p>Trace and evaluate language model applications and intelligent agents.</p>
<h3 id="trulens-truera">TruLens (TruEra)</h3>
<p>TruLens provides a set of tools for developing and monitoring neural nets, including LLMs.</p>
<h3 id="vertex-ai-studio-google">Vertex AI Studio (Google)</h3>
<p>You can evaluate the performance of foundation models and your tuned generative AI models on Vertex AI.</p>
<h3 id="amazon-bedrock">Amazon Bedrock</h3>
<p>Amazon Bedrock supports model evaluation jobs.</p>
<h3 id="deepeval-confident-ai">DeepEval (Confident AI)</h3>
<p>An open-source LLM evaluation framework for LLM applications.</p>
<p><a href="https://github.com/confident-ai/deepeval/tree/main/examples">Sample Codes</a>.</p>
<h3 id="parea-ai">Parea AI</h3>
<p>Parea provides tools for debugging, testing, evaluating, and monitoring LLM-powered applications.</p>
<h3 id="optik-by-comet">Optik by Comet</h3>
<p>Opik is an open-source platform by Comet for evaluating, testing, and monitoring Large Language Models (LLMs). 
It provides flexible tools to track, annotate, and refine LLM applications across development and production environments.</p>
<pre><code class="language-python">from opik.evaluation.metrics import Hallucination

metric = Hallucination()
score = metric.score(
    input=&quot;What is the capital of France?&quot;,
    output=&quot;Paris&quot;,
    context=[&quot;France is a country in Europe.&quot;]
)
print(score)
</code></pre>
<h2 id="code-based-llm-evaluations">Code-Based LLM Evaluations</h2>
<h3 id="introduction_7">Introduction</h3>
<ul>
<li>It involves creating automated CI/CD test cases to evaluate how the LLM performs on specific tasks or datasets</li>
<li>The advantage is that it's cost-efficient as it does not introduce token usage or latency</li>
</ul>
<h3 id="use-cases">Use Cases</h3>
<ul>
<li>Test Correct Structure of Output</li>
<li>Test Specific Data in Output by verifying that the LLM output contains specific data points</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../fine_tuning/" class="btn btn-neutral float-left" title="Fine-Tuning"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../deepeval/" class="btn btn-neutral float-right" title="DeepEval">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/Volscente/NexusLLM" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../fine_tuning/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../deepeval/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
