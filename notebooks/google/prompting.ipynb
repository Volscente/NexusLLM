{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Google GenAI - Prompting\n",
    "\n",
    "Explore different prompting techniques through Google AI Studio.\n",
    "\n",
    "Resources:\n",
    "- [Kaggle | Day 1 - Prompting](https://www.kaggle.com/code/markishere/day-1-prompting)\n",
    "\n",
    "NOTE: Setup a Google AI Studio API Key"
   ],
   "id": "be26c99df921261e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup Notebook",
   "id": "68d745c602b0eb58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "32106f5ff5e0246d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:26.866141Z",
     "start_time": "2024-11-12T13:42:26.862165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Standard Libraries\n",
    "import enum\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from pydantic import BaseModel\n",
    "from typing import List"
   ],
   "id": "8fba22dfb6626f2f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Environment Variables",
   "id": "d20cbfdadb79e33d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:18:54.012099Z",
     "start_time": "2024-11-12T13:18:54.000551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load environment variables\n",
    "load_dotenv('./../../.env')"
   ],
   "id": "74d0200bf10e8442",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:18:54.023150Z",
     "start_time": "2024-11-12T13:18:54.015522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read environment varialbes\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ],
   "id": "ed7c93686004202",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "51869e1faa9b7b61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configure Authentication",
   "id": "c8be47ec42c1424b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:18:54.067381Z",
     "start_time": "2024-11-12T13:18:54.062830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the Google AI Studio API Key for genai SDK\n",
    "genai.configure(api_key=google_api_key)"
   ],
   "id": "4570814030d5f598",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "3f039a339965ee11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## List Available Models",
   "id": "c55dc405aea59657"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:54:07.300524Z",
     "start_time": "2024-11-12T10:54:06.527938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List all available models\n",
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ],
   "id": "635366f66d62c84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get Model Information",
   "id": "a28b6049af8746f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:55:37.663558Z",
     "start_time": "2024-11-12T10:55:37.496136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get information about a specific model\n",
    "for model in genai.list_models():\n",
    "  print(model)\n",
    "  break"
   ],
   "id": "b602528309c4c2be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# General Prompting",
   "id": "58bff18308ed98fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Usage Example",
   "id": "13eb5d793d4da7f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:46:01.416571Z",
     "start_time": "2024-11-12T10:45:58.534549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instance model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Inference\n",
    "response = model.generate_content(\"What's an LLM?\")\n",
    "\n",
    "# Print response\n",
    "Markdown(response.text)"
   ],
   "id": "62fecc8e564634e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "LLM stands for **Large Language Model**. It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \n\nHere's a breakdown:\n\n* **Large:** LLMs are trained on massive amounts of text data, often scraped from the internet. This allows them to learn complex patterns and nuances of language.\n* **Language:** LLMs focus on language, specifically text. They can process, understand, and generate text in various forms, including articles, stories, poems, code, and more.\n* **Model:** LLMs are statistical models, meaning they learn from data and make predictions based on what they've learned. They don't \"think\" like humans, but they can mimic human language in surprising ways.\n\n**Think of LLMs as AI that can do the following:**\n\n* **Understand your requests:** You can ask them questions, give them instructions, or even have a conversation with them.\n* **Generate text:** They can write summaries, create stories, translate languages, and even compose different creative forms of text.\n* **Learn and adapt:** With more data and training, LLMs continue to improve their abilities.\n\n**Examples of LLMs:**\n\n* **GPT-3:** Developed by OpenAI, it's known for its ability to write creative and informative text.\n* **LaMDA:** Developed by Google, it's focused on generating dialogue and conversation.\n* **BERT:** Developed by Google, it excels at understanding the meaning of words in context.\n\n**Applications of LLMs:**\n\n* **Chatbots and virtual assistants:** Providing personalized interactions and answering questions.\n* **Content creation:** Generating articles, stories, and even poems.\n* **Translation:** Translating text between languages.\n* **Code generation:** Helping developers write code more efficiently.\n* **Personalized learning:** Tailoring educational materials to individual needs.\n\nLLMs are a rapidly evolving field with incredible potential to transform how we interact with technology and information. They are still under development, but they are already having a significant impact on various industries and aspects of our lives.\n"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parameters Settings\n",
    "\n",
    "List of available parameters:\n",
    "- `max_output_tokens` - Maximum numbers of output tokens\n",
    "- `temperature` - Degree of randomness in the token selection (0 is greedy decoding &rarr; Most probable token only)\n",
    "- `top_k` - Select only among the top K most probable tokens (1 is greedy decoding &rarr; Most probable token only)\n",
    "- `top_p` - Select only tokens whose probability is up to P (0 is greedy decoding &rarr; Most probable token only)\n",
    "\n",
    "**NOTE:** If both `top_k` and `top_p`, the `top_k` is used first"
   ],
   "id": "f4e1f6226d0c2f67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:19:05.913137Z",
     "start_time": "2024-11-12T13:19:03.933747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define configurations\n",
    "llm_config = genai.GenerationConfig(\n",
    "    max_output_tokens=200,\n",
    "    temperature=0.2,\n",
    "    top_k=64,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Instance model\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash', \n",
    "                              generation_config=llm_config)\n",
    "\n",
    "# Inference\n",
    "response = model.generate_content(\"Write a 1000 word essay on the importance of olives in modern society.\")\n",
    "\n",
    "Markdown(response.text)"
   ],
   "id": "3025b174b0ad3ff1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## The Enduring Appeal of the Olive: A Culinary and Cultural Icon in Modern Society\n\nThe olive, a small, unassuming fruit, has played a pivotal role in human history for millennia. From its humble beginnings as a staple food in ancient civilizations to its modern-day status as a culinary icon, the olive has transcended time and geography, leaving an indelible mark on cultures and societies worldwide. This essay will explore the multifaceted importance of olives in modern society, examining their culinary versatility, nutritional value, economic significance, and cultural impact.\n\n**A Culinary Cornerstone:**\n\nThe olive's journey from ancient food source to modern culinary staple is a testament to its versatility and enduring appeal. Its unique flavor profile, ranging from the briny bite of green olives to the rich, buttery notes of black olives, has captivated palates for centuries. Olives are a cornerstone of Mediterranean cuisine, where they are enjoyed as a snack, appetizer, or ingredient in countless dishes. From the classic Greek salad to"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tools\n",
    "\n",
    "They allow a ReAct (Reason & Act) Prompting Technique: the LLM is able to perform operations in order to answer the prompt, exactly as a human would do (e.g., checking Wikipedia or running a code)."
   ],
   "id": "ff3d37d72791b06b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:50:25.053028Z",
     "start_time": "2024-11-12T13:50:20.457965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instance model with tools\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution')\n",
    "\n",
    "# Define prompt\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\n",
    "\"\"\"\n",
    "\n",
    "# Inference\n",
    "response = model.generate_content(code_exec_prompt)\n",
    "Markdown(response.text)"
   ],
   "id": "87ed8c566e108d81",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "I will calculate the sum of the first 14 odd prime numbers.\n\n\n``` python\nimport sympy\n\nprimes = list(sympy.primerange(1, 100))\nodd_primes = [prime for prime in primes if prime % 2 != 0]\nsum_primes = sum(odd_primes[:14])\nprint(f'{sum_primes=}')\n\n```\n```\nsum_primes=326\n\n```\nThe sum of the first 14 odd prime numbers is 326. \n"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prompting Techniques",
   "id": "e8167944ee605ab7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero-Shot Prompting with Enumeration Output\n",
    "\n",
    "Ask the LLM to solve a problem without previous reference or example"
   ],
   "id": "81edc605ae26885a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:35:46.625910Z",
     "start_time": "2024-11-12T13:35:45.348231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the prompt\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "# Define class of possible output values\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "# Define configurations\n",
    "llm_config = genai.GenerationConfig(\n",
    "    max_output_tokens=5,\n",
    "    temperature=0.1,\n",
    "    top_k=1,\n",
    "    top_p=0,\n",
    "    response_mime_type=\"text/x.enum\",\n",
    "    response_schema=Sentiment\n",
    ")\n",
    "\n",
    "# Instance model\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash', \n",
    "                              generation_config=llm_config)\n",
    "\n",
    "# Inference\n",
    "response = model.generate_content(zero_shot_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ],
   "id": "daa8a01c9c2033c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "positive"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few-Shot Prompting with Pydantic\n",
    "\n"
   ],
   "id": "3e40e4c54a7ec1cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:46:04.184790Z",
     "start_time": "2024-11-12T13:46:03.744042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the system prompt\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "# Define the user-role prompt\n",
    "customer_order = \"Can I have a large dessert pizza with apple and chocolate\"\n",
    "\n",
    "# Define Pydantic model for order\n",
    "class PizzaOrder(BaseModel):\n",
    "    size: str\n",
    "    type: str\n",
    "    ingredients: List[str]\n",
    "\n",
    "# Define configurations\n",
    "llm_config = genai.GenerationConfig(\n",
    "    max_output_tokens=20,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=PizzaOrder\n",
    ")\n",
    "\n",
    "# Instance model\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest', \n",
    "                              generation_config=llm_config)\n",
    "\n",
    "# Inference\n",
    "response = model.generate_content(customer_order)\n",
    "\n",
    "print(response.text)"
   ],
   "id": "70869e2d0506d137",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conversation",
   "id": "3c2b1a528c3b7756"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Start Chat",
   "id": "8f899fca2a1e4e2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:50:54.058071Z",
     "start_time": "2024-11-12T10:50:53.008872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instance model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Start a chat\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "# Inference\n",
    "response = chat.send_message(\"Hello! My name is Kuzko\")\n",
    "\n",
    "Markdown(response.text)"
   ],
   "id": "7d5ade24d7df5b3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Hello Kuzko! It's nice to meet you. What can I do for you today? ðŸ˜Š \n"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:51:40.168084Z",
     "start_time": "2024-11-12T10:51:39.589369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the history\n",
    "response = chat.send_message(\"Do you remember what my name is?\")\n",
    "\n",
    "Markdown(response.text)"
   ],
   "id": "9003ab1d28f33cc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Of course I remember! You said your name is Kuzko. ðŸ˜„  It's nice to chat with you, Kuzko. What's on your mind? \n"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexusllm",
   "language": "python",
   "name": "nexusllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
