{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c334d93-ff04-4ee2-956a-8bd184e877ee",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is intended to develop a character-based LLM trained on [Shakespeare Literature](https://github.com/karpathy/ng-video-lecture/blob/master/input.txt).\n",
    "\n",
    "**Resources**\n",
    "- [Reference tutorial from Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0ade33-2fe6-4a2f-9f05-3e973305b12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f9811-274c-4084-8783-48590eca1fce",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2ba6b6-2572-4769-baee-76e609d52c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define local data file path\n",
    "data_file_path = Path(os.path.abspath('')).parents[1] / 'data' / 'character_based_llm_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad88529-29f5-4360-8044-f9f7fef40489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(data_file_path, 'r', encoding='utf-8') as data_file:\n",
    "    data = data_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0dffaaa-f1a4-4f99-ad1d-782c435ad4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the vocabulary of characters in the data\n",
    "vocaulary = sorted(list(set(data)))\n",
    "vocaulary_size = len(vocaulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67909db2-9f64-4a76-b5e3-109da6e37d5d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60957e1e-328a-4beb-b32e-1fa6c689ac72",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "It is an important data preprocessing operation which converts the single portion of the sequence \n",
    "(characters or tokens of words) into numerical value based on all the possible values of the train vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0b9c8-2c27-4408-a2a2-0ed083844aca",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9790d9a8-e5f2-400e-8ac5-81d23317475e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String to integer encoder\n",
    "string_integer_encoder = {character: integer for integer, character in enumerate(vocaulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9490cb69-06c2-4189-9c29-d390dccd5009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String to integer decoder\n",
    "string_integer_decoder = {integer: character for integer, character in enumerate(vocaulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f056b10-ab3b-4a62-a1e8-70c8aca1fea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "encoder = lambda string: [string_integer_encoder[character] for character in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc290e3-085b-421b-a13f-0618a8f960da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the decoder\n",
    "decoder = lambda integers_list: ''.join([string_integer_decoder[integer] for integer in integers_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33977d7c-3e37-4048-9817-64fdd8699797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a sample sentence\n",
    "tokeniser_sample_sentence = 'Hello there'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987a6fc1-60bc-43e7-9486-7cd3cd798b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Encoding and Decoding\n",
      "Example sentence: Hello there\n",
      "Encode: [20, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "Decode: Hello there\n"
     ]
    }
   ],
   "source": [
    "print('Example of Encoding and Decoding')\n",
    "print('Example sentence: {}'.format(tokeniser_sample_sentence))\n",
    "print('Encode: {}'.format(encoder(tokeniser_sample_sentence)))\n",
    "print('Decode: {}'.format(decoder(encoder(tokeniser_sample_sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942d7eb-0538-4984-9292-c0d3c5b5b283",
   "metadata": {},
   "source": [
    "### TikToken\n",
    "\n",
    "There are also already available Tokenizer as [TikToken](https://github.com/openai/tiktoken) from OpenAI. The goal is the same: produce a numerical representation from a string sequence, but they are based over a different vocabulary and transform the sequence in a different manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92563305-172b-4a0f-b277-b155abf80f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the Tokenizer\n",
    "tiktoken_tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c2c8f8-163c-4eac-9de5-e54c7e24954c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Vocabularies of TikToken: 50257\n",
      "List of Vocabularies from Custom Tokenizer: 65\n"
     ]
    }
   ],
   "source": [
    "print('List of Vocabularies of TikToken: {}'.format(tiktoken_tokenizer.n_vocab))\n",
    "print('List of Vocabularies from Custom Tokenizer: {}'.format(vocaulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b1a572-cdf1-4d98-89fd-7f632a572162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Encoding and Decoding\n",
      "Example sentence: Hello there\n",
      "Encode: [15496, 612]\n",
      "Decode: Hello there\n"
     ]
    }
   ],
   "source": [
    "print('Example of Encoding and Decoding')\n",
    "print('Example sentence: {}'.format(tokeniser_sample_sentence))\n",
    "print('Encode: {}'.format(tiktoken_tokenizer.encode(tokeniser_sample_sentence)))\n",
    "print('Decode: {}'.format(tiktoken_tokenizer.decode(tiktoken_tokenizer.encode(tokeniser_sample_sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29179834-3679-4cad-a5ef-af20dded824a",
   "metadata": {},
   "source": [
    "### Tokenize the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd8c4db0-fcb9-42d7-a433-1756dc8aa587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the data and store it in a PyTorch Tensor\n",
    "data_encoded_tensor = torch.tensor(encoder(data), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a1ef6c5-785a-43d9-bc63-496207a48fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115389]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data_encoded_tensor.shape, data_encoded_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "114efac7-6539-4368-9059-94fc7c06521d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# The entire data are represented as a sequence of integeres now\n",
    "print(data_encoded_tensor[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac56642-e9db-4af3-a9f2-7c00ed471c43",
   "metadata": {},
   "source": [
    "## Train & Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae7c63e-8d99-4eb1-ae06-42606f8fa7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the train percentage\n",
    "train_percentage = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a83385aa-2451-442f-a7c8-c0a69797bd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split in train and validation data\n",
    "train_data = data_encoded_tensor[:int(train_percentage * len(data_encoded_tensor))]\n",
    "validation_data = data_encoded_tensor[int(train_percentage * len(data_encoded_tensor)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decac60-ed4f-411f-859b-d0fbc63a5535",
   "metadata": {},
   "source": [
    "## Blocks\n",
    "\n",
    "The training would be splitted into Blocks, which are sequence of characters randomly selected within the training data. The length of a single block is the Block Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0468d5cc-8d9a-441d-9364-0af01296e425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Block\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First Cit\n"
     ]
    }
   ],
   "source": [
    "# Define the block_size\n",
    "block_size = 8\n",
    "\n",
    "print('First Block')\n",
    "print(train_data[:block_size + 1])\n",
    "print(data[:block_size + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bd13f-c094-45e2-a6a5-dac50d735492",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "Within each block, the transformer will learn multiple sequences at a time. \n",
    "- With [18], usually [47] comes next\n",
    "- With [18, 47], usually [56] comes next\n",
    "- With [18, 47, 56], usually [57] comes next\n",
    "- ...\n",
    "\n",
    "So everytime, in each block, there would be different sequences with different labels. The transformer will learn all of these sequences within a single block everytime. In this way, the Transformer will learn several contexts of different sizes and get used to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8be8109c-7abc-4c7c-9055-3345a4ac43d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When features are tensor([18]) the label is 47\n",
      "When features are tensor([18, 47]) the label is 56\n",
      "When features are tensor([18, 47, 56]) the label is 57\n",
      "When features are tensor([18, 47, 56, 57]) the label is 58\n",
      "When features are tensor([18, 47, 56, 57, 58]) the label is 1\n",
      "When features are tensor([18, 47, 56, 57, 58,  1]) the label is 15\n",
      "When features are tensor([18, 47, 56, 57, 58,  1, 15]) the label is 47\n",
      "When features are tensor([18, 47, 56, 57, 58,  1, 15, 47]) the label is 58\n"
     ]
    }
   ],
   "source": [
    "# Example of features and labels within each batch\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for index in range(block_size):\n",
    "    \n",
    "    features = x[:index + 1]\n",
    "    label = y[index]\n",
    "    \n",
    "    print(f\"When features are {features} the label is {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02241d-1f5c-4680-a7e4-fe622a2cd162",
   "metadata": {},
   "source": [
    "## Batches\n",
    "It is a set of blocks that are passed to the Transformer at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cafdec0-5148-47ce-9a51-133251b78aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109be49d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Set Torch Seed\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "619c9a56-9d9e-4774-a3e3-904f681e8bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    \"\"\"\n",
    "    Return the x and y for the passed dataset\n",
    "    \n",
    "    Args:\n",
    "        data: torch.Tensor input data\n",
    "    \n",
    "    Returns:\n",
    "        x: torch.Tensor features values x batch_size\n",
    "        y: torch.Tensor label values x batch_size\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the block index for each of the batch\n",
    "    block_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Retrieve x and y from the data\n",
    "    x = torch.stack([data[block_index:block_index + block_size] for block_index in block_indices])\n",
    "    y = torch.stack([data[block_index + 1:block_index + block_size + 1] for block_index in block_indices])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25aeecbb-ee3f-41b9-9612-60df48fa212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve sample batches\n",
    "x_batch_sample, y_batch_sample = get_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca2b1be5-2e67-4ab3-9934-536228399dae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Batch Sample Shape: torch.Size([4, 8])\n",
      "X Batch Sample Shape: torch.Size([4, 8])\n",
      "\n",
      "\n",
      "X First Batch Sample: tensor([59, 57,  1, 58, 56, 39, 47, 58])\n",
      "Y First Batch Sample: tensor([57,  1, 58, 56, 39, 47, 58, 53])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X Batch Sample Shape: {x_batch_sample.shape}\")\n",
    "print(f\"X Batch Sample Shape: {y_batch_sample.shape}\")\n",
    "print('\\n')\n",
    "print(f\"X First Batch Sample: {x_batch_sample[0]}\")\n",
    "print(f\"Y First Batch Sample: {y_batch_sample[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bcad7-77cf-401a-900f-2027404806b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Each batch has 8 indpendent data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d20b5-2dc0-428f-8a69-8f90eb06fbc6",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd6cb0-6bc2-44ab-a8c9-3d9de2393de3",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "This is the simplest kind of Neural Network model you can imagine. We're going to implement it from PyTorch based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d5633a7-5a66-420b-bbce-ea23f448e93e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109be49d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Torch Seed\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ac08f7c-a0f0-4ef4-99a8-8bd5367757e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabulary_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a Token Embedding Table, which is a matrix vocabulary_size x vocabulary_size\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "    \n",
    "        # When passing an index to the token_embedding_table, it will return that specific row next characters logits (probabilities)\n",
    "        # In a (Batch, Times, Channels) fashion -> torch.Size([4, 8, 65])\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        # Compute the loss in case there are the target labels\n",
    "        if targets is None:\n",
    "            \n",
    "            loss = None\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            # Beore calculating the loss of the logits, we need to reshape them, because the cross_entropy function expects a (Batch, Channels, Times) input\n",
    "            # Get the logits shape\n",
    "            batch_dim, times_dim, channels_dim = logits.shape\n",
    "\n",
    "            # Reshape the logits\n",
    "            logits = logits.view(batch_dim * times_dim, channels_dim)\n",
    "\n",
    "            # Reshape the targets as well\n",
    "            targets = targets.view(batch_dim * times_dim)\n",
    "\n",
    "            # Measure the quality of the logits predictions\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates new tokens for the given maximum dimension, thus predicting the very next character\n",
    "        \"\"\"\n",
    "        \n",
    "        # index is a (B, T) array\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # Get predictions (B, T, C)\n",
    "            logits, loss = self(index)\n",
    "            \n",
    "            # Select only the predictions in the last element (B, C)\n",
    "            # NOTE: This is not correct, because you should feed the entire sequence up to the last element, and not just the last one.\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            # Get probabilities through the Softmax function (B, C)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution (B, 1) and obtain the next index character for all the batches\n",
    "            index_next_character = torch.multinomial(probabilities, num_samples=1)\n",
    "            \n",
    "            # Append the sampled index of the next character to the sequence (B, T+1)\n",
    "            index = torch.cat((index, index_next_character), dim=1)\n",
    "            \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c8ef925-9f9e-4c55-bdc0-d8674cefd425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instance the Bigram language Model\n",
    "bigram_language_model = BigramLanguageModel(vocabulary_size=vocaulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62c69c55-a688-4528-b8a4-42d502cd160f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the logits and the loss for a single batch data sample\n",
    "logits, loss = bigram_language_model(x_batch_sample, targets=y_batch_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "036a763f-9e25-451e-ade3-b2e90541aef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc35814e-dfbe-40b4-a2c8-1015d669a621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5242, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222da13-bd46-4e9f-8002-1144298fb91d",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16b18108-fab5-4f60-b159-9d39ead2f789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the a 1 x 1 Tensor holding a zero value (It corresponds to 'new line' character)\n",
    "# It would be our first character that will kick off the generation\n",
    "initial_seed = torch.zeros((1, 1), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e0bfd04-9b78-47c8-bdf9-0843a540dbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 more characters ('max_new_tokens=100')\n",
    "# Retrieve the first and alone batch ('[0]')\n",
    "print(decoder(bigram_language_model.generate(initial_seed, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44239a57-6016-4096-9316-4e24188d7143",
   "metadata": {},
   "source": [
    "As expecting, the output is crap. That's also because, in order to generate the 'f' character at the 7th position, only the previous character, ':', has been fed. Insted, all the sequence 'Qd&!e:' should be ingested to predict the next element and not only the last one.  This refers to the 'NOTE' warning in the generate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426f711-36e2-4045-8f92-65b5a385eea0",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21788b74-25c1-420b-8937-284bfce42edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Optimizer\n",
    "optimizer = torch.optim.AdamW(bigram_language_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea92359-d4d4-4ba2-995c-14ceb59fbc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 - Loss: 4.7736\n",
      "Step: 1000 - Loss: 3.7155\n",
      "Step: 2000 - Loss: 3.1113\n",
      "Step: 3000 - Loss: 2.8313\n",
      "Step: 4000 - Loss: 2.487\n",
      "Step: 5000 - Loss: 2.5177\n",
      "Step: 6000 - Loss: 2.5832\n",
      "Step: 7000 - Loss: 2.5644\n",
      "Step: 8000 - Loss: 2.4895\n",
      "Step: 9000 - Loss: 2.5055\n",
      "Step: 9999 - Loss: 2.3863\n"
     ]
    }
   ],
   "source": [
    "# Increase the batch size from 4 to 32\n",
    "batch_size = 32\n",
    "\n",
    "# Loop over 100 iterations\n",
    "for step in range(10000):\n",
    "    \n",
    "    # Sample data\n",
    "    x_batch, y_batch = get_batch(train_data)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = bigram_language_model(x_batch, y_batch)\n",
    "    \n",
    "    # Reset the gradient\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Backpropagate the error and getting the gradients for all the weights\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss every 100 steps\n",
    "    if step % 1000 == 0:\n",
    "        print(f'Step: {step} - Loss: {round(loss.item(), 4)}')\n",
    "        \n",
    "print(f'Step: {step} - Loss: {round(loss.item(), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a2ece-ab05-4278-a61c-0651f478aebf",
   "metadata": {},
   "source": [
    "As we can see, the loss is going down slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1287321-d8a3-49d2-b774-c57542a02b88",
   "metadata": {},
   "source": [
    "### Example after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d76d247b-98c7-4c53-b2b0-496fb068e512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "RDe hicomyonthar's\n",
      "PlinseKEd ith henouratucenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind ttid?\n",
      "ig t ouchos tes; st yo hind wotin grotonear 'so it t jod weancotha:\n",
      "h haybet--s n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scar t tridesar, wnl'shenou\n"
     ]
    }
   ],
   "source": [
    "# Generate again 400 tokens and let's see the improvement\n",
    "print(decoder(bigram_language_model.generate(initial_seed, max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b7014-aa92-4b10-98c8-819a8393f11b",
   "metadata": {},
   "source": [
    "Much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc49f3a-b601-441b-9daa-0ea5b71a0cff",
   "metadata": {},
   "source": [
    "## Estimate Loss\n",
    "\n",
    "While training the Transformer in the Training the Model Section, we print out the loss. However, that is really not precise, because it depends on the batch on which it is calculated. With the Estimate Loss, we want to estimate the loss over multiple batches through the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3064c80f-25dd-4502-98ef-b5318fe0e3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.BigramLanguageModel"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigram_language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30401a-3be6-4aa8-b352-0040a19f8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Compute the average loss over a set of batches\n",
    "    \"\"\"\n",
    "    \n",
    "    out = {}\n",
    "    \n",
    "    # Model does not have any dropout layers, so it does not have a 'eval' or 'train' mode\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        \n",
    "    # Model does not have any dropout layers, so it does not have a 'eval' or 'train' mode\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
