{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c334d93-ff04-4ee2-956a-8bd184e877ee",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is intended to develop a character-based LLM trained on [Shakespeare Literature](https://github.com/karpathy/ng-video-lecture/blob/master/input.txt).\n",
    "\n",
    "**Resources**\n",
    "- [Reference tutorial from Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0ade33-2fe6-4a2f-9f05-3e973305b12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f9811-274c-4084-8783-48590eca1fce",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2ba6b6-2572-4769-baee-76e609d52c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define local data file path\n",
    "train_data_file_path = Path(os.path.abspath('')).parents[1] / 'data' / 'character_based_llm_train_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad88529-29f5-4360-8044-f9f7fef40489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(train_data_file_path, 'r', encoding='utf-8') as train_data_file:\n",
    "    train_data = train_data_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0dffaaa-f1a4-4f99-ad1d-782c435ad4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the vocabulary of characters in the train data\n",
    "train_vocaulary = sorted(list(set(train_data)))\n",
    "train_vocaulary_size = len(train_vocaulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60957e1e-328a-4beb-b32e-1fa6c689ac72",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "It is an important data preprocessing operation which converts the single portion of the sequence \n",
    "(characters or tokens of words) into numerical value based on all the possible values of the train vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0b9c8-2c27-4408-a2a2-0ed083844aca",
   "metadata": {},
   "source": [
    "## Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9790d9a8-e5f2-400e-8ac5-81d23317475e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String to integer encoder\n",
    "string_integer_encoder = {character: integer for integer, character in enumerate(train_vocaulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9490cb69-06c2-4189-9c29-d390dccd5009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String to integer decoder\n",
    "string_integer_decoder = {integer: character for integer, character in enumerate(train_vocaulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f056b10-ab3b-4a62-a1e8-70c8aca1fea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Â Define the encoder\n",
    "encoder = lambda string: [string_integer_encoder[character] for character in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc290e3-085b-421b-a13f-0618a8f960da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the decoder\n",
    "decoder = lambda integers_list: ''.join([string_integer_decoder[integer] for integer in integers_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33977d7c-3e37-4048-9817-64fdd8699797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a sample sentence\n",
    "tokeniser_sample_sentence = 'Hello there'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987a6fc1-60bc-43e7-9486-7cd3cd798b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Encoding and Decoding\n",
      "Example sentence: Hello there\n",
      "Encode: [20, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "Decode: Hello there\n"
     ]
    }
   ],
   "source": [
    "print('Example of Encoding and Decoding')\n",
    "print('Example sentence: {}'.format(tokeniser_sample_sentence))\n",
    "print('Encode: {}'.format(encoder(tokeniser_sample_sentence)))\n",
    "print('Decode: {}'.format(decoder(encoder(tokeniser_sample_sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942d7eb-0538-4984-9292-c0d3c5b5b283",
   "metadata": {},
   "source": [
    "## TikToken\n",
    "\n",
    "There are also already available Tokenizer as [TikToken](https://github.com/openai/tiktoken) from OpenAI. The goal is the same: produce a numerical representation from a string sequence, but they are based over a different vocabulary and transform the sequence in a different manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92563305-172b-4a0f-b277-b155abf80f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the Tokenizer\n",
    "tiktoken_tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c2c8f8-163c-4eac-9de5-e54c7e24954c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Vocabularies of TikToken: 50257\n",
      "List of Vocabularies from Custom Tokenizer: 65\n"
     ]
    }
   ],
   "source": [
    "print('List of Vocabularies of TikToken: {}'.format(tiktoken_tokenizer.n_vocab))\n",
    "print('List of Vocabularies from Custom Tokenizer: {}'.format(train_vocaulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b1a572-cdf1-4d98-89fd-7f632a572162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Encoding and Decoding\n",
      "Example sentence: Hello there\n",
      "Encode: [15496, 612]\n",
      "Decode: Hello there\n"
     ]
    }
   ],
   "source": [
    "print('Example of Encoding and Decoding')\n",
    "print('Example sentence: {}'.format(tokeniser_sample_sentence))\n",
    "print('Encode: {}'.format(tiktoken_tokenizer.encode(tokeniser_sample_sentence)))\n",
    "print('Decode: {}'.format(tiktoken_tokenizer.decode(tiktoken_tokenizer.encode(tokeniser_sample_sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29179834-3679-4cad-a5ef-af20dded824a",
   "metadata": {},
   "source": [
    "## Tokenize the Train Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c4db0-fcb9-42d7-a433-1756dc8aa587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the train_vocaulary and store it in a PyTorch Tensor\n",
    "train_vocaulary_encoded_tensor = torch.tensor(string_integer_encoder(train_vocaulary), dtype=torch.lomg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ef6c5-785a-43d9-bc63-496207a48fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
