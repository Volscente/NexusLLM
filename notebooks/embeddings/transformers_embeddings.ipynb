{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers - Embeddings\n",
    "\n",
    "This notebook includes experimentation with the Embeddings through the usage of the Transformers."
   ],
   "id": "d376cbbbb1aa2535"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup Notebook",
   "id": "5d6cb2bf6c3b3634"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "f2f0ab6053a9fe4a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Standard Libraries\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ],
   "id": "b72cd9efd6f46013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experimentations",
   "id": "5ceb9e5fa5117be4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AutoTokenizer with BERT",
   "id": "4e6a32d7fdd6deca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenization",
   "id": "171a69badf29040c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:53:58.037564Z",
     "start_time": "2025-03-02T21:53:54.153233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instance the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "id": "dff5546421045e3d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "-\n",
    "-\n",
    "- The Tokenizer is used to converts raw text into tokens\n",
    "<br>\n",
    "\n",
    "**The Process:**\n",
    "1) The first step is WordPiece Tokenization (e.g., \"playing\" &rarr; [\"play\", \"##ing\"])\n",
    "2) The second step maps tokens into numerical IDs (based on BERT's vocabulary)\n",
    "3) Add special tokens (`[CLS]` and `[SEP]`)\n",
    "4) Pads and truncate text sequence to fix model's input\n",
    "5) Create attention mask"
   ],
   "id": "dac6706f4bb5e3bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:53:58.059222Z",
     "start_time": "2025-03-02T21:53:58.045229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"I love Data Science\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")  # Convert to PyTorch tensors\n",
    "\n",
    "print('Tokens:' , tokens.input_ids)\n",
    "print('Attention Mask:', tokens.attention_mask)"
   ],
   "id": "e3b5989ea404bd24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([[ 101, 1045, 2293, 2951, 2671,  102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- [101] = [CLS] token (start of sentence)\n",
    "- 1045 = \"I\", 2293 = \"love\", 2951 = \"data\", 2671 = \"science\"\n",
    "- [102] = [SEP] token (end of sentence)"
   ],
   "id": "6ebc4c6f77f8369a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Embeddings\n",
    "\n",
    "The algorithm process an input tokenised sequence and compressed them into a lower dimensional representation called \"Embeddings\".\n",
    "\n"
   ],
   "id": "2d704a22c9a1d21d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:54:06.785495Z",
     "start_time": "2025-03-02T21:53:58.120373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instance model\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ],
   "id": "77e5bf633f5d29dc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48c7ce327af0463bb69141760623c686"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:54:19.926183Z",
     "start_time": "2025-03-02T21:54:19.723115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pass tokenized input into the model\n",
    "outputs = model(**tokens)\n",
    "\n",
    "# Extract last hidden state (word embeddings for each token)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(embeddings.shape)  # (batch_size, sequence_length, hidden_size)"
   ],
   "id": "420f52354aab79e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Each token (word or subword) gets a 768-dimensional vector that captures its meaning in context.",
   "id": "ce6ac4a1b9e905ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentence Transformers",
   "id": "fc41b549f95d5d1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T12:11:23.513586Z",
     "start_time": "2025-03-16T12:11:23.511026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the sentences\n",
    "sentences = [\n",
    "    \"I took my dog for a walk\",\n",
    "    \"Today is going to rain\",\n",
    "    \"I took my cat for a walk\"\n",
    "]"
   ],
   "id": "76cc52d46a261d1f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T12:12:13.013857Z",
     "start_time": "2025-03-16T12:12:08.241465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define tokenizer and the model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "id": "43a52262f33ff675",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26355f732de44d218696742dcbe0614e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "458080bbbcc54179bea6b7b70cb4880f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67d8f626aa554cf4b5ce2a16a449dc55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ade4e6b2f85a4d169621eec3a787b899"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10a44e0babdd4f6eb05675edf9e12196"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nNo module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1863\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1006\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:688\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:47\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_outputs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     37\u001B[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001B[1;32m     38\u001B[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     45\u001B[0m     TokenClassifierOutput,\n\u001B[1;32m     46\u001B[0m )\n\u001B[0;32m---> 47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreTrainedModel\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:53\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msdpa_attention\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sdpa_attention_forward\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LOSS_MAPPING\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     Conv1D,\n\u001B[1;32m     56\u001B[0m     apply_chunking_to_forward,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     62\u001B[0m     translate_to_torch_parallel_style,\n\u001B[1;32m     63\u001B[0m )\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/loss/loss_utils.py:19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BCEWithLogitsLoss, MSELoss\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_deformable_detr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_for_object_detection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/loss/loss_deformable_detr.py:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_transforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m center_to_corners_format\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_scipy_available\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/image_transforms.py:22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     23\u001B[0m     ChannelDimension,\n\u001B[1;32m     24\u001B[0m     ImageInput,\n\u001B[1;32m     25\u001B[0m     get_channel_dimension_axis,\n\u001B[1;32m     26\u001B[0m     get_image_size,\n\u001B[1;32m     27\u001B[0m     infer_channel_dimension_format,\n\u001B[1;32m     28\u001B[0m )\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/image_utils.py:65\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torchvision_available():\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m io \u001B[38;5;28;01mas\u001B[39;00m torchvision_io\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InterpolationMode\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/torchvision/__init__.py:6\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets, io, models, ops, transforms, utils\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/torchvision/datasets/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_optical_flow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_stereo_matching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      3\u001B[0m     CarlaStereo,\n\u001B[1;32m      4\u001B[0m     CREStereo,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     SintelStereo,\n\u001B[1;32m     13\u001B[0m )\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/torchvision/datasets/_optical_flow.py:13\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _read_png_16\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _read_pfm, verify_str_arg\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VisionDataset\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/torchvision/datasets/utils.py:6\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlzma\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/lzma.py:27\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m_lzma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m_lzma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _encode_filter_properties, _decode_filter_properties\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named '_lzma'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    560\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    561\u001B[0m     )\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m--> 563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m \u001B[43m_get_model_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    566\u001B[0m     )\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:388\u001B[0m, in \u001B[0;36m_get_model_class\u001B[0;34m(config, model_mapping)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_model_class\u001B[39m(config, model_mapping):\n\u001B[0;32m--> 388\u001B[0m     supported_models \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_mapping\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(supported_models, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    390\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m supported_models\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:763\u001B[0m, in \u001B[0;36m_LazyAutoMapping.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_type \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping:\n\u001B[1;32m    762\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping[model_type]\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_attr_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;66;03m# Maybe there was several model types associated with this config.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m model_types \u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config_mapping\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;241m==\u001B[39m key\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m]\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:777\u001B[0m, in \u001B[0;36m_LazyAutoMapping._load_attr_from_module\u001B[0;34m(self, model_type, attr)\u001B[0m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m module_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n\u001B[1;32m    776\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules[module_name] \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformers.models\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 777\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgetattribute_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:693\u001B[0m, in \u001B[0;36mgetattribute_from_module\u001B[0;34m(module, attr)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attr, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(getattribute_from_module(module, a) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m attr)\n\u001B[0;32m--> 693\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    694\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, attr)\n\u001B[1;32m    695\u001B[0m \u001B[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001B[39;00m\n\u001B[1;32m    696\u001B[0m \u001B[38;5;66;03m# object at the top level.\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1851\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1849\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1851\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1852\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[1;32m   1853\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[0;32m~/Projects/NexusLLM/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1865\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1865\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1866\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1867\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1868\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nNo module named '_lzma'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T12:13:38.300442Z",
     "start_time": "2025-03-16T12:13:38.273592Z"
    }
   },
   "cell_type": "code",
   "source": "import lzma",
   "id": "b4180cc146af627e",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlzma\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/lzma.py:27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m_lzma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m_lzma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _encode_filter_properties, _decode_filter_properties\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01m_compression\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named '_lzma'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd0c6fb1c765872a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexusllm",
   "language": "python",
   "name": "nexusllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
