{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Embeddings - Fine Tuning Models\n",
    "\n",
    "The goal is to research different techniques on how to fine-tune embedding models.\n",
    "\n",
    "**Resources**\n",
    "- [Sentence Transformer - Loss Functions](https://sbert.net/docs/sentence_transformer/loss_overview.html#custom-loss-functions)"
   ],
   "id": "c616ee408a76b58d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Notebook Setup",
   "id": "caca82b7528d4b8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "2e36bec4d703d96e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Standard Libraries\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import (\n",
    "    MultipleNegativesRankingLoss,\n",
    "    CoSENTLoss\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator"
   ],
   "id": "d458ead545074057",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Read Data",
   "id": "caad35376609fdd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All NLI - Pair Class",
   "id": "139a2340c7d303a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.925860Z",
     "start_time": "2025-04-28T08:28:03.130355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read data the All NLI \"Pair Class\" datasets for SentenceTransformerTrainer\n",
    "all_nli_pair_class_train = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"train\")\n",
    "all_nli_pair_class_test = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"dev\")"
   ],
   "id": "50c4d76af1b7f514",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.927400Z",
     "start_time": "2025-04-28T08:28:09.508363Z"
    }
   },
   "cell_type": "code",
   "source": "all_nli_pair_class_train[5]",
   "id": "561ceb0ce3c5ddd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Children smiling and waving at camera',\n",
       " 'hypothesis': 'The kids are frowning',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `label` is `{\"0\": \"entailment\", \"1\": \"neutral\", \"2\", \"contradiction\"}`.",
   "id": "758903e4336acf63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All NLI - Triplets",
   "id": "4f83286d390a6eea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.928072Z",
     "start_time": "2025-04-28T08:28:09.524710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read data the All NLI \"Triplets\" datasets for SentenceTransformerTrainer\n",
    "all_nli_triplets_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")\n",
    "all_nli_triplets_train = all_nli_triplets_dataset[\"train\"].select(range(1_000))\n",
    "all_nli_triplets_eval = all_nli_triplets_dataset[\"dev\"].select(range(300))\n",
    "all_nli_triplets_test = all_nli_triplets_dataset[\"test\"].select(range(300))"
   ],
   "id": "84c042bf6faa8c42",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.931060Z",
     "start_time": "2025-04-28T08:28:11.388844Z"
    }
   },
   "cell_type": "code",
   "source": "all_nli_triplets_train[5]",
   "id": "3a6ec44616693c2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': 'An older man is drinking orange juice at a restaurant.',\n",
       " 'positive': 'A man is drinking juice.',\n",
       " 'negative': 'Two women are at a restaurant drinking wine.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "\n",
    "It is important to prepare the dataset in order to repsect a certain format expected by the Loss Function.\n",
    "\n",
    "- **Sentence Transformer** - The Loss Function expected format is reported in the [Loss Table](https://sbert.net/docs/sentence_transformer/loss_overview.html) and *label* column is generally indicated as `label` or `score`"
   ],
   "id": "5d583d769069fcfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset - from_dict\n",
    "\n",
    "In case your data needs to be prepared, you can use the `Dataset.from_dict` and construct the list of values to insert into your dataset."
   ],
   "id": "ec9f4a81fda0da48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.931662Z",
     "start_time": "2025-04-28T08:28:11.407302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialise the data\n",
    "anchors = []\n",
    "positives = []\n",
    "\n",
    "# Open a file, perform preprocessing, filtering, cleaning, etc.\n",
    "# and append to the lists\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"anchor\": anchors,\n",
    "    \"positive\": positives,\n",
    "})"
   ],
   "id": "508840c42702c719",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SentenceTransformerTrainer\n",
    "\n",
    "This library uses `datasets.Dataset ` ([Reference](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset)) or `datasets.DatasetDict` ([Reference](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict)) instances for both training and evaluation.\n",
    "\n",
    "They accept CSV, JSON, Parquet, Arrow or SQL.\n",
    "\n",
    "Such datasets are marked with `setnence-transformers` in the HuggingFace Datasets Hub."
   ],
   "id": "1712b9df63c323d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loss Functions - Cosine Sentence Similarity",
   "id": "29219b6158706107"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.931916Z",
     "start_time": "2025-04-28T08:28:11.427056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instance model\n",
    "model = SentenceTransformer(\"microsoft/mpnet-base\")\n",
    "\n",
    "# Fine-tuning dataset with 2 samples\n",
    "# Data point: {text_1, text_2, expected_similarity}\n",
    "dataset = Dataset.from_dict({\n",
    "    \"sentence1\": [\"It's nice weather outside today.\", \"He drove to work.\"],\n",
    "    \"sentence2\": [\"It's so sunny.\", \"She walked to the store.\"],\n",
    "    \"score\": [1.0, 0.3]\n",
    "})\n",
    "\n",
    "# Cosine Sentence Loss -> Text 1, Text 2, Expected Similarity\n",
    "loss_function = CoSENTLoss(model)\n",
    "\n",
    "# Fine-tune\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    loss=loss_function\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "6e2f9a630597a9b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/mpnet-base. Creating a new one with mean pooling.\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CoSENTLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      6\u001B[39m dataset = Dataset.from_dict({\n\u001B[32m      7\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33msentence1\u001B[39m\u001B[33m\"\u001B[39m: [\u001B[33m\"\u001B[39m\u001B[33mIt\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms nice weather outside today.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mHe drove to work.\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      8\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33msentence2\u001B[39m\u001B[33m\"\u001B[39m: [\u001B[33m\"\u001B[39m\u001B[33mIt\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms so sunny.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mShe walked to the store.\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      9\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mscore\u001B[39m\u001B[33m\"\u001B[39m: [\u001B[32m1.0\u001B[39m, \u001B[32m0.3\u001B[39m]\n\u001B[32m     10\u001B[39m })\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Cosine Sentence Loss -> Text 1, Text 2, Expected Similarity\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m loss_function = \u001B[43mCoSENTLoss\u001B[49m(model)\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# Fine-tune\u001B[39;00m\n\u001B[32m     16\u001B[39m trainer = SentenceTransformerTrainer(\n\u001B[32m     17\u001B[39m     model=model,\n\u001B[32m     18\u001B[39m     train_dataset=dataset,\n\u001B[32m     19\u001B[39m     loss=loss_function\n\u001B[32m     20\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'CoSENTLoss' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tune",
   "id": "3fc6b50657b06989"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load base Model",
   "id": "53dad4b877ea5c16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.932098Z",
     "start_time": "2025-04-28T08:16:42.621163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model to fine-tune\n",
    "model = SentenceTransformer(\n",
    "    \"microsoft/mpnet-base\",\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"MPNet base trained on AllNLI triplets\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "loss = MultipleNegativesRankingLoss(model)"
   ],
   "id": "5c220fbee1d9c73b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/mpnet-base. Creating a new one with mean pooling.\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine-Tuning Arguments",
   "id": "4a2de43465417cb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.932268Z",
     "start_time": "2025-04-28T08:16:46.727428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the training arguments\n",
    "train_args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/mpnet-base-all-nli-triplet\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # GPU's specific\n",
    "    bf16=True,  # GPU's specific\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"all-nli-triplet\"\n",
    ")"
   ],
   "id": "b52b129ef6a892d5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Base Model Evaluation\n",
    "\n",
    "The package `sentence_transformers.evaluation` offers several evaluation strategies for each specific use case. For example pair-class or triplets."
   ],
   "id": "49f1aec16b247007"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.932458Z",
     "start_time": "2025-04-28T08:16:52.005053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Base model evaluator\n",
    "base_evaluator = TripletEvaluator(\n",
    "    anchors=all_nli_triplets_eval[\"anchor\"],\n",
    "    positives=all_nli_triplets_eval[\"positive\"],\n",
    "    negatives=all_nli_triplets_eval[\"negative\"],\n",
    "    name=\"all-nli-dev\",\n",
    ")\n",
    "base_evaluator(model)"
   ],
   "id": "1291dfbf0310292f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all-nli-dev_cosine_accuracy': 0.5933333039283752}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "4d4b554a12d0fc80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T08:30:45.932692Z",
     "start_time": "2025-04-28T08:16:55.628588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the trainer and start training\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=all_nli_triplets_train,\n",
    "    eval_dataset=all_nli_triplets_eval,\n",
    "    loss=loss,\n",
    "    evaluator=base_evaluator,\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "2ea071c7d22a3ebd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1abe67823b54937bd4146c49a7b77dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=1.8256043933686756, metrics={'train_runtime': 35.531, 'train_samples_per_second': 28.144, 'train_steps_per_second': 1.773, 'total_flos': 0.0, 'train_loss': 1.8256043933686756, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_evaluator = TripletEvaluator(\n",
    "    anchors=test_dataset[\"anchor\"],\n",
    "    positives=test_dataset[\"positive\"],\n",
    "    negatives=test_dataset[\"negative\"],\n",
    "    name=\"all-nli-test\",\n",
    ")\n",
    "test_evaluator(model)"
   ],
   "id": "b5db0e63f6dd973d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
