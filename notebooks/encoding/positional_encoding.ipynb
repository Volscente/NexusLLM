{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding\n",
    "\n",
    "This notebook experiments with different Positional Encoding techniques."
   ],
   "id": "893bc57fe8697416"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup Notebook",
   "id": "862ad27775dd2aaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "82ac28bd36c043b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:15:01.523964Z",
     "start_time": "2025-03-22T15:15:00.634767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Standard Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ],
   "id": "420bcd11f749fe26",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sinusoidal",
   "id": "7927933248ac0d2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:15:19.891058Z",
     "start_time": "2025-03-22T15:15:19.885219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Positional Encoder through sinusoidal functions.\n",
    "\n",
    "    PE(pos, 2i) = sin(pos/(10000^(2i/d_model)))\n",
    "    PE(pos, 2i + 1) = cost(pos/(10000^(2i/d_model)))\n",
    "\n",
    "    pos: position in the sequence\n",
    "    i: is the dimension index (2i = half of the model dimension d_model)\n",
    "    d_model: is the model dimension (the embedding dimension)\n",
    "\n",
    "    NOTE: 2i and 2i + 1 is for separate sine and cosine values into even and odd indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings_size, dropout_probability=0.1, max_len_sequence=5000):\n",
    "\n",
    "        # Initialise the super class\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        # Set the dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        # Initialise positional encoding matrix of dimension (max_length_sequence x embeddings_size)\n",
    "        positional_encoding_matrix = torch.zeros(max_len_sequence, embeddings_size)\n",
    "\n",
    "        # Create the position from 1 to the max length of the input sequence (reshape x -> (x, 1))\n",
    "        position = torch.arange(0, max_len_sequence, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Create the dividend term as 10000^(2i/d)\n",
    "        dividend_term = torch.exp(torch.arange(0, embeddings_size, 2).float() * (-np.log(10000.0) / embeddings_size))\n",
    "\n",
    "        # Compute positional encoding for even and odd columns in the Positional Encoding Matrix\n",
    "        positional_encoding_matrix[:, 0::2] = torch.sin(position * dividend_term)\n",
    "        positional_encoding_matrix[:, 1::2] = torch.cos(position * dividend_term)\n",
    "\n",
    "        # Add a dimension for the batch_size through 'unsqueeze(0)' in the first index and then transpose\n",
    "        # NOTE: (max_length_sequence, embeddings_size) -> (max_length_sequence, batch_size, embeddings_size)\n",
    "        positional_encoding_matrix = positional_encoding_matrix.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        # Save the PE matrix\n",
    "        self.register_buffer('positional_encoding_matrix', positional_encoding_matrix)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "\n",
    "        # Add the positional encoding to the input sequence (Just sum them up)\n",
    "        output = sequence + self.positional_encoding_matrix[:sequence.size(0), :]\n",
    "\n",
    "        # Apply dropout\n",
    "        return self.dropout(output)\n"
   ],
   "id": "734b133e78e1a7d4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:16:30.472753Z",
     "start_time": "2025-03-22T15:16:30.440487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of positional encoding\n",
    "example_sequence_len = 5\n",
    "example_embeddings_size = 4\n",
    "\n",
    "# Initialise variables\n",
    "example_pe = torch.zeros(example_sequence_len, example_embeddings_size)\n",
    "example_position = torch.arange(0, example_sequence_len, dtype=torch.float).unsqueeze(1)\n",
    "example_dividend_term = torch.arange(0, example_embeddings_size, 2).float()\n",
    "\n",
    "# Compute positional encoding for even and odd columns in the Positional Encoding Matrix\n",
    "example_pe[:, 0::2] = example_position * example_dividend_term\n",
    "example_pe[:, 1::2] = (example_position * example_dividend_term) - 1\n",
    "\n",
    "# Define a sequence\n",
    "example_sequence = torch.tensor([\n",
    "    [1, 11, 111, 1111],\n",
    "    [2, 22, 222, 2222],\n",
    "    [3, 33, 333, 3333],\n",
    "    [4, 44, 444, 4444],\n",
    "    [5, 55, 555, 5555]]\n",
    ")\n",
    "\n",
    "print('Sequence Length: ', example_sequence_len)\n",
    "print('Embeddings Size: ', example_embeddings_size)\n",
    "print('Position: ', example_position)\n",
    "print('Dividend term: ', example_dividend_term)\n",
    "print('Positional Encoding (Pre Transformation): ', example_pe)\n",
    "print('Positional Encoding (Transformed): ', example_pe.unsqueeze(0).transpose(0, 1))\n",
    "print('Positional Encoding (Pre Shape)', example_pe.shape)\n",
    "print('Positional Encoding (After Shape)', example_pe.unsqueeze(0).transpose(0, 1).shape)\n",
    "print('Sequence: ', example_sequence)\n",
    "print('Sequence shape: ', example_sequence.shape)"
   ],
   "id": "4dc76e297aeb8bcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length:  5\n",
      "Embeddings Size:  4\n",
      "Position:  tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n",
      "Dividend term:  tensor([0., 2.])\n",
      "Positional Encoding (Pre Transformation):  tensor([[ 0., -1.,  0., -1.],\n",
      "        [ 0., -1.,  2.,  1.],\n",
      "        [ 0., -1.,  4.,  3.],\n",
      "        [ 0., -1.,  6.,  5.],\n",
      "        [ 0., -1.,  8.,  7.]])\n",
      "Positional Encoding (Transformed):  tensor([[[ 0., -1.,  0., -1.]],\n",
      "\n",
      "        [[ 0., -1.,  2.,  1.]],\n",
      "\n",
      "        [[ 0., -1.,  4.,  3.]],\n",
      "\n",
      "        [[ 0., -1.,  6.,  5.]],\n",
      "\n",
      "        [[ 0., -1.,  8.,  7.]]])\n",
      "Positional Encoding (Pre Shape) torch.Size([5, 4])\n",
      "Positional Encoding (After Shape) torch.Size([5, 1, 4])\n",
      "Sequence:  tensor([[   1,   11,  111, 1111],\n",
      "        [   2,   22,  222, 2222],\n",
      "        [   3,   33,  333, 3333],\n",
      "        [   4,   44,  444, 4444],\n",
      "        [   5,   55,  555, 5555]])\n",
      "Sequence shape:  torch.Size([5, 4])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0dc1691cc629590"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
