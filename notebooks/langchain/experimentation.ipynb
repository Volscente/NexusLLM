{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facc9b58-9f0a-49bb-a9c8-d7ff6e00d591",
   "metadata": {},
   "source": [
    "# LangChain Experimentation\n",
    "\n",
    "Experiment with LangChain and OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41190dec-ec54-4208-8e3f-667625c4cd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0969968-22d2-4cd1-a0be-d68bfb3632a6",
   "metadata": {},
   "source": [
    "# OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec70375b-b444-4705-9392-d69f5790672e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set OpenAI API Key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ecaba5c-e425-44b5-8457-e11b4a2e3642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_inference(prompt: str, \n",
    "                  model: str = 'gpt-3.5-turbo') -> str:\n",
    "    \n",
    "    # Compose the request body\n",
    "    messages = [\n",
    "        {'role': 'user',\n",
    "         'content': prompt}]\n",
    "    \n",
    "    # Send the request and retrieve the response\n",
    "    response = openai.ChatCompletion.create(model=model, \n",
    "                                            messages=messages, \n",
    "                                            temperature=0)\n",
    "    \n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770ef242-5049-4ebe-a2b7-d30fd9367fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get_inference('What is 1+1?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c222da-374e-43eb-beb6-e79aeef13eda",
   "metadata": {},
   "source": [
    "# Open AI Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215d5b15-b7fd-428f-8a94-f0b433e7aca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a ChatOpenAI object\n",
    "# NOTE: temperature at 0.0 reduces the noise, but also generalization capabilities\n",
    "# Note2: it retrieves the API KEY from 'openai.api_key'\n",
    "langchain_chat = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d13518-4636-4c9a-8949-51642ab91ea6",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e477be79-c424-4caa-a1e5-84c8b4461cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt template string\n",
    "string_prompt_template = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3ceda0-69ba-4f00-83f1-34624476d558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialise a Prompt Template\n",
    "langchain_prompt_template = ChatPromptTemplate.from_template(string_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292759f3-856f-40bd-84ec-4e932da194f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7ac8e7-d781-456d-b007-5a7d28b6dd37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve prompt input variables\n",
    "langchain_prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0075b8de-dae6-418d-b881-91a4b026e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt style\n",
    "prompt_style = \"\"\"American english \\\n",
    "in a calm and respectful tone.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a706e7e0-416a-469a-a059-8c73538ab3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt text\n",
    "prompt_text = \"\"\"Arr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need your help \\\n",
    "right now, mattey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0f59f2-61ff-4898-ab57-79553a1a9ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the message from the prompt style and text\n",
    "message = langchain_prompt_template.format_messages(style=prompt_style, text=prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b417ecff-4a82-4ee9-a96b-4e083feca869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American english in a calm and respectful tone.\\n. text: ```Arr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need your help right now, mattey!\\n```\\n\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ccec8a7-7172-4768-8d56-986be5fe555a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the response\n",
    "#message_response = langchain_chat(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b86d1-74d2-4016-a4a6-8bca52320904",
   "metadata": {},
   "source": [
    "# Parser\n",
    "\n",
    "It is used to extract information from LLM’s output in a machine-readable format (e.g., JSON from LLM’s output text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20de38ea-7402-4bfa-9a50-8da871fc6a64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected JSON output\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81258fe5-ebb0-4f1e-974d-141060a638af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input customer review\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt tempalte\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec0808a4-7326-4057-970a-958c0fcb5432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a700532e-06ed-4fe3-96f7-4c6a0e8e0e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the LLM\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "#response = chat(messages)\n",
    "#print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9afdd2b4-34d3-4bce-b721-9a87f9cae9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "#response.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721bf3f7-9deb-4264-a8cb-de82208a838b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a set of schemas to extract the information from the customer review\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a6c71d8-525e-454c-ae47-77340e7c219c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the Output Parser from the above define schemas\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f5edb6b-7f70-4404-94bb-edf0902d8497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Reitreve instructions that the prompt will pass to the LLM to have an output able to be parsed\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96383a4a-39e4-4aec-b44e-3eb450166aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define new prompt\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Define new prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "# Create a message with the format instruction\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e6f70dc-3225-4fef-a331-150d7d81d28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the LLM\n",
    "#response = langchain_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d688a592-77ed-47e5-8fb9-9845b0db1b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the output\n",
    "#output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73b6e7-c810-4ee8-a7cb-f0a54b05efdf",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c12e6-d5df-4459-8b29-bfb763ebbb24",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory\n",
    "\n",
    "It helps to feed to the LLM the history of the current conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34dc5b59-9e52-42a1-aaff-0a1b98376da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an Open AI Chat\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# Create a Conversation Memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a Conversation that allows to interact with the LLM while keeping the memory of the conversation\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "521f56a9-75ae-4a52-b6aa-f2461fe33a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First prompt\n",
    "#conversation.predict(input=\"Hi, my name is Simone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19e37127-32cc-41d8-8d9f-c6d617d2cde0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Intermediate prompt\n",
    "#conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1d4c580-26f2-4e56-82de-e9fd70487ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final prompt (it remembers my name)\n",
    "#conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f25c22-c64e-4cdf-8820-ebc8341365be",
   "metadata": {},
   "source": [
    "By setting `verbose=True` the ChatOpenAI object will feed the history of the conversation every time as part of the next prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92d94729-205b-4a56-aefa-324ae881bfac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664dd62d-70c1-4b0f-9303-f7a21fa851bf",
   "metadata": {},
   "source": [
    "The above object is keeping track of the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e49de92-e6ab-45e7-ad5f-4dfd25b03e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "# It is possible to restore the conversation into another ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Restore\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c442013-9fa5-46c2-82de-0b029c41230f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top up\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85f3e574-c660-4b05-a165-60338568fd03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0d502-b36a-46d9-8425-c1af6e3bbd1d",
   "metadata": {},
   "source": [
    "## Conversation Buffer Window Memory\n",
    "\n",
    "Upon sending each time the whole conversation history to LLM, the prompt starts to begin quite long in terms of tokens (and thus expensive). The Conversation Buffer Window Memory allows to keep just a subset of the whole conversation. The last `k` interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b397c08-72b8-4c4e-8cb2-bc46703c9cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a Conversation Window of one history\n",
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d344df9c-4663-4e38-b1ea-8001a4bfae21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set history\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21b3e433-5828-49de-8035-72906c57f7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only last interaction is kept\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa03c31-8553-48a4-becf-17b53bd87e4d",
   "metadata": {},
   "source": [
    "## Conversation Token Buffer Memory\n",
    "\n",
    "It remembers only the last k tokens of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f092eef-64b1-4a77-a359-a2bd446580d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the conversation token buffer\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "\n",
    "# Set history\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bec396e-1795-4829-8dda-e528683e65e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68655f52-6f50-404f-9d3b-5c9c644b07bd",
   "metadata": {},
   "source": [
    "## Conversation Summary Memory\n",
    "\n",
    "It uses the LLM to write a summary of the conversation so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4199d79d-63ca-4190-a4d3-17d2b27712b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Define the Conversation Summary\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, \n",
    "                                         max_token_limit=100)\n",
    "\n",
    "# Set history\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "# memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "#                     {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e41c80-a460-4aa3-8d03-f5fbd574d66e",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "A chain is a combination of a LLM with a generic input prompt. It is possible to combine input/output sequence of LLMs prompts and related outputs.\n",
    "\n",
    "The idea is to combine multiple chgains where the output of one is the input of the next one.\n",
    "\n",
    "\n",
    "**Types:**\n",
    "- Simple Sequential Chain: Single input/output\n",
    "- Sequential Chain: Multiple inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf52c08d-2466-433d-9169-617a9c652bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "llm = ChatOpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0b85934-c9f6-4bf6-8b21-e82d3a5e531c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f5dc819-0eaf-4880-a1a3-e15f3bb949dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a2b7efa-cdde-41de-886f-d0247993a0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chain LLM and Prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c66c096a-d6e8-4373-8adf-328d0e77b8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute the chain: pass the prompt to the LLM\n",
    "product = \"Queen Size Sheet Set\"\n",
    "#chain.run(product) # Output: \"Royal Comfort Linens\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1fcd8-8c94-44f1-9f54-147e2c0a0ee2",
   "metadata": {},
   "source": [
    "## Simple Sequential Chains\n",
    "\n",
    "The chain links a single output of a Prompt + LLM chain as an input to just another chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc021305-cd4b-4710-b0dd-6cd458290cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define LLM\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# Define prompt template for first chain\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# First chain\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "\n",
    "# Define prompt template for second chain\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "\n",
    "# Chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eaf707db-72b6-4c27-9ec6-1e9cfaa699a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Simple chain of chain_one + chain_two\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aa27adc-ca48-4d40-b192-65b0d2377b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f2912-01ef-4f8c-8f99-4ffd59685f81",
   "metadata": {},
   "source": [
    "> Entering new SimpleSequentialChain chain...\n",
    "One possible name to describe a company that makes Queen Size Sheet Sets could be \"RoyalRest Linens\" or \"MajesticSleep Bedding\". These names evoke a sense of luxury and elegance that align with the product being offered.\n",
    "\"RoyalRest Linens\" and \"MajesticSleep Bedding\" are companies specializing in luxurious and elegant Queen Size Sheet Sets.\n",
    "\n",
    "> Finished chain.\n",
    "'\"RoyalRest Linens\" and \"MajesticSleep Bedding\" are companies specializing in luxurious and elegant Queen Size Sheet Sets.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f1556-6e02-4453-beea-f7a5d65dfbfe",
   "metadata": {},
   "source": [
    "## Simple Chains\n",
    "\n",
    "It combines multiple chains together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe34ec66-c2ae-4c2a-81be-172642b7516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79f9b4ec-662d-44d6-ade8-fdfa3eeaf012",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22443ef9-df59-4c54-99e8-f096cd45040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9036dd7e-47d7-4ce4-a766-2323fa72091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a18a27b6-0b54-45bd-a503-1ce40bcc5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea925dfe-53a0-4498-a3c1-d00afccca4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sort of review\n",
    "# overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ae1a3-ff03-4733-b49b-a7559a88b16c",
   "metadata": {},
   "source": [
    "## Router Chains\n",
    "\n",
    "Depending on the matched criteria, it routes the output to a specific chain with respect to another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "802cc863-4127-48fd-b16d-226130720e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define different prompt templates for different topics\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f89e4ee-b356-48a5-8226-4a4dca650cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the routes\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcedad2e-8d92-4ea1-ae6d-84692109bdc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f4d707e-37d2-47ee-a231-a466cc0dbd22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize dictionary for destination chains\n",
    "destination_chains = {}\n",
    "\n",
    "# Fetch the routes\n",
    "for p_info in prompt_infos:\n",
    "    \n",
    "    # Extract name\n",
    "    name = p_info[\"name\"]\n",
    "    \n",
    "    # Extract the prompt tempalte\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    \n",
    "    # Construct the prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    \n",
    "    # Let's create one destination chain for each prompt\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    # Add the destination chain to the dictionary\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93f08ade-2811-4118-bef0-1f9c4b81488c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['physics: Good for answering questions about physics',\n",
       " 'math: Good for answering math questions',\n",
       " 'History: Good for answering history questions',\n",
       " 'computer science: Good for answering computer science questions']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b659b6ec-8f99-4a4e-bd3e-fe21616644ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the default chain\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10b2a366-3678-48df-9975-7b42277b6a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt template to route the input to the different chains\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73d5d7bd-cddd-4658-9612-7a3c9ad23f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add destination chains to the prompt template for routing\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "\n",
    "# Create the prompt template for routing\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "# Create the router chain that will route the intput to the best suited chain\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b854666-d1a0-4c1b-a136-2904ef074fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#chain.run(\"What is black body radiation?\") # Output will be routed to Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409db42b-f070-42c9-8f76-6bda1fa728f6",
   "metadata": {},
   "source": [
    "# Question and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a37e1-a0be-410f-8488-b904710e6f9d",
   "metadata": {},
   "source": [
    "## Vector Store Index\n",
    "\n",
    "It allows to implement a Q&A LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e744926-7f1f-48f3-8528-b2b7e5503834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the loader for the .CSV file\n",
    "# loader = CSVLoader(file_path='csv_file_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a9c6200-9383-49d9-bf40-dc0e8437ccda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a Vector Store that will store embeddings of the columns inside the .CSV loaded in 'loader'\n",
    "# NOTE: Embeddings are easier to be queried from a LLM, since there are less tokens that an actual text\n",
    "# index = VectorstoreIndexCreator(\n",
    "#     vectorstore_cls=DocArrayInMemorySearch\n",
    "# ).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98e89cdd-1a68-4115-8e05-7f5451b25bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4429ca85-9540-4e7d-955b-e15566c151d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve LLM response\n",
    "# response = index.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de484a-43d2-49b5-82b0-b3bcdd00bd55",
   "metadata": {},
   "source": [
    "## Open AI Embeddings\n",
    "\n",
    "It returns the embedding of the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "daebdd55-e463-4e1a-86f1-6ab6edbac9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedder client\n",
    "#embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad12abb8-f641-4634-922b-2f2d2a3d0c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the embedding\n",
    "#embed = embeddings.embed_query(\"Hi my name is Harrison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7950a7-fca0-4197-9704-af14b3b73383",
   "metadata": {},
   "source": [
    "### Embeddings DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4929fae-540f-4399-8b2c-b07f583d8335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all the docs from the .CSV file\n",
    "#docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8fdd1-6922-446f-b4d8-06cd09065156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DB\n",
    "# db = DocArrayInMemorySearch.from_documents(\n",
    "#     docs, \n",
    "#     embeddings\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74e9a354-7e90-4e74-8e0b-67257a7a7022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49274b2c-88c9-48e2-9e0a-f1528a58f153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look for similar documents\n",
    "#docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f52086ca-3920-47c0-81f9-d81e8425bed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In order to incorporate the returned docs into a more suitable response, let's use an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9397e3de-d15a-449b-8bc1-778de21358a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a retriever to query the Embeddings DB\n",
    "# retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d92d884-7b24-4bca-8fb3-78a86be0ff5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "#llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5902c4a6-3239-4371-8684-2f0b0419ea49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare prompt\n",
    "#qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b81ee6d-1c63-4e4d-8d49-bff4682a70b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the LLM\n",
    "# response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "# shirts with sun protection in a table in markdown and summarize each one.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "170eb329-d27d-4209-a416-c615eecf752c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a chain from the retriever to the LLM\n",
    "# qa_stuff = RetrievalQA.from_chain_type(\n",
    "#     llm=llm, \n",
    "#     chain_type=\"stuff\", \n",
    "#     retriever=retriever, \n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5f14315-a532-4c6c-bf98-47f9567ba798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5eb01cf7-444a-4722-9da4-97b79fa514de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a7863-6cc3-45ec-9dbd-27c8d8a3cee3",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ab582-9abc-4e4c-8acc-04d44a96ce17",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc74d97-9b42-484b-a591-4cef5f4347fa",
   "metadata": {},
   "source": [
    "First we need to nstall wikipedia as the source of information\n",
    "\n",
    "``` bash\n",
    "pip install -U wikipedia\n",
    "```\n",
    "\n",
    "Let's define the LLM (the reasoning engine that will guide the Agent) and the tools the agent will use\n",
    "\n",
    "``` python\n",
    "# Define the LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Load the tools the Agent will use\n",
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n",
    "```\n",
    "\n",
    "Finally define the Agent with the LLM and the Tools:\n",
    "\n",
    "``` python\n",
    "# Define the agent\n",
    "agent= initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)\n",
    "\n",
    "# Query the agent\n",
    "agent(\"What is the 25% of 300?\")\n",
    "```\n",
    "\n",
    "The LLM would eventually understand they do not know the answer and need to rely on one of their tools (llm-math).\n",
    "\n",
    "It is also possible to define your own tools or source of information used by the Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e308b-8d36-463b-8cd3-b03d1369ab15",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889a72c-7bd6-469c-a098-1d3a9d6b9050",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Define agent\n",
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
